{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpnphvn8/PjSOmlDgrr3/j"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT word embedding + XGBoost WRAPPER\n",
        "## First working wrapper"
      ],
      "metadata": {
        "id": "W-VExrTG7LY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "HtWXP4vQ7RgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as functional\n",
        "import matplotlib.pyplot as plt\n",
        "import transformers\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "import gc\n",
        "from transformers import BertModel\n",
        "from sklearn.metrics import roc_auc_score,f1_score\n",
        "import time\n",
        "import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "import re,string,unicodedata\n",
        "import nltk\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ],
      "metadata": {
        "id": "rX0jUHlz7WO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "fake = pd.read_csv('/content/drive/MyDrive/thesis-data/Fake.csv')\n",
        "true = pd.read_csv('/content/drive/MyDrive/thesis-data/True.csv')\n",
        "\n",
        "fake[\"label\"] = 0\n",
        "true[\"label\"] = 1\n",
        "\n",
        "df = pd.concat([fake, true], ignore_index = True)\n",
        "\n",
        "df['text'] = df['title'] + \" \" + df['text']\n",
        "df.drop(columns=['title', 'date', 'subject'], inplace = True)"
      ],
      "metadata": {
        "id": "Na-L7Eab7YbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "punctuation = list(string.punctuation)\n",
        "stop.update(punctuation)\n",
        "\n",
        "\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "# Removing URL's\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub(r'http\\S+', '', text)\n",
        "#Removing the stopwords from text\n",
        "def remove_stopwords(text):\n",
        "    final_text = []\n",
        "    for i in text.split():\n",
        "        if i.strip().lower() not in stop:\n",
        "            final_text.append(i.strip())\n",
        "    return \" \".join(final_text)\n",
        "    \n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    text = remove_stopwords(text)\n",
        "    return text\n",
        "\n",
        "#Apply function on review column\n",
        "df['text']=df['text'].apply(denoise_text)"
      ],
      "metadata": {
        "id": "bmcTHIfz7cnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_original = df.copy()\n",
        "df = df.sample(frac=1).reset_index(drop=True)[:1000]"
      ],
      "metadata": {
        "id": "tcCeOZKC7h0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrapper"
      ],
      "metadata": {
        "id": "gs7_i5gJ7nSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "\n",
        "class XGBClassifier:\n",
        "    def __init__(self, max_depth=5, n_estimators=100, learning_rate=0.1):\n",
        "        self.max_depth = max_depth\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        self.bert = AutoModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
        "\n",
        "    def _get_bert_embedding(self, text):\n",
        "        input_ids = self.tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=64)\n",
        "        input_ids = np.array(input_ids)\n",
        "        input_ids = np.expand_dims(input_ids, axis=0)\n",
        "        input_ids = torch.tensor(input_ids)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.bert(input_ids)\n",
        "            last_hidden_state = outputs.last_hidden_state\n",
        "            last_hidden_state = last_hidden_state[:, 0, :].numpy()\n",
        "\n",
        "        return last_hidden_state\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        X_train_embeddings = []\n",
        "        for text in X_train:\n",
        "            embedding = self._get_bert_embedding(text)\n",
        "            X_train_embeddings.append(embedding)\n",
        "        X_train_embeddings = np.array(X_train_embeddings)\n",
        "        X_train_embeddings = np.squeeze(X_train_embeddings, axis=1)\n",
        "\n",
        "        self.model = xgb.XGBClassifier(max_depth=self.max_depth, n_estimators=self.n_estimators, learning_rate=self.learning_rate)\n",
        "        self.model.fit(X_train_embeddings, y_train)\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        X_test_embeddings = []\n",
        "        for text in X_test:\n",
        "            embedding = self._get_bert_embedding(text)\n",
        "            X_test_embeddings.append(embedding)\n",
        "        X_test_embeddings = np.array(X_test_embeddings)\n",
        "        X_test_embeddings = np.squeeze(X_test_embeddings, axis=1)\n",
        "\n",
        "        y_pred = self.model.predict(X_test_embeddings)\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def randomized_search(self, X_train, y_train, param_distributions, cv=2, n_iter=1):\n",
        "        X_train_embeddings = []\n",
        "        for text in X_train:\n",
        "            embedding = self._get_bert_embedding(text)\n",
        "            X_train_embeddings.append(embedding)\n",
        "        X_train_embeddings = np.array(X_train_embeddings)\n",
        "        X_train_embeddings = np.squeeze(X_train_embeddings, axis=1)\n",
        "\n",
        "        self.model = xgb.XGBClassifier()\n",
        "        random_search = RandomizedSearchCV(self.model, param_distributions=param_distributions, cv=cv, n_iter=n_iter)\n",
        "        random_search.fit(X_train_embeddings, y_train)\n",
        "\n",
        "        self.max_depth = random_search.best_params_['max_depth']\n",
        "        self.n_estimators = random_search.best_params_['n_estimators']\n",
        "        self.learning_rate = random_search.best_params_['learning_rate']\n",
        "\n",
        "        self.model = xgb.XGBClassifier(max_depth=self.max_depth, n_estimators=self.n_estimators, learning_rate=self.learning_rate)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        y_pred = self.predict(X_test)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        return acc"
      ],
      "metadata": {
        "id": "qWc8RJnd7n3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "#df = pd.read_csv('data.csv')\n",
        "X = df['text'].tolist()\n",
        "y = df['label'].tolist()\n",
        "\n",
        "# Split data into training and test sets\n",
        "train_size = int(0.8 * len(X))\n",
        "X_train = X[:train_size]\n",
        "y_train = y[:train_size]\n",
        "X_test = X[train_size:]\n",
        "y_test = y[train_size:]\n",
        "\n",
        "# Instantiate classifier\n",
        "classifier = XGBClassifier()\n",
        "\n",
        "# Perform randomized search over hyperparameters\n",
        "param_distributions = {\n",
        "    'max_depth': [3, 5, 7, 9],\n",
        "    'n_estimators': [50, 100, 200, 500],\n",
        "    'learning_rate': [0.01, 0.1, 0.3, 0.5]\n",
        "}\n",
        "classifier.randomized_search(X_train, y_train, param_distributions)\n",
        "\n",
        "# Train classifier on training data\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate classifier on test data\n",
        "accuracy = classifier.evaluate(X_test, y_test)\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "id": "MM7kHFRb7qTI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}