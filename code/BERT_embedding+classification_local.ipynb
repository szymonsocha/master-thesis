{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpFId1mjCySe"
   },
   "source": [
    "# Word Embeddings + various classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:58:29.587869874Z",
     "start_time": "2023-06-03T10:58:29.528231993Z"
    },
    "id": "TCU7NPLUuQrU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import gensim\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "import time\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "from tabulate import tabulate\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, balanced_accuracy_score, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:58:29.747891438Z",
     "start_time": "2023-06-03T10:58:29.669982987Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../data/liar_dataset/train.tsv', sep=\"\\t\", usecols=[1, 2], names = [\"label\", \"text\"])\n",
    "test = pd.read_csv('../../data/liar_dataset/test.tsv', sep=\"\\t\", usecols=[1, 2], names = [\"label\", \"text\"])\n",
    "valid = pd.read_csv('../../data/liar_dataset/valid.tsv', sep=\"\\t\", usecols=[1, 2], names = [\"label\", \"text\"])\n",
    "\n",
    "train = pd.concat([train, valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:58:29.885040352Z",
     "start_time": "2023-06-03T10:58:29.803226787Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "half-true      2362\n",
       "false          2258\n",
       "mostly-true    2213\n",
       "barely-true    1891\n",
       "true           1845\n",
       "pants-fire      955\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:58:30.503732663Z",
     "start_time": "2023-06-03T10:58:30.477163787Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "convert_text_labels = lambda x: 0 if x in ['true', 'mostly-true', 'half-true'] else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:58:30.717908895Z",
     "start_time": "2023-06-03T10:58:30.633727455Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train['label'] = train['label'].apply(convert_text_labels)\n",
    "test['label'] = test['label'].apply(convert_text_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:58:30.984290446Z",
     "start_time": "2023-06-03T10:58:30.940366213Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6420\n",
       "1    5104\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:58:31.418876561Z",
     "start_time": "2023-06-03T10:58:31.297142986Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(train['label'].value_counts().index, train['label'].value_counts().values)\n",
    "\n",
    "plt.xticks(train['label'].value_counts().index)\n",
    "\n",
    "plt.title('Train')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:58:31.939199988Z",
     "start_time": "2023-06-03T10:58:31.903140123Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    714\n",
       "1    553\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:58:32.435096966Z",
     "start_time": "2023-06-03T10:58:32.323299654Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(test['label'].value_counts().index, test['label'].value_counts().values)\n",
    "\n",
    "plt.xticks(test['label'].value_counts().index)\n",
    "\n",
    "plt.title('Test')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:58:32.714519185Z",
     "start_time": "2023-06-03T10:58:32.683315139Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    11524.000000\n",
       "mean       106.895783\n",
       "std         58.415051\n",
       "min         11.000000\n",
       "25%         73.000000\n",
       "50%         99.000000\n",
       "75%        133.000000\n",
       "max       3192.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text'].str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:58:33.157164231Z",
     "start_time": "2023-06-03T10:58:33.109327443Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1267.000000\n",
       "mean      109.578532\n",
       "std        98.031030\n",
       "min        12.000000\n",
       "25%        74.000000\n",
       "50%        98.000000\n",
       "75%       133.000000\n",
       "max      2941.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['text'].str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:58:33.478275240Z",
     "start_time": "2023-06-03T10:58:33.461453199Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qaFYAFhluSCU",
    "outputId": "cf3c01ae-ba06-4501-d53b-8926d079f411"
   },
   "outputs": [],
   "source": [
    "# # OLD data\n",
    "# fake = pd.read_csv('../../data/Fake.csv')\n",
    "# true = pd.read_csv('../../data/True.csv')\n",
    "#\n",
    "# fake[\"label\"] = 0\n",
    "# true[\"label\"] = 1\n",
    "#\n",
    "# df = pd.concat([fake, true], ignore_index = True)\n",
    "#\n",
    "# df['text'] = df['title'] + \" \" + df['text']\n",
    "# df.drop(columns=['title', 'date', 'subject'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:58:34.667521400Z",
     "start_time": "2023-06-03T10:58:33.781067708Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ieOrxB_AuTwA",
    "outputId": "f5f4e1d6-b001-4dab-b81f-7b08125dcdfc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Wesub\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "<timed exec>:8: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 469 ms\n",
      "Wall time: 977 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)\n",
    "\n",
    "\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    text = re.sub('\\[[^]]*\\]', '', text)\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop:\n",
    "            final_text.append(i.strip())\n",
    "    return \" \".join(final_text)\n",
    "    \n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "\n",
    "train['text']=train['text'].apply(denoise_text)\n",
    "test['text']=test['text'].apply(denoise_text)\n",
    "\n",
    "train.to_csv(\"../../data/train.csv\", index=False)\n",
    "test.to_csv(\"../../data/test.csv\", index=False)\n",
    "\n",
    "\n",
    "X_train = train['text'].tolist()\n",
    "y_train = train['label'].tolist()\n",
    "with open(\"../../data/X_train\", \"wb\") as fp:\n",
    "    pickle.dump(X_train, fp)\n",
    "with open(\"../../data/y_train\", \"wb\") as fp:\n",
    "    pickle.dump(y_train, fp)\n",
    "\n",
    "X_test = test['text'].tolist()\n",
    "y_test = test['label'].tolist()\n",
    "with open(\"../../data/X_test\", \"wb\") as fp:\n",
    "    pickle.dump(X_test, fp)\n",
    "with open(\"../../data/y_test\", \"wb\") as fp:\n",
    "    pickle.dump(y_test, fp)\n",
    "\n",
    "\n",
    "train_small = train.sample(frac=1).reset_index(drop=True)[:1000]\n",
    "train_small.to_csv(\"../../data/train_small.csv\", index=False)\n",
    "\n",
    "X_train_small = train_small['text'].tolist()\n",
    "y_train_small = train_small['label'].tolist()\n",
    "with open(\"../../data/small/X_train\", \"wb\") as fp:\n",
    "    pickle.dump(X_train_small, fp)\n",
    "with open(\"../../data/small/y_train\", \"wb\") as fp:\n",
    "    pickle.dump(y_train_small, fp)\n",
    "with open(\"../../data/small/X_test\", \"wb\") as fp:\n",
    "    pickle.dump(X_test, fp)\n",
    "with open(\"../../data/small/y_test\", \"wb\") as fp:\n",
    "    pickle.dump(y_test, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQMXCPQ97YRZ"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIG7v_fk7jbE"
   },
   "source": [
    "Reduce dataset for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:58:35.190752477Z",
     "start_time": "2023-06-03T10:58:35.162240642Z"
    },
    "id": "3OAKnGLyuVS0"
   },
   "outputs": [],
   "source": [
    "# df_original = df.copy()\n",
    "# df = df.sample(frac=1).reset_index(drop=True)[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oA_tL8Sg7VWO"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-lrFVWwGiwt"
   },
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:58:36.219897278Z",
     "start_time": "2023-06-03T10:58:36.204952835Z"
    }
   },
   "outputs": [],
   "source": [
    "redo_embedding = False # recalculate embeddings\n",
    "fast = False # True if use reduced dataset (1000 obs) vs. False if full dataset (40000 obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:58:37.885660702Z",
     "start_time": "2023-06-03T10:58:37.836968179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 18.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load data\n",
    "\n",
    "# OLD approach ------------------------\n",
    "# if fast:\n",
    "#     df_original = df.copy()\n",
    "#     df = df.sample(frac=1).reset_index(drop=True)[:1000]\n",
    "#\n",
    "# if redo_embedding:\n",
    "#     X = df['text'].tolist()\n",
    "#     y = df['label'].tolist()\n",
    "#\n",
    "#     with open(\"X\", \"wb\") as fp:\n",
    "#       pickle.dump(X, fp)\n",
    "#     with open(\"y\", \"wb\") as fp:\n",
    "#       pickle.dump(y, fp)\n",
    "# elif fast:\n",
    "#     with open(\"../../data/small/X\", \"rb\") as fp:\n",
    "#       X = pickle.load(fp)\n",
    "#     with open(\"../../data/small/y\", \"rb\") as fp:\n",
    "#       y = pickle.load(fp)\n",
    "# else:\n",
    "#     with open(\"../../data/X\", \"rb\") as fp:\n",
    "#       X = pickle.load(fp)\n",
    "#     with open(\"../../data/y\", \"rb\") as fp:\n",
    "#       y = pickle.load(fp)\n",
    "#\n",
    "# if redo_embedding:\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "#     with open(\"X_train\", \"wb\") as fp:\n",
    "#       pickle.dump(X_train, fp)\n",
    "#     with open(\"X_test\", \"wb\") as fp:\n",
    "#       pickle.dump(X_test, fp)\n",
    "#     with open(\"y_train\", \"wb\") as fp:\n",
    "#       pickle.dump(y_train, fp)\n",
    "#     with open(\"y_test\", \"wb\") as fp:\n",
    "#       pickle.dump(y_test, fp)\n",
    "# elif fast:\n",
    "#     with open(\"../../data/small/X_train\", \"rb\") as fp:\n",
    "#       X_train = pickle.load(fp)\n",
    "#     with open(\"../../data/small/X_test\", \"rb\") as fp:\n",
    "#       X_test = pickle.load(fp)\n",
    "#     with open(\"../../data/small/y_train\", \"rb\") as fp:\n",
    "#       y_train = pickle.load(fp)\n",
    "#     with open(\"../../data/small/y_test\", \"rb\") as fp:\n",
    "#       y_test = pickle.load(fp)\n",
    "# else:\n",
    "#     with open(\"../../data/X_train\", \"rb\") as fp:\n",
    "#       X_train = pickle.load(fp)\n",
    "#     with open(\"../../data/X_test\", \"rb\") as fp:\n",
    "#       X_test = pickle.load(fp)\n",
    "#     with open(\"../../data/y_train\", \"rb\") as fp:\n",
    "#       y_train = pickle.load(fp)\n",
    "#     with open(\"../../data/y_test\", \"rb\") as fp:\n",
    "#       y_test = pickle.load(fp)\n",
    "\n",
    "\n",
    "# NEW approach ------------------------\n",
    "if fast:\n",
    "    with open(\"../../data/small/X_train\", \"rb\") as fp:\n",
    "      X_train = pickle.load(fp)\n",
    "    with open(\"../../data/small/X_test\", \"rb\") as fp:\n",
    "      X_test = pickle.load(fp)\n",
    "    with open(\"../../data/small/y_train\", \"rb\") as fp:\n",
    "      y_train = pickle.load(fp)\n",
    "    with open(\"../../data/small/y_test\", \"rb\") as fp:\n",
    "      y_test = pickle.load(fp)\n",
    "else:\n",
    "    with open(\"../../data/X_train\", \"rb\") as fp:\n",
    "      X_train = pickle.load(fp)\n",
    "    with open(\"../../data/X_test\", \"rb\") as fp:\n",
    "      X_test = pickle.load(fp)\n",
    "    with open(\"../../data/y_train\", \"rb\") as fp:\n",
    "      y_train = pickle.load(fp)\n",
    "    with open(\"../../data/y_test\", \"rb\") as fp:\n",
    "      y_test = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqQEmeGREcUg"
   },
   "source": [
    "## BERT Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:58:39.736095366Z",
     "start_time": "2023-06-03T10:58:38.605767163Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pbSRaebguY2a",
    "outputId": "a3fc1906-e2a4-4a92-8d1c-a656f2fdd828",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if redo_embedding:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    bert = AutoModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    bert.to(device)\n",
    "\n",
    "    def _get_bert_embedding(text):\n",
    "        input_ids = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=512)\n",
    "        input_ids = np.array(input_ids)\n",
    "        input_ids = np.expand_dims(input_ids, axis=0)\n",
    "        input_ids = torch.tensor(input_ids).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = bert(input_ids)\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            last_hidden_state = last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "        return last_hidden_state\n",
    "\n",
    "    print(\"TRAIN\")\n",
    "    X_train_embeddings = []\n",
    "    for text in tqdm(X_train):\n",
    "        embedding = _get_bert_embedding(text)\n",
    "        X_train_embeddings.append(embedding)\n",
    "    X_train_embeddings = np.array(X_train_embeddings)\n",
    "    X_train_embeddings_bert = np.squeeze(X_train_embeddings, axis=1)\n",
    "\n",
    "    print(\"TEST\")\n",
    "    X_test_embeddings = []\n",
    "    for text in tqdm(X_test):\n",
    "        embedding = _get_bert_embedding(text)\n",
    "        X_test_embeddings.append(embedding)\n",
    "    X_test_embeddings = np.array(X_test_embeddings)\n",
    "    X_test_embeddings_bert = np.squeeze(X_test_embeddings, axis=1)\n",
    "    \n",
    "    if fast:\n",
    "        pd.DataFrame(X_train_embeddings_bert).to_csv(\"../../data/small/embeddings/X_train_embeddings_bert_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "        pd.DataFrame(X_test_embeddings_bert).to_csv(\"../../data/small/embeddings/X_test_embeddings_bert_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "    else:\n",
    "        pd.DataFrame(X_train_embeddings_bert).to_csv(\"../../data/embeddings/X_train_embeddings_bert_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "        pd.DataFrame(X_test_embeddings_bert).to_csv(\"../../data/embeddings/X_test_embeddings_bert_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "    \n",
    "elif fast:\n",
    "    X_train_embeddings_bert = pd.read_csv('../../data/small/embeddings/X_train_embeddings_bert.csv', sep=',', header=None).values\n",
    "    X_test_embeddings_bert = pd.read_csv('../../data/small/embeddings/X_test_embeddings_bert.csv', sep=',', header=None).values\n",
    "else:\n",
    "    X_train_embeddings_bert = pd.read_csv('../../data/embeddings/X_train_embeddings_bert.csv', sep=',', header=None).values\n",
    "    X_test_embeddings_bert = pd.read_csv('../../data/embeddings/X_test_embeddings_bert.csv', sep=',', header=None).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhGA1YV7GaKD"
   },
   "source": [
    "## GloVe Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:58:40.551452456Z",
     "start_time": "2023-06-03T10:58:39.743173091Z"
    }
   },
   "outputs": [],
   "source": [
    "if redo_embedding:\n",
    "\n",
    "    def load_glove_embeddings(filename):\n",
    "        embeddings_index = {}\n",
    "        with open(filename) as f:\n",
    "            for line in tqdm(f):\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                if len(values[1:]) == 300:\n",
    "                    coefs = np.asarray(values[1:], dtype='float32')\n",
    "                    embeddings_index[word] = coefs\n",
    "        return embeddings_index\n",
    "\n",
    "    glove_embeddings = load_glove_embeddings('../../glove/glove.840B.300d.txt')\n",
    "\n",
    "    def text_to_glove_embeddings(text, embeddings_index, embedding_dim):\n",
    "        embeddings = []\n",
    "        for sentence in text:\n",
    "            sentence_embeddings = []\n",
    "            for word in sentence.split():\n",
    "                if word in embeddings_index:\n",
    "                    sentence_embeddings.append(embeddings_index[word])\n",
    "            if len(sentence_embeddings) > 0:\n",
    "                embeddings.append(np.mean(sentence_embeddings, axis=0))\n",
    "            else:\n",
    "                embeddings.append(np.zeros(embedding_dim))\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    X_train_embeddings_glove = text_to_glove_embeddings(X_train, glove_embeddings, embedding_dim=300)\n",
    "    X_test_embeddings_glove = text_to_glove_embeddings(X_test, glove_embeddings, embedding_dim=300)\n",
    "    \n",
    "    if fast:\n",
    "        pd.DataFrame(X_train_embeddings_glove).to_csv(\"../../data/small/embeddings/X_train_embeddings_glove_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "        pd.DataFrame(X_test_embeddings_glove).to_csv(\"../../data/small/embeddings/X_test_embeddings_glove_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "    else:\n",
    "        pd.DataFrame(X_train_embeddings_glove).to_csv(\"../../data/embeddings/X_train_embeddings_glove_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "        pd.DataFrame(X_test_embeddings_glove).to_csv(\"../../data/embeddings/X_test_embeddings_glove_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "\n",
    "elif fast:\n",
    "    X_train_embeddings_glove = pd.read_csv('../../data/small/embeddings/X_train_embeddings_glove.csv', sep=',', header=None).values\n",
    "    X_test_embeddings_glove = pd.read_csv('../../data/small/embeddings/X_test_embeddings_glove.csv', sep=',', header=None).values\n",
    "\n",
    "else:\n",
    "    X_train_embeddings_glove = pd.read_csv('../../data/embeddings/X_train_embeddings_glove.csv', sep=',', header=None).values\n",
    "    X_test_embeddings_glove = pd.read_csv('../../data/embeddings/X_test_embeddings_glove.csv', sep=',', header=None).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:58:41.963639819Z",
     "start_time": "2023-06-03T10:58:41.271486876Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if redo_embedding:\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format('../../word2vec/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "def get_word2vec_embeddings(text):\n",
    "    embeddings = []\n",
    "    for sentence in tqdm(text):\n",
    "        tokens = sentence.split()\n",
    "        doc_vecs = [model[token] for token in tokens if token in model.key_to_index]\n",
    "        if len(doc_vecs) > 0:\n",
    "            doc_vec = np.mean(doc_vecs, axis=0)\n",
    "            embeddings.append(doc_vec)\n",
    "        else:\n",
    "            embeddings.append([0] * 300) # if vocabulary does not exist in Word2Vec append a vector of zeros\n",
    "    return np.array(embeddings)\n",
    "\n",
    "\n",
    "if redo_embedding:\n",
    "    # noinspection PyUnboundLocalVariable\n",
    "    X_train_embeddings_word2vec = get_word2vec_embeddings(X_train)\n",
    "    X_test_embeddings_word2vec = get_word2vec_embeddings(X_test)\n",
    "\n",
    "    if fast:\n",
    "        pd.DataFrame(X_train_embeddings_word2vec).to_csv(\"../../data/small/embeddings/X_train_embeddings_word2vec_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "        pd.DataFrame(X_test_embeddings_word2vec).to_csv(\"../../data/small/embeddings/X_test_embeddings_word2vec_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "    else:\n",
    "        pd.DataFrame(X_train_embeddings_word2vec).to_csv(\"../../data/embeddings/X_train_embeddings_word2vec_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "        pd.DataFrame(X_test_embeddings_word2vec).to_csv(\"../../data/embeddings/X_test_embeddings_word2vec_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "\n",
    "elif fast:\n",
    "    X_train_embeddings_word2vec = pd.read_csv('../../data/small/embeddings/X_train_embeddings_word2vec.csv', sep=',', header=None).values\n",
    "    X_test_embeddings_word2vec = pd.read_csv('../../data/small/embeddings/X_test_embeddings_word2vec.csv', sep=',', header=None).values\n",
    "\n",
    "else:\n",
    "    X_train_embeddings_word2vec = pd.read_csv('../../data/embeddings/X_train_embeddings_word2vec.csv', sep=',', header=None).values\n",
    "    X_test_embeddings_word2vec = pd.read_csv('../../data/embeddings/X_test_embeddings_word2vec.csv', sep=',', header=None).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:58:43.264496653Z",
     "start_time": "2023-06-03T10:58:42.024218748Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if redo_embedding:\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\", output_hidden_states=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    gpt2.to(device)\n",
    "\n",
    "    def _get_gpt2_embedding(text):\n",
    "        input_ids = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=1024)\n",
    "        input_ids = torch.tensor(input_ids).unsqueeze(0).to(device).long()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = gpt2.transformer.wte(input_ids)\n",
    "            mean_embedding = embeddings.mean(dim=1)\n",
    "        #     outputs = bert(input_ids)\n",
    "        #     last_hidden_state = outputs.last_hidden_state\n",
    "        #     last_hidden_state = last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "            #vector = gpt2.transformer.wte.weight[input_ids,:]\n",
    "        mean_embedding = mean_embedding.cpu().numpy()\n",
    "        return mean_embedding\n",
    "\n",
    "    print(\"TRAIN\")\n",
    "    X_train_embeddings = []\n",
    "    for text in tqdm(X_train):\n",
    "        embedding = _get_gpt2_embedding(text)\n",
    "        X_train_embeddings.append(embedding)\n",
    "    X_train_embeddings = np.array(X_train_embeddings)\n",
    "    X_train_embeddings_gpt2 = np.squeeze(X_train_embeddings, axis=1)\n",
    "\n",
    "    print(\"TEST\")\n",
    "    X_test_embeddings = []\n",
    "    for text in tqdm(X_test):\n",
    "        embedding = _get_gpt2_embedding(text)\n",
    "        X_test_embeddings.append(embedding)\n",
    "    X_test_embeddings = np.array(X_test_embeddings)\n",
    "    X_test_embeddings_gpt2 = np.squeeze(X_test_embeddings, axis=1)\n",
    "\n",
    "    if fast:\n",
    "        pd.DataFrame(X_train_embeddings_gpt2).to_csv(\"../../data/small/embeddings/X_train_embeddings_gpt2_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "        pd.DataFrame(X_test_embeddings_gpt2).to_csv(\"../../data/small/embeddings/X_test_embeddings_gpt2_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "    else:\n",
    "        pd.DataFrame(X_train_embeddings_gpt2).to_csv(\"../../data/embeddings/X_train_embeddings_gpt2_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "        pd.DataFrame(X_test_embeddings_gpt2).to_csv(\"../../data/embeddings/X_test_embeddings_gpt2_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "\n",
    "elif fast:\n",
    "    X_train_embeddings_gpt2 = pd.read_csv('../../data/small/embeddings/X_train_embeddings_gpt2.csv', sep=',', header=None).values\n",
    "    X_test_embeddings_gpt2 = pd.read_csv('../../data/small/embeddings/X_test_embeddings_gpt2.csv', sep=',', header=None).values\n",
    "else:\n",
    "    X_train_embeddings_gpt2 = pd.read_csv('../../data/embeddings/X_train_embeddings_gpt2.csv', sep=',', header=None).values\n",
    "    X_test_embeddings_gpt2 = pd.read_csv('../../data/embeddings/X_test_embeddings_gpt2.csv', sep=',', header=None).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:58:44.748419263Z",
     "start_time": "2023-06-03T10:58:43.265323467Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if redo_embedding:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "    roberta = AutoModel.from_pretrained(\"roberta-base\", output_hidden_states=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    roberta.to(device)\n",
    "\n",
    "    def _get_roberta_embedding(text):\n",
    "        input_ids = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=512)\n",
    "        input_ids = np.array(input_ids)\n",
    "        input_ids = np.expand_dims(input_ids, axis=0)\n",
    "        input_ids = torch.tensor(input_ids).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = roberta(input_ids)\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            last_hidden_state = last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "        return last_hidden_state\n",
    "\n",
    "    print(\"TRAIN\")\n",
    "    X_train_embeddings = []\n",
    "    for text in tqdm(X_train):\n",
    "        embedding = _get_roberta_embedding(text)\n",
    "        X_train_embeddings.append(embedding)\n",
    "    X_train_embeddings = np.array(X_train_embeddings)\n",
    "    X_train_embeddings_roberta = np.squeeze(X_train_embeddings, axis=1)\n",
    "\n",
    "    print(\"TEST\")\n",
    "    X_test_embeddings = []\n",
    "    for text in tqdm(X_test):\n",
    "        embedding = _get_roberta_embedding(text)\n",
    "        X_test_embeddings.append(embedding)\n",
    "    X_test_embeddings = np.array(X_test_embeddings)\n",
    "    X_test_embeddings_roberta = np.squeeze(X_test_embeddings, axis=1)\n",
    "\n",
    "    if fast:\n",
    "        pd.DataFrame(X_train_embeddings_roberta).to_csv(\"../../data/small/embeddings/X_train_embeddings_roberta_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "        pd.DataFrame(X_test_embeddings_roberta).to_csv(\"../../data/small/embeddings/X_test_embeddings_roberta_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "    else:\n",
    "        pd.DataFrame(X_train_embeddings_roberta).to_csv(\"../../data/embeddings/X_train_embeddings_roberta_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "        pd.DataFrame(X_test_embeddings_roberta).to_csv(\"../../data/embeddings/X_test_embeddings_roberta_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "\n",
    "elif fast:\n",
    "    X_train_embeddings_roberta = pd.read_csv('../../data/small/embeddings/X_train_embeddings_roberta.csv', sep=',', header=None).values\n",
    "    X_test_embeddings_roberta = pd.read_csv('../../data/small/embeddings/X_test_embeddings_roberta.csv', sep=',', header=None).values\n",
    "else:\n",
    "    X_train_embeddings_roberta = pd.read_csv('../../data/embeddings/X_train_embeddings_roberta.csv', sep=',', header=None).values\n",
    "    X_test_embeddings_roberta = pd.read_csv('../../data/embeddings/X_test_embeddings_roberta.csv', sep=',', header=None).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMvpJtV-EiIo"
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:58:52.817302627Z",
     "start_time": "2023-06-03T10:58:52.715551820Z"
    }
   },
   "outputs": [],
   "source": [
    "class KNNClassifier:\n",
    "    def __init__(self, n_neighbors=2, weights='uniform', metric='minkowski'):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.weights = weights\n",
    "        self.metric = metric\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model = KNeighborsClassifier(n_neighbors=self.n_neighbors, weights=self.weights, metric=self.metric)\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        if self.model is None:\n",
    "          raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        return y_pred\n",
    "\n",
    "    def randomized_search(self, X_train, y_train, param_distributions, cv=5, n_iter=20):\n",
    "        self.model = KNeighborsClassifier()\n",
    "        random_search = RandomizedSearchCV(self.model, param_distributions=param_distributions, cv=cv, n_iter=n_iter, scoring=\"f1\", verbose=3)\n",
    "        random_search.fit(X_train, y_train)\n",
    "\n",
    "        print('Best parameters:', random_search.best_params_, '\\n')\n",
    "        self.n_neighbors = random_search.best_params_['n_neighbors']\n",
    "        self.weights = random_search.best_params_['weights']\n",
    "        self.metric = random_search.best_params_['metric']\n",
    "\n",
    "        self.model = KNeighborsClassifier(n_neighbors=self.n_neighbors, weights=self.weights, metric=self.metric)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        if self.model is None:\n",
    "              raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "        y_pred = self.predict(X_test)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        accuracy = round(accuracy_score(y_test, y_pred)*100,1)\n",
    "        f1 = round(f1_score(y_test, y_pred)*100,1)\n",
    "        balanced_accuracy = round(balanced_accuracy_score(y_test, y_pred)*100,1)\n",
    "        auc = round(roc_auc_score(y_test, y_pred)*100, 1)\n",
    "\n",
    "        print('\\nConfusion matrix\\n', cm, '\\n')\n",
    "        print('Accuracy:', accuracy, '\\n')\n",
    "        print('F1 Score:', f1, '\\n')\n",
    "        print('Balanced accuracy:', f1, '\\n')\n",
    "        print('AUC Score:', auc, '\\n')\n",
    "\n",
    "        return cm, accuracy, f1, balanced_accuracy, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:58:56.291014245Z",
     "start_time": "2023-06-03T10:58:56.265756686Z"
    }
   },
   "outputs": [],
   "source": [
    "class XGBoostClassifier:\n",
    "    def __init__(self, learning_rate=0.1, max_depth=5, min_child_weight=1, subsample=0.5, colsample_bytree=0.5, n_estimators=100, objective='req:squarederror'):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.subsample = subsample\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.n_estimators = n_estimators\n",
    "        self.objective = objective\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model = xgb.XGBClassifier(learning_rate=self.learning_rate, max_depth=self.max_depth, min_child_weight=self.min_child_weight, subsample=self.subsample, colsample_bytree=self.colsample_bytree, n_estimators=self.n_estimators, objective=self.objective)\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        if self.model is None:\n",
    "          raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "        y_pred = self.model.predict(X_test)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def randomized_search(self, X_train, y_train, param_distributions, cv=5, n_iter=20):\n",
    "        self.model = xgb.XGBClassifier()\n",
    "        random_search = RandomizedSearchCV(self.model, param_distributions=param_distributions, cv=cv, n_iter=n_iter, scoring=\"f1\", verbose=3)\n",
    "        random_search.fit(X_train, y_train)\n",
    "\n",
    "        print('Best parameters:', random_search.best_params_, '\\n')\n",
    "        self.learning_rate = random_search.best_params_['learning_rate']\n",
    "        self.max_depth = random_search.best_params_['max_depth']\n",
    "        self.min_child_weight = random_search.best_params_['min_child_weight']\n",
    "        self.subsample = random_search.best_params_['subsample']\n",
    "        self.colsample_bytree = random_search.best_params_['colsample_bytree']\n",
    "        self.n_estimators = random_search.best_params_['n_estimators']\n",
    "        self.objective = random_search.best_params_['objective']\n",
    "\n",
    "        self.model = xgb.XGBClassifier(learning_rate=self.learning_rate, max_depth=self.max_depth, min_child_weight=self.min_child_weight, subsample=self.subsample, colsample_bytree=self.colsample_bytree, n_estimators=self.n_estimators, objective=self.objective)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        if self.model is None:\n",
    "              raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "        y_pred = self.predict(X_test)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        accuracy = round(accuracy_score(y_test, y_pred)*100,1)\n",
    "        f1 = round(f1_score(y_test, y_pred)*100,1)\n",
    "        balanced_accuracy = round(balanced_accuracy_score(y_test, y_pred)*100,1)\n",
    "        auc = round(roc_auc_score(y_test, y_pred)*100, 1)\n",
    "\n",
    "        print('\\nConfusion matrix\\n', cm, '\\n')\n",
    "        print('Accuracy:', accuracy, '\\n')\n",
    "        print('F1 Score:', f1, '\\n')\n",
    "        print('Balanced accuracy:', f1, '\\n')\n",
    "        print('AUC Score:', auc, '\\n')\n",
    "\n",
    "        return cm, accuracy, f1, balanced_accuracy, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:58:57.504449313Z",
     "start_time": "2023-06-03T10:58:57.481635399Z"
    }
   },
   "outputs": [],
   "source": [
    "class RFClassifier:\n",
    "    def __init__(self, n_estimators=100, max_features='sqrt', max_depth='none', bootstrap=True):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.max_depth = max_depth\n",
    "        self.bootstrap = bootstrap\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model = RandomForestClassifier(n_estimators=self.n_estimators, max_features=self.max_features, max_depth=self.max_depth, bootstrap=self.bootstrap, verbose=True)\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        if self.model is None:\n",
    "          raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "        y_pred = self.model.predict(X_test)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def randomized_search(self, X_train, y_train, param_distributions, cv=5, n_iter=20):\n",
    "        self.model = RandomForestClassifier()\n",
    "        random_search = RandomizedSearchCV(self.model, param_distributions=param_distributions, cv=cv, n_iter=n_iter, scoring=\"f1\", verbose=3)\n",
    "        random_search.fit(X_train, y_train)\n",
    "\n",
    "        print('Best parameters:', random_search.best_params_, '\\n')\n",
    "        self.n_estimators = random_search.best_params_['n_estimators']\n",
    "        self.max_features = random_search.best_params_['max_features']\n",
    "        self.max_depth = random_search.best_params_['max_depth']\n",
    "        self.bootstrap = random_search.best_params_['bootstrap']\n",
    "\n",
    "        self.model = RandomForestClassifier(n_estimators=self.n_estimators, max_features=self.max_features, max_depth=self.max_depth, bootstrap=self.bootstrap)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        if self.model is None:\n",
    "              raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "        y_pred = self.predict(X_test)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        accuracy = round(accuracy_score(y_test, y_pred)*100,1)\n",
    "        f1 = round(f1_score(y_test, y_pred)*100,1)\n",
    "        balanced_accuracy = round(balanced_accuracy_score(y_test, y_pred)*100,1)\n",
    "        auc = round(roc_auc_score(y_test, y_pred)*100, 1)\n",
    "\n",
    "        print('\\nConfusion matrix\\n', cm, '\\n')\n",
    "        print('Accuracy:', accuracy, '\\n')\n",
    "        print('F1 Score:', f1, '\\n')\n",
    "        print('Balanced accuracy:', f1, '\\n')\n",
    "        print('AUC Score:', auc, '\\n')\n",
    "\n",
    "        return cm, accuracy, f1, balanced_accuracy, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:58:58.556356969Z",
     "start_time": "2023-06-03T10:58:58.476178251Z"
    }
   },
   "outputs": [],
   "source": [
    "class SVClassifier:\n",
    "    def __init__(self, C = 1, kernel='linear', gamma = 0.2):\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.gamma = gamma\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model = svm.SVC(C=self.C, kernel=self.kernel, gamma=self.gamma, verbose=True)\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        if self.model is None:\n",
    "          raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "        y_pred = self.model.predict(X_test)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def randomized_search(self, X_train, y_train, param_distributions, cv=5, n_iter=20):\n",
    "        self.model = svm.SVC()\n",
    "        random_search = RandomizedSearchCV(self.model, param_distributions=param_distributions, cv=cv, n_iter=n_iter, scoring=\"f1\", verbose=3)\n",
    "        random_search.fit(X_train, y_train)\n",
    "\n",
    "        print('Best parameters:', random_search.best_params_, '\\n')\n",
    "        self.C = random_search.best_params_['C']\n",
    "        self.kernel = random_search.best_params_['kernel']\n",
    "        self.gamma = random_search.best_params_['gamma']\n",
    "\n",
    "        self.model = svm.SVC(C=self.C, kernel=self.kernel, gamma=self.gamma)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        if self.model is None:\n",
    "              raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "        y_pred = self.predict(X_test)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        accuracy = round(accuracy_score(y_test, y_pred)*100,1)\n",
    "        f1 = round(f1_score(y_test, y_pred)*100,1)\n",
    "        balanced_accuracy = round(balanced_accuracy_score(y_test, y_pred)*100,1)\n",
    "        auc = round(roc_auc_score(y_test, y_pred)*100, 1)\n",
    "\n",
    "        print('\\nConfusion matrix\\n', cm, '\\n')\n",
    "        print('Accuracy:', accuracy, '\\n')\n",
    "        print('F1 Score:', f1, '\\n')\n",
    "        print('Balanced accuracy:', f1, '\\n')\n",
    "        print('AUC Score:', auc, '\\n')\n",
    "\n",
    "        return cm, accuracy, f1, balanced_accuracy, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:59:00.687298447Z",
     "start_time": "2023-06-03T10:59:00.621633385Z"
    }
   },
   "outputs": [],
   "source": [
    "class LRClassifier:\n",
    "    def __init__(self, penalty = 'l2', solver = 'libinear', C = 0.5):\n",
    "        self.penalty = penalty\n",
    "        self.solver = solver\n",
    "        self.C = C\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model = LogisticRegression(penalty=self.penalty, solver=self.solver, C=self.C)\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        if self.model is None:\n",
    "          raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "        y_pred = self.model.predict(X_test)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def randomized_search(self, X_train, y_train, param_distributions, cv=5, n_iter=20):\n",
    "        self.model = LogisticRegression()\n",
    "        random_search = RandomizedSearchCV(self.model, param_distributions=param_distributions, cv=cv, n_iter=n_iter, scoring=\"f1\", verbose=3)\n",
    "        random_search.fit(X_train, y_train)\n",
    "\n",
    "        print('Best parameters:', random_search.best_params_, '\\n')\n",
    "        self.penalty = random_search.best_params_['penalty']\n",
    "        self.solver = random_search.best_params_['solver']\n",
    "        self.C = random_search.best_params_['C']\n",
    "\n",
    "        self.model = LogisticRegression(penalty=self.penalty, solver=self.solver, C=self.C)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        if self.model is None:\n",
    "              raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "        y_pred = self.predict(X_test)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        accuracy = round(accuracy_score(y_test, y_pred)*100,1)\n",
    "        f1 = round(f1_score(y_test, y_pred)*100,1)\n",
    "        balanced_accuracy = round(balanced_accuracy_score(y_test, y_pred)*100,1)\n",
    "        auc = round(roc_auc_score(y_test, y_pred)*100, 1)\n",
    "\n",
    "        print('\\nConfusion matrix\\n', cm, '\\n')\n",
    "        print('Accuracy:', accuracy, '\\n')\n",
    "        print('F1 Score:', f1, '\\n')\n",
    "        print('Balanced accuracy:', f1, '\\n')\n",
    "        print('AUC Score:', auc, '\\n')\n",
    "\n",
    "        return cm, accuracy, f1, balanced_accuracy, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:59:01.711727205Z",
     "start_time": "2023-06-03T10:59:01.670615767Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class FakeNewsClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, dropout):\n",
    "        super(FakeNewsClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.relu(self.fc1(x))\n",
    "        hidden = self.dropout(hidden)\n",
    "        output = self.fc2(hidden)\n",
    "        return output\n",
    "\n",
    "class NeuralNetworkClassifier:\n",
    "    def __init__(self, input_dim, hidden_dim, dropout, output_dim=2):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val, patience, num_epochs=10, lr=0.001):\n",
    "        self.model = None\n",
    "\n",
    "        self.model = FakeNewsClassifier(self.input_dim, self.hidden_dim, self.output_dim, self.dropout)\n",
    "        self.model = self.model.float()\n",
    "\n",
    "        class_counts = np.bincount(y_train)\n",
    "        num_classes = len(class_counts)\n",
    "        class_weights = torch.tensor(class_counts.sum() / (num_classes * class_counts), dtype=torch.float)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        early_stop_counter = 0\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        f1_scores = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = 0.0\n",
    "\n",
    "            self.model.train()\n",
    "\n",
    "            for embedding, label in zip(X_train, y_train):\n",
    "                embedding_tensor = torch.from_numpy(embedding).float().unsqueeze(0)\n",
    "                label_tensor = torch.tensor([label])\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(embedding_tensor)\n",
    "                loss = criterion(outputs, label_tensor)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = train_loss / len(X_train)\n",
    "\n",
    "            self.model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0.0\n",
    "\n",
    "                for embedding, label in zip(X_val, y_val):\n",
    "                    embedding_tensor = torch.from_numpy(embedding).float().unsqueeze(0)\n",
    "                    label_tensor = torch.tensor([label])\n",
    "                    outputs = self.model(embedding_tensor)\n",
    "                    loss = criterion(outputs, label_tensor)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                avg_val_loss = val_loss / len(X_val)\n",
    "\n",
    "                train_losses.append(avg_train_loss)\n",
    "                val_losses.append(avg_val_loss)\n",
    "\n",
    "                y_pred = self.predict(X_val)\n",
    "                f1 = f1_score(y_val, y_pred)\n",
    "                f1_scores.append(f1)\n",
    "\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | F1 Score: {f1:.4f} | Balanced Accuracy: {balanced_accuracy_score(y_val, y_pred):.4f} | AUC: {roc_auc_score(y_val, y_pred):.4f}\")\n",
    "\n",
    "                if avg_val_loss < best_loss:\n",
    "                    best_loss = avg_val_loss\n",
    "                    early_stop_counter = 0\n",
    "                else:\n",
    "                    early_stop_counter += 1\n",
    "                    if early_stop_counter >= patience:\n",
    "                        print(f\"Early stopping triggered. No improvement in {patience} epochs.\")\n",
    "                        break\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        epochs = np.arange(1, len(train_losses) + 1)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, train_losses, label='Train Loss')\n",
    "        plt.plot(epochs, val_losses, label='Val Loss')\n",
    "        plt.plot(epochs, f1_scores, label='F1 Score')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')  # Adding the y-axis label\n",
    "        plt.legend()  # Adding the legend\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "        test_inputs = torch.from_numpy(X_test).float()\n",
    "        predictions = self.model(test_inputs)\n",
    "        predicted_classes = torch.argmax(predictions, dim=1)\n",
    "        predicted_classes = predicted_classes.numpy()\n",
    "        return predicted_classes\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "        y_pred = self.predict(X_test)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        accuracy = round(accuracy_score(y_test, y_pred) * 100, 1)\n",
    "        f1 = round(f1_score(y_test, y_pred) * 100, 1)\n",
    "        balanced_accuracy = round(balanced_accuracy_score(y_test, y_pred) * 100, 1)\n",
    "        auc = round(roc_auc_score(y_test, y_pred) * 100, 1)\n",
    "\n",
    "        print('\\nConfusion matrix\\n', cm, '\\n')\n",
    "        print('Accuracy:', accuracy, '\\n')\n",
    "        print('F1 Score:', f1, '\\n')\n",
    "        print('Balanced accuracy:', balanced_accuracy, '\\n')\n",
    "        print('AUC Score:', auc, '\\n')\n",
    "\n",
    "        return cm, accuracy, f1, balanced_accuracy, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLD NN Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:59:02.981138131Z",
     "start_time": "2023-06-03T10:59:02.952267725Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# class FakeNewsClassifier(nn.Module):\n",
    "#     def __init__(self, embedding_dim, hidden_dim, output_dim):\n",
    "#         super(FakeNewsClassifier, self).__init__()\n",
    "#         self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "#         self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "#         self.relu = nn.ReLU()\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         hidden = self.relu(self.fc1(x))\n",
    "#         output = self.fc2(hidden)\n",
    "#         return output\n",
    "#\n",
    "# class NeuralNetworkClassifier:\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim=2):\n",
    "#         self.input_dim = input_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.output_dim = output_dim\n",
    "#         self.model = None\n",
    "#\n",
    "#     def fit(self, X_train, y_train, num_epochs=10, lr=0.001):\n",
    "#         self.model = FakeNewsClassifier(self.input_dim, self.hidden_dim, self.output_dim)\n",
    "#         self.model = self.model.double()\n",
    "#\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#         optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "#\n",
    "#         for _ in tqdm(range(num_epochs), desc=\"Epoch\"):\n",
    "#             for embedding, label in zip(X_train, y_train):\n",
    "#                 embedding_tensor = torch.from_numpy(embedding).double().unsqueeze(0)\n",
    "#                 label_tensor = torch.tensor([label])\n",
    "#                 optimizer.zero_grad()\n",
    "#                 outputs = self.model(embedding_tensor)\n",
    "#                 loss = criterion(outputs, label_tensor)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#\n",
    "#     def predict(self, X_test):\n",
    "#         if self.model is None:\n",
    "#             raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "#         test_inputs = torch.from_numpy(X_test).double()\n",
    "#         predictions = self.model(test_inputs)\n",
    "#         predicted_classes = torch.argmax(predictions, dim=1)\n",
    "#         predicted_classes = predicted_classes.numpy()\n",
    "#         return predicted_classes\n",
    "#\n",
    "#     def evaluate(self, X_test, y_test):\n",
    "#         if self.model is None:\n",
    "#             raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "#         y_pred = self.predict(X_test)\n",
    "#\n",
    "#         cm = confusion_matrix(y_test, y_pred)\n",
    "#         accuracy = round(accuracy_score(y_test, y_pred)*100, 1)\n",
    "#         f1 = round(f1_score(y_test, y_pred)*100, 1)\n",
    "#         balanced_accuracy = round(balanced_accuracy_score(y_test, y_pred)*100,1)\n",
    "#         auc = round(roc_auc_score(y_test, y_pred)*100, 1)\n",
    "#\n",
    "#         print('\\nConfusion matrix\\n', cm, '\\n')\n",
    "#         print('Accuracy:', accuracy, '\\n')\n",
    "#         print('F1 Score:', f1, '\\n')\n",
    "#         print('Balanced accuracy:', f1, '\\n')\n",
    "#         print('AUC Score:', auc, '\\n')\n",
    "#\n",
    "#         return cm, accuracy, f1, balanced_accuracy, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlaJrgisGNfg"
   },
   "source": [
    "## BERT embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMOZaIWp0_oy"
   },
   "source": [
    "### BERT + KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T11:10:35.608071393Z",
     "start_time": "2023-06-03T10:59:05.171551719Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "7HzI3FV7zy6t",
    "outputId": "8ca1830c-a309-4f27-f6dd-cee648ee0978"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=2, weights=uniform;, score=0.352 total time=   0.3s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=2, weights=uniform;, score=0.363 total time=   0.3s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=2, weights=uniform;, score=0.357 total time=   0.3s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=2, weights=uniform;, score=0.316 total time=   0.3s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=2, weights=uniform;, score=0.333 total time=   0.3s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=3, weights=uniform;, score=0.520 total time=   7.7s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=3, weights=uniform;, score=0.514 total time=   7.6s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=3, weights=uniform;, score=0.512 total time=   7.8s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=3, weights=uniform;, score=0.471 total time=   7.5s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=3, weights=uniform;, score=0.487 total time=   7.5s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=7, weights=distance;, score=0.515 total time=   0.2s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=7, weights=distance;, score=0.520 total time=   0.3s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=7, weights=distance;, score=0.501 total time=   0.2s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=7, weights=distance;, score=0.474 total time=   0.2s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=7, weights=distance;, score=0.477 total time=   0.3s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=6, weights=uniform;, score=0.429 total time=   0.3s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=6, weights=uniform;, score=0.458 total time=   0.3s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=6, weights=uniform;, score=0.436 total time=   0.3s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=6, weights=uniform;, score=0.391 total time=   0.3s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=6, weights=uniform;, score=0.401 total time=   0.3s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=3, weights=distance;, score=0.519 total time=   0.3s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=3, weights=distance;, score=0.499 total time=   0.2s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=3, weights=distance;, score=0.507 total time=   0.3s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=3, weights=distance;, score=0.480 total time=   0.2s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=3, weights=distance;, score=0.477 total time=   0.3s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=6, weights=distance;, score=0.522 total time=   0.2s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=6, weights=distance;, score=0.526 total time=   0.3s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=6, weights=distance;, score=0.501 total time=   0.2s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=6, weights=distance;, score=0.494 total time=   0.3s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=6, weights=distance;, score=0.473 total time=   0.2s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=7, weights=uniform;, score=0.512 total time=   0.3s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=7, weights=uniform;, score=0.519 total time=   0.3s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=7, weights=uniform;, score=0.501 total time=   0.3s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=7, weights=uniform;, score=0.474 total time=   0.3s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=7, weights=uniform;, score=0.476 total time=   0.3s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.519 total time=   0.3s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.498 total time=   0.3s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.507 total time=   0.3s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.480 total time=   0.3s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.478 total time=   0.3s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=6, weights=uniform;, score=0.435 total time=   7.5s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=6, weights=uniform;, score=0.431 total time=   7.5s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=6, weights=uniform;, score=0.445 total time=   7.6s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=6, weights=uniform;, score=0.386 total time=   7.6s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=6, weights=uniform;, score=0.419 total time=   7.5s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=9, weights=distance;, score=0.502 total time=   0.3s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=9, weights=distance;, score=0.515 total time=   0.2s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=9, weights=distance;, score=0.486 total time=   0.3s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=9, weights=distance;, score=0.493 total time=   0.3s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=9, weights=distance;, score=0.492 total time=   0.3s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.504 total time=   7.5s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.513 total time=   7.6s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.501 total time=   7.5s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.485 total time=   7.5s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.492 total time=   7.5s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.499 total time=   0.2s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.511 total time=   0.3s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.498 total time=   0.3s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.488 total time=   0.3s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.475 total time=   0.3s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=4, weights=uniform;, score=0.408 total time=   0.3s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=4, weights=uniform;, score=0.409 total time=   0.3s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=4, weights=uniform;, score=0.398 total time=   0.3s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=4, weights=uniform;, score=0.364 total time=   0.3s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=4, weights=uniform;, score=0.384 total time=   0.3s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=9, weights=uniform;, score=0.499 total time=   0.3s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=9, weights=uniform;, score=0.515 total time=   0.3s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=9, weights=uniform;, score=0.485 total time=   0.3s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=9, weights=uniform;, score=0.492 total time=   0.3s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=9, weights=uniform;, score=0.492 total time=   0.3s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=3, weights=distance;, score=0.519 total time=   0.3s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=3, weights=distance;, score=0.499 total time=   0.3s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=3, weights=distance;, score=0.507 total time=   0.2s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=3, weights=distance;, score=0.480 total time=   0.3s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=3, weights=distance;, score=0.477 total time=   0.3s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=8, weights=uniform;, score=0.451 total time=   0.3s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=8, weights=uniform;, score=0.455 total time=   0.3s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=8, weights=uniform;, score=0.427 total time=   0.3s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=8, weights=uniform;, score=0.415 total time=   0.3s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=8, weights=uniform;, score=0.419 total time=   0.3s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=5, weights=distance;, score=0.506 total time=   0.2s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=5, weights=distance;, score=0.517 total time=   0.3s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=5, weights=distance;, score=0.504 total time=   0.3s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=5, weights=distance;, score=0.468 total time=   0.2s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=5, weights=distance;, score=0.487 total time=   0.3s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=4, weights=distance;, score=0.512 total time=   0.3s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=4, weights=distance;, score=0.510 total time=   0.3s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=4, weights=distance;, score=0.498 total time=   0.3s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=4, weights=distance;, score=0.489 total time=   0.2s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=4, weights=distance;, score=0.478 total time=   0.3s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=8, weights=uniform;, score=0.429 total time=   7.5s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=8, weights=uniform;, score=0.463 total time=   7.5s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=8, weights=uniform;, score=0.431 total time=   7.5s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=8, weights=uniform;, score=0.403 total time=   7.5s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=8, weights=uniform;, score=0.420 total time=   7.5s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=7, weights=distance;, score=0.515 total time=   0.3s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=7, weights=distance;, score=0.520 total time=   0.2s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=7, weights=distance;, score=0.501 total time=   0.2s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=7, weights=distance;, score=0.474 total time=   0.3s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=7, weights=distance;, score=0.477 total time=   0.3s\n",
      "Best parameters: {'weights': 'distance', 'n_neighbors': 6, 'metric': 'minkowski'} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[6420    0]\n",
      " [   4 5100]] \n",
      "\n",
      "Accuracy: 100.0 \n",
      "\n",
      "F1 Score: 100.0 \n",
      "\n",
      "Balanced accuracy: 100.0 \n",
      "\n",
      "AUC Score: 100.0 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[447 267]\n",
      " [292 261]] \n",
      "\n",
      "Accuracy: 55.9 \n",
      "\n",
      "F1 Score: 48.3 \n",
      "\n",
      "Balanced accuracy: 48.3 \n",
      "\n",
      "AUC Score: 54.9 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = KNNClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'weights' : ['uniform','distance'],\n",
    "    'metric' : ['minkowski','euclidean','manhattan']\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_bert, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_bert, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_knn_bert_train, accuracy_knn_bert_train, f1_knn_bert_train, balaccuracy_knn_bert_train, rocauc_knn_bert_train = classifier.evaluate(X_train_embeddings_bert, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_knn_bert_test, accuracy_knn_bert_test, f1_knn_bert_test, balaccuracy_knn_bert_test, rocauc_knn_bert_test = classifier.evaluate(X_test_embeddings_bert, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T12:09:11.253497226Z",
     "start_time": "2023-06-03T12:09:11.233274895Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_knn_bert_train_tosave = repr(cm_knn_bert_train)\n",
    "accuracy_knn_bert_train_tosave = repr(accuracy_knn_bert_train)\n",
    "f1_knn_bert_train_tosave = repr(f1_knn_bert_train)\n",
    "balaccuracy_knn_bert_train_tosave = repr(balaccuracy_knn_bert_train)\n",
    "rocauc_knn_bert_train_tosave = repr(rocauc_knn_bert_train)\n",
    "\n",
    "cm_knn_bert_test_tosave = repr(cm_knn_bert_test)\n",
    "accuracy_knn_bert_test_tosave = repr(accuracy_knn_bert_test)\n",
    "f1_knn_bert_test_tosave = repr(f1_knn_bert_test)\n",
    "balaccuracy_knn_bert_test_tosave = repr(balaccuracy_knn_bert_test)\n",
    "rocauc_knn_bert_test_tosave = repr(rocauc_knn_bert_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/knn_bert_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_knn_bert_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_knn_bert_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_knn_bert_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_knn_bert_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_knn_bert_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_knn_bert_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_knn_bert_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_knn_bert_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_knn_bert_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_knn_bert_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bg4FzG2htmwr"
   },
   "source": [
    "### BERT + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T11:44:02.413415076Z",
     "start_time": "2023-06-03T11:10:44.583290995Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "psJLbuc_win3",
    "outputId": "554936c2-e2c5-4b6f-df80-f9e4054f0dbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.488 total time=  19.1s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.475 total time=  19.1s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.489 total time=  19.5s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.461 total time=  19.5s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.496 total time=  19.3s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.481 total time=  22.5s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.491 total time=  22.4s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.463 total time=  22.0s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.473 total time=  22.3s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.466 total time=  22.3s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.482 total time= 1.6min\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.474 total time= 1.5min\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.492 total time= 1.6min\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.469 total time= 1.6min\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.499 total time= 1.5min\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.375 total time=   7.2s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.347 total time=   7.2s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.360 total time=   7.3s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.346 total time=   7.3s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.339 total time=   7.1s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.422 total time=  28.4s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.403 total time=  28.1s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.404 total time=  28.2s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.417 total time=  28.9s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.422 total time=  28.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.484 total time= 2.1min\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.512 total time= 2.0min\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.473 total time= 2.0min\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.476 total time= 2.0min\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.491 total time= 2.0min\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.486 total time= 3.2min\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.473 total time= 3.3min\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.487 total time= 3.2min\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.486 total time= 3.2min\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.472 total time= 3.2min\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.476 total time=   9.8s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.464 total time=  10.1s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.484 total time=   9.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.468 total time=   9.7s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.505 total time=   9.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.421 total time=  17.0s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.385 total time=  16.8s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.414 total time=  17.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.398 total time=  16.9s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.420 total time=  16.8s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.445 total time=  10.0s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.427 total time=  10.0s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.456 total time=  10.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.429 total time=   9.9s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.463 total time=  10.0s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.479 total time= 1.4min\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.482 total time= 1.4min\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.487 total time= 1.5min\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.488 total time= 1.4min\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.500 total time= 1.4min\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.483 total time= 1.4min\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.500 total time= 1.4min\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.497 total time= 1.4min\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.477 total time= 1.4min\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.487 total time= 1.4min\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.468 total time= 1.6min\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.496 total time= 1.6min\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.479 total time= 1.6min\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.485 total time= 1.6min\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.480 total time= 1.6min\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.481 total time=  24.6s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.504 total time=  23.9s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.456 total time=  23.7s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.484 total time=  23.9s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.496 total time=  24.7s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=7, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.496 total time=  16.9s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=7, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.469 total time=  16.8s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=7, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.467 total time=  16.9s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=7, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.489 total time=  17.2s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=7, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.488 total time=  16.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.507 total time=  40.0s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.476 total time=  40.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.482 total time=  39.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.491 total time=  40.0s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.508 total time=  40.1s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.466 total time=  39.0s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.464 total time=  39.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.474 total time=  39.0s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.446 total time=  39.1s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.486 total time=  39.1s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.496 total time=  13.8s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.491 total time=  14.1s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.481 total time=  13.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.493 total time=  13.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.475 total time=  13.9s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.472 total time= 1.1min\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.453 total time= 1.1min\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.462 total time= 1.1min\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.468 total time= 1.2min\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.483 total time= 1.2min\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.497 total time= 2.0min\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.496 total time= 2.0min\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.509 total time= 2.0min\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.495 total time= 2.0min\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.497 total time= 2.0min\n",
      "Best parameters: {'subsample': 0.7, 'objective': 'reg:squarederror', 'n_estimators': 500, 'min_child_weight': 5, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 0.5} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[6419    1]\n",
      " [   3 5101]] \n",
      "\n",
      "Accuracy: 100.0 \n",
      "\n",
      "F1 Score: 100.0 \n",
      "\n",
      "Balanced accuracy: 100.0 \n",
      "\n",
      "AUC Score: 100.0 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[495 219]\n",
      " [295 258]] \n",
      "\n",
      "Accuracy: 59.4 \n",
      "\n",
      "F1 Score: 50.1 \n",
      "\n",
      "Balanced accuracy: 50.1 \n",
      "\n",
      "AUC Score: 58.0 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = XGBoostClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'learning_rate': [0.01, 0.1, 0.3, 0.5],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.5, 0.7],\n",
    "    'colsample_bytree': [0.5, 0.7],\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'objective': ['reg:squarederror']\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_bert, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_bert, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_xgb_bert_train, accuracy_xgb_bert_train, f1_xgb_bert_train, balaccuracy_xgb_bert_train, rocauc_xgb_bert_train = classifier.evaluate(X_train_embeddings_bert, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_xgb_bert_test, accuracy_xgb_bert_test, f1_xgb_bert_test, balaccuracy_xgb_bert_test, rocauc_xgb_bert_test = classifier.evaluate(X_test_embeddings_bert, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T12:09:29.973114803Z",
     "start_time": "2023-06-03T12:09:29.915390746Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_xgb_bert_train_tosave = repr(cm_xgb_bert_train)\n",
    "accuracy_xgb_bert_train_tosave = repr(accuracy_xgb_bert_train)\n",
    "f1_xgb_bert_train_tosave = repr(f1_xgb_bert_train)\n",
    "balaccuracy_xgb_bert_train_tosave = repr(balaccuracy_xgb_bert_train)\n",
    "rocauc_xgb_bert_train_tosave = repr(rocauc_xgb_bert_train)\n",
    "\n",
    "cm_xgb_bert_test_tosave = repr(cm_xgb_bert_test)\n",
    "accuracy_xgb_bert_test_tosave = repr(accuracy_xgb_bert_test)\n",
    "f1_xgb_bert_test_tosave = repr(f1_xgb_bert_test)\n",
    "balaccuracy_xgb_bert_test_tosave = repr(balaccuracy_xgb_bert_test)\n",
    "rocauc_xgb_bert_test_tosave = repr(rocauc_xgb_bert_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/xgb_bert_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_xgb_bert_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_xgb_bert_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_xgb_bert_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_xgb_bert_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_xgb_bert_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_xgb_bert_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_xgb_bert_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_xgb_bert_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_xgb_bert_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_xgb_bert_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekftGhJPFFFX"
   },
   "source": [
    "### BERT + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T12:12:18.888634451Z",
     "start_time": "2023-06-03T12:09:51.129280194Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ks6w5IOrJYG",
    "outputId": "0341c3cf-975e-4e0c-8a26-01064b132ae1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.405 total time=   0.4s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.369 total time=   0.4s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.384 total time=   0.4s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.379 total time=   0.4s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.393 total time=   0.4s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.437 total time=   2.2s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.442 total time=   2.2s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.461 total time=   2.4s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.451 total time=   2.3s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.460 total time=   2.3s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.369 total time=   0.4s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.412 total time=   0.4s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.373 total time=   0.4s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.362 total time=   0.4s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.350 total time=   0.5s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.384 total time=   0.5s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.401 total time=   0.5s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.398 total time=   0.4s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.392 total time=   0.4s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.413 total time=   0.4s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.370 total time=   0.9s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.383 total time=   0.9s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.382 total time=   0.9s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.388 total time=   0.9s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.385 total time=   0.8s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.475 total time=   1.2s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.451 total time=   1.2s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.463 total time=   1.2s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.479 total time=   1.2s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.439 total time=   1.2s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.462 total time=   2.2s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.440 total time=   2.2s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.444 total time=   2.2s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.413 total time=   2.2s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.426 total time=   2.2s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.369 total time=   0.8s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.381 total time=   0.8s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.375 total time=   0.8s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.402 total time=   0.8s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.382 total time=   0.8s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.442 total time=   1.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.431 total time=   1.2s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.433 total time=   1.2s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.423 total time=   1.2s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.433 total time=   1.2s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.423 total time=   0.6s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.450 total time=   0.6s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.432 total time=   0.6s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.430 total time=   0.6s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.446 total time=   0.6s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.391 total time=   0.3s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.410 total time=   0.3s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.410 total time=   0.3s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.406 total time=   0.3s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.409 total time=   0.3s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.436 total time=   1.7s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.410 total time=   1.6s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.433 total time=   1.6s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.394 total time=   1.6s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.420 total time=   1.6s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.408 total time=   0.6s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.424 total time=   0.6s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.424 total time=   0.6s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.428 total time=   0.6s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.434 total time=   0.6s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.431 total time=   1.3s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.433 total time=   1.4s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.465 total time=   1.2s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.444 total time=   1.2s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.455 total time=   1.3s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.415 total time=   0.3s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.410 total time=   0.3s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.445 total time=   0.3s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.406 total time=   0.3s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.402 total time=   0.3s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.437 total time=   1.2s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.440 total time=   1.2s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.439 total time=   1.2s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.437 total time=   1.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.423 total time=   1.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.446 total time=   2.3s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.462 total time=   2.3s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.452 total time=   2.3s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.443 total time=   2.2s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.454 total time=   2.2s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.451 total time=   2.2s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.463 total time=   2.2s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.466 total time=   2.2s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.423 total time=   2.2s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.430 total time=   2.3s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.378 total time=   0.8s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.387 total time=   0.8s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.389 total time=   0.8s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.355 total time=   0.8s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.388 total time=   0.8s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.368 total time=   0.4s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.372 total time=   0.4s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.375 total time=   0.5s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.379 total time=   0.4s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.385 total time=   0.5s\n",
      "Best parameters: {'n_estimators': 25, 'max_features': 5, 'max_depth': None, 'bootstrap': False} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[6420    0]\n",
      " [   4 5100]] \n",
      "\n",
      "Accuracy: 100.0 \n",
      "\n",
      "F1 Score: 100.0 \n",
      "\n",
      "Balanced accuracy: 100.0 \n",
      "\n",
      "AUC Score: 100.0 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[504 210]\n",
      " [327 226]] \n",
      "\n",
      "Accuracy: 57.6 \n",
      "\n",
      "F1 Score: 45.7 \n",
      "\n",
      "Balanced accuracy: 45.7 \n",
      "\n",
      "AUC Score: 55.7 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = RFClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'n_estimators': [10, 25], \n",
    "    'max_features': [5, 10],\n",
    "    'max_depth': [10, 50, None], \n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_bert, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_bert, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_rf_bert_train, accuracy_rf_bert_train, f1_rf_bert_train, balaccuracy_rf_bert_train, rocauc_rf_bert_train = classifier.evaluate(X_train_embeddings_bert, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_rf_bert_test, accuracy_rf_bert_test, f1_rf_bert_test, balaccuracy_rf_bert_test, rocauc_rf_bert_test = classifier.evaluate(X_test_embeddings_bert, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T12:19:45.764024124Z",
     "start_time": "2023-06-03T12:19:45.719117462Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_rf_bert_train_tosave = repr(cm_rf_bert_train)\n",
    "accuracy_rf_bert_train_tosave = repr(accuracy_rf_bert_train)\n",
    "f1_rf_bert_train_tosave = repr(f1_rf_bert_train)\n",
    "balaccuracy_rf_bert_train_tosave = repr(balaccuracy_rf_bert_train)\n",
    "rocauc_rf_bert_train_tosave = repr(rocauc_rf_bert_train)\n",
    "\n",
    "cm_rf_bert_test_tosave = repr(cm_rf_bert_test)\n",
    "accuracy_rf_bert_test_tosave = repr(accuracy_rf_bert_test)\n",
    "f1_rf_bert_test_tosave = repr(f1_rf_bert_test)\n",
    "balaccuracy_rf_bert_test_tosave = repr(balaccuracy_rf_bert_test)\n",
    "rocauc_rf_bert_test_tosave = repr(rocauc_rf_bert_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/rf_bert_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_rf_bert_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_rf_bert_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_rf_bert_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_rf_bert_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_rf_bert_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_rf_bert_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_rf_bert_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_rf_bert_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_rf_bert_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_rf_bert_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbX9j48J1a9j"
   },
   "source": [
    "### BERT + SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-03T12:19:51.649534269Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fSTFuYyH63Mk",
    "is_executing": true,
    "outputId": "6ba15a33-8e98-4a4e-9113-8c771dc0dede"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:305: UserWarning: The total space of parameters 16 is smaller than n_iter=20. Running 16 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV 1/5] END ...C=1, gamma=scale, kernel=linear;, score=0.507 total time= 1.6min\n",
      "[CV 2/5] END ...C=1, gamma=scale, kernel=linear;, score=0.510 total time= 1.6min\n",
      "[CV 3/5] END ...C=1, gamma=scale, kernel=linear;, score=0.482 total time= 1.5min\n",
      "[CV 4/5] END ...C=1, gamma=scale, kernel=linear;, score=0.508 total time= 1.6min\n",
      "[CV 5/5] END ...C=1, gamma=scale, kernel=linear;, score=0.522 total time= 1.6min\n",
      "[CV 1/5] END .....C=1, gamma=scale, kernel=poly;, score=0.494 total time=  25.7s\n",
      "[CV 2/5] END .....C=1, gamma=scale, kernel=poly;, score=0.506 total time=  26.0s\n",
      "[CV 3/5] END .....C=1, gamma=scale, kernel=poly;, score=0.491 total time=  25.7s\n",
      "[CV 4/5] END .....C=1, gamma=scale, kernel=poly;, score=0.494 total time=  25.8s\n",
      "[CV 5/5] END .....C=1, gamma=scale, kernel=poly;, score=0.520 total time=  26.1s\n",
      "[CV 1/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.490 total time=  30.4s\n",
      "[CV 2/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.508 total time=  30.6s\n",
      "[CV 3/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.486 total time=  30.4s\n",
      "[CV 4/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.496 total time=  30.4s\n",
      "[CV 5/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.516 total time=  30.7s\n",
      "[CV 1/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.463 total time=  29.4s\n",
      "[CV 2/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.494 total time=  29.5s\n",
      "[CV 3/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.469 total time=  28.4s\n",
      "[CV 4/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.448 total time=  27.4s\n",
      "[CV 5/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.470 total time=  29.3s\n",
      "[CV 1/5] END ..C=10, gamma=scale, kernel=linear;, score=0.509 total time=12.2min\n",
      "[CV 2/5] END ..C=10, gamma=scale, kernel=linear;, score=0.514 total time=12.4min\n",
      "[CV 3/5] END ..C=10, gamma=scale, kernel=linear;, score=0.479 total time=12.2min\n",
      "[CV 4/5] END ..C=10, gamma=scale, kernel=linear;, score=0.503 total time=12.2min\n",
      "[CV 5/5] END ..C=10, gamma=scale, kernel=linear;, score=0.518 total time=12.5min\n",
      "[CV 1/5] END ....C=10, gamma=scale, kernel=poly;, score=0.512 total time=  36.8s\n",
      "[CV 2/5] END ....C=10, gamma=scale, kernel=poly;, score=0.524 total time=  37.0s\n",
      "[CV 3/5] END ....C=10, gamma=scale, kernel=poly;, score=0.511 total time=  36.7s\n",
      "[CV 4/5] END ....C=10, gamma=scale, kernel=poly;, score=0.502 total time=  36.9s\n",
      "[CV 5/5] END ....C=10, gamma=scale, kernel=poly;, score=0.536 total time=  38.1s\n",
      "[CV 1/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.500 total time=  38.7s\n",
      "[CV 2/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.525 total time=  39.2s\n",
      "[CV 3/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.510 total time=  39.6s\n",
      "[CV 4/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.515 total time=  39.0s\n",
      "[CV 5/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.533 total time=  39.3s\n",
      "[CV 1/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.480 total time=  17.4s\n",
      "[CV 2/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.463 total time=  16.9s\n",
      "[CV 3/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.477 total time=  16.7s\n",
      "[CV 4/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.471 total time=  16.9s\n",
      "[CV 5/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.455 total time=  17.1s\n",
      "[CV 1/5] END C=100, gamma=scale, kernel=linear;, score=0.514 total time=119.5min\n",
      "[CV 2/5] END C=100, gamma=scale, kernel=linear;, score=0.507 total time=119.8min\n",
      "[CV 3/5] END C=100, gamma=scale, kernel=linear;, score=0.478 total time=116.2min\n",
      "[CV 4/5] END C=100, gamma=scale, kernel=linear;, score=0.502 total time=117.7min\n",
      "[CV 5/5] END C=100, gamma=scale, kernel=linear;, score=0.515 total time=118.8min\n",
      "[CV 1/5] END ...C=100, gamma=scale, kernel=poly;, score=0.503 total time= 2.1min\n",
      "[CV 2/5] END ...C=100, gamma=scale, kernel=poly;, score=0.507 total time= 2.1min\n",
      "[CV 3/5] END ...C=100, gamma=scale, kernel=poly;, score=0.513 total time= 2.2min\n",
      "[CV 4/5] END ...C=100, gamma=scale, kernel=poly;, score=0.502 total time= 2.1min\n",
      "[CV 5/5] END ...C=100, gamma=scale, kernel=poly;, score=0.538 total time= 2.1min\n",
      "[CV 1/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.503 total time= 2.6min\n",
      "[CV 2/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.511 total time= 2.6min\n",
      "[CV 3/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.518 total time= 2.7min\n",
      "[CV 4/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.503 total time= 2.6min\n",
      "[CV 5/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.535 total time= 2.7min\n",
      "[CV 1/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.472 total time=  15.1s\n",
      "[CV 2/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.465 total time=  15.1s\n",
      "[CV 3/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.474 total time=  15.1s\n",
      "[CV 4/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.469 total time=  15.0s\n",
      "[CV 5/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.460 total time=  15.0s\n",
      "[CV 1/5] END C=1000, gamma=scale, kernel=linear;, score=0.507 total time=794.7min\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = SVClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'C': [1, 10, 100, 1000],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale']\n",
    "    #'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_bert, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_bert, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_svc_bert_train, accuracy_svc_bert_train, f1_svc_bert_train, balaccuracy_svc_bert_train, rocauc_svc_bert_train = classifier.evaluate(X_train_embeddings_bert, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_svc_bert_test, accuracy_svc_bert_test, f1_svc_bert_test, balaccuracy_svc_bert_test, rocauc_svc_bert_test = classifier.evaluate(X_test_embeddings_bert, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_svc_bert_train_tosave = repr(cm_svc_bert_train)\n",
    "accuracy_svc_bert_train_tosave = repr(accuracy_svc_bert_train)\n",
    "f1_svc_bert_train_tosave = repr(f1_svc_bert_train)\n",
    "balaccuracy_svc_bert_train_tosave = repr(balaccuracy_svc_bert_train)\n",
    "rocauc_svc_bert_train_tosave = repr(rocauc_svc_bert_train)\n",
    "\n",
    "cm_svc_bert_test_tosave = repr(cm_svc_bert_test)\n",
    "accuracy_svc_bert_test_tosave = repr(accuracy_svc_bert_test)\n",
    "f1_svc_bert_test_tosave = repr(f1_svc_bert_test)\n",
    "balaccuracy_svc_bert_test_tosave = repr(balaccuracy_svc_bert_test)\n",
    "rocauc_svc_bert_test_tosave = repr(rocauc_svc_bert_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/svc_bert_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_svc_bert_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_svc_bert_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_svc_bert_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_svc_bert_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_svc_bert_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_svc_bert_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_svc_bert_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_svc_bert_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_svc_bert_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_svc_bert_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYoz0OB11ayT"
   },
   "source": [
    "### BERT + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YijN0v7JCFbG",
    "is_executing": true,
    "outputId": "4bf95058-f264-43c2-e84d-1572b4814428"
   },
   "outputs": [],
   "source": [
    "# Instantiate classifier\n",
    "classifier = LRClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'C' : np.arange(0, 1, 0.01)\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_bert, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_bert, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_lr_bert_train, accuracy_lr_bert_train, f1_lr_bert_train, balaccuracy_lr_bert_train, rocauc_lr_bert_train = classifier.evaluate(X_train_embeddings_bert, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_lr_bert_test, accuracy_lr_bert_test, f1_lr_bert_test, balaccuracy_lr_bert_test, rocauc_lr_bert_test = classifier.evaluate(X_test_embeddings_bert, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_lr_bert_train_tosave = repr(cm_lr_bert_train)\n",
    "accuracy_lr_bert_train_tosave = repr(accuracy_lr_bert_train)\n",
    "f1_lr_bert_train_tosave = repr(f1_lr_bert_train)\n",
    "balaccuracy_lr_bert_train_tosave = repr(balaccuracy_lr_bert_train)\n",
    "rocauc_lr_bert_train_tosave = repr(rocauc_lr_bert_train)\n",
    "\n",
    "cm_lr_bert_test_tosave = repr(cm_lr_bert_test)\n",
    "accuracy_lr_bert_test_tosave = repr(accuracy_lr_bert_test)\n",
    "f1_lr_bert_test_tosave = repr(f1_lr_bert_test)\n",
    "balaccuracy_lr_bert_test_tosave = repr(balaccuracy_lr_bert_test)\n",
    "rocauc_lr_bert_test_tosave = repr(rocauc_lr_bert_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/lr_bert_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_lr_bert_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_lr_bert_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_lr_bert_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_lr_bert_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_lr_bert_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_lr_bert_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_lr_bert_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_lr_bert_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_lr_bert_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_lr_bert_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT + Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# SET PARAMETERS\n",
    "dropout_rate = 0.1\n",
    "epochs_number = 15\n",
    "learning_rate = 0.0001\n",
    "early_stopping = 5\n",
    "# --------------------\n",
    "\n",
    "input_dim = X_train_embeddings_bert.shape[1]  # Dimensionality of word embeddings\n",
    "hidden_dim = X_train_embeddings_bert.shape[1]*2  # Number of units in the hidden layer\n",
    "\n",
    "classifier = NeuralNetworkClassifier(input_dim, hidden_dim, dropout=dropout_rate)\n",
    "\n",
    "# Fit the model\n",
    "classifier.fit(X_train_embeddings_bert, y_train, X_test_embeddings_bert, y_test, num_epochs=epochs_number, lr=learning_rate, patience=early_stopping)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_nn_bert_train, accuracy_nn_bert_train, f1_nn_bert_train, balaccuracy_nn_bert_train, rocauc_nn_bert_train = classifier.evaluate(X_train_embeddings_bert, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_nn_bert_test, accuracy_nn_bert_test, f1_nn_bert_test, balaccuracy_nn_bert_test, rocauc_nn_bert_test = classifier.evaluate(X_test_embeddings_bert, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_nn_bert_train_tosave = repr(cm_nn_bert_train)\n",
    "accuracy_nn_bert_train_tosave = repr(accuracy_nn_bert_train)\n",
    "f1_nn_bert_train_tosave = repr(f1_nn_bert_train)\n",
    "balaccuracy_nn_bert_train_tosave = repr(balaccuracy_nn_bert_train)\n",
    "rocauc_nn_bert_train_tosave = repr(rocauc_nn_bert_train)\n",
    "\n",
    "cm_nn_bert_test_tosave = repr(cm_nn_bert_test)\n",
    "accuracy_nn_bert_test_tosave = repr(accuracy_nn_bert_test)\n",
    "f1_nn_bert_test_tosave = repr(f1_nn_bert_test)\n",
    "balaccuracy_nn_bert_test_tosave = repr(balaccuracy_nn_bert_test)\n",
    "rocauc_nn_bert_test_tosave = repr(rocauc_nn_bert_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/nn_bert_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_nn_bert_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_nn_bert_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_nn_bert_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_nn_bert_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_nn_bert_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_nn_bert_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_nn_bert_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_nn_bert_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_nn_bert_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_nn_bert_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uSk7WKhGB1v"
   },
   "source": [
    "## GloVe embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1D_jiR2GI7H"
   },
   "source": [
    "### GloVe + KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "LA5swddzG28v",
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=9, weights=distance;, score=0.460 total time=   2.8s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=9, weights=distance;, score=0.466 total time=   2.8s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=9, weights=distance;, score=0.488 total time=   2.8s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=9, weights=distance;, score=0.481 total time=   2.8s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=9, weights=distance;, score=0.513 total time=   2.7s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=6, weights=distance;, score=0.471 total time=   0.1s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=6, weights=distance;, score=0.490 total time=   0.1s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=6, weights=distance;, score=0.508 total time=   0.1s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=6, weights=distance;, score=0.482 total time=   0.1s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=6, weights=distance;, score=0.515 total time=   0.1s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=8, weights=distance;, score=0.485 total time=   0.1s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=8, weights=distance;, score=0.490 total time=   0.1s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=8, weights=distance;, score=0.508 total time=   0.1s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=8, weights=distance;, score=0.476 total time=   0.1s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=8, weights=distance;, score=0.519 total time=   0.1s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=8, weights=distance;, score=0.477 total time=   2.8s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=8, weights=distance;, score=0.468 total time=   2.7s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=8, weights=distance;, score=0.506 total time=   2.8s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=8, weights=distance;, score=0.499 total time=   2.8s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=8, weights=distance;, score=0.517 total time=   2.7s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.471 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.490 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.508 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.482 total time=   0.1s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.515 total time=   0.1s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.493 total time=   0.1s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.493 total time=   0.1s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.514 total time=   0.1s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.486 total time=   0.1s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.511 total time=   0.1s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.394 total time=   0.1s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.355 total time=   0.1s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.421 total time=   0.1s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.379 total time=   0.1s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.408 total time=   0.1s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=2, weights=uniform;, score=0.339 total time=   0.1s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=2, weights=uniform;, score=0.322 total time=   0.1s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=2, weights=uniform;, score=0.356 total time=   0.1s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=2, weights=uniform;, score=0.353 total time=   0.1s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=2, weights=uniform;, score=0.359 total time=   0.1s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.480 total time=   2.8s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.470 total time=   2.8s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.508 total time=   2.8s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.487 total time=   2.8s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.510 total time=   2.8s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.476 total time=   0.1s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.482 total time=   0.1s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.499 total time=   0.1s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.487 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.512 total time=   0.1s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=6, weights=uniform;, score=0.385 total time=   0.1s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=6, weights=uniform;, score=0.401 total time=   0.1s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=6, weights=uniform;, score=0.432 total time=   0.1s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=6, weights=uniform;, score=0.397 total time=   0.1s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=6, weights=uniform;, score=0.427 total time=   0.1s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=6, weights=distance;, score=0.475 total time=   2.8s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=6, weights=distance;, score=0.476 total time=   2.7s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=6, weights=distance;, score=0.505 total time=   2.7s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=6, weights=distance;, score=0.494 total time=   2.7s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=6, weights=distance;, score=0.516 total time=   2.7s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=2, weights=distance;, score=0.476 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=2, weights=distance;, score=0.482 total time=   0.1s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=2, weights=distance;, score=0.499 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=2, weights=distance;, score=0.487 total time=   0.1s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=2, weights=distance;, score=0.512 total time=   0.1s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=8, weights=uniform;, score=0.405 total time=   0.1s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=8, weights=uniform;, score=0.410 total time=   0.1s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=8, weights=uniform;, score=0.445 total time=   0.1s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=8, weights=uniform;, score=0.399 total time=   0.1s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=8, weights=uniform;, score=0.437 total time=   0.1s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=7, weights=uniform;, score=0.470 total time=   2.8s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=7, weights=uniform;, score=0.480 total time=   2.8s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=7, weights=uniform;, score=0.510 total time=   2.8s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=7, weights=uniform;, score=0.490 total time=   2.8s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=7, weights=uniform;, score=0.506 total time=   2.8s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.472 total time=   0.1s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.478 total time=   0.1s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.513 total time=   0.1s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.463 total time=   0.1s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.505 total time=   0.1s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.485 total time=   0.1s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.490 total time=   0.1s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.508 total time=   0.1s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.476 total time=   0.1s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.519 total time=   0.1s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=5, weights=distance;, score=0.482 total time=   2.7s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=5, weights=distance;, score=0.472 total time=   2.7s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=5, weights=distance;, score=0.510 total time=   2.7s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=5, weights=distance;, score=0.488 total time=   2.8s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=5, weights=distance;, score=0.512 total time=   2.7s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=9, weights=distance;, score=0.473 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=9, weights=distance;, score=0.480 total time=   0.1s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=9, weights=distance;, score=0.515 total time=   0.1s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=9, weights=distance;, score=0.462 total time=   0.1s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=9, weights=distance;, score=0.507 total time=   0.1s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.485 total time=   0.1s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.491 total time=   0.1s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.506 total time=   0.1s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.480 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.506 total time=   0.1s\n",
      "Best parameters: {'weights': 'uniform', 'n_neighbors': 3, 'metric': 'euclidean'} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[5402 1018]\n",
      " [1424 3680]] \n",
      "\n",
      "Accuracy: 78.8 \n",
      "\n",
      "F1 Score: 75.1 \n",
      "\n",
      "Balanced accuracy: 75.1 \n",
      "\n",
      "AUC Score: 78.1 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[465 249]\n",
      " [317 236]] \n",
      "\n",
      "Accuracy: 55.3 \n",
      "\n",
      "F1 Score: 45.5 \n",
      "\n",
      "Balanced accuracy: 45.5 \n",
      "\n",
      "AUC Score: 53.9 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = KNNClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'weights' : ['uniform','distance'],\n",
    "    'metric' : ['minkowski','euclidean','manhattan']\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_glove, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_glove, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_knn_glove_train, accuracy_knn_glove_train, f1_knn_glove_train, balaccuracy_knn_glove_train, rocauc_knn_glove_train = classifier.evaluate(X_train_embeddings_glove, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_knn_glove_test, accuracy_knn_glove_test, f1_knn_glove_test, balaccuracy_knn_glove_test, rocauc_knn_glove_test = classifier.evaluate(X_test_embeddings_glove, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_knn_glove_train_tosave = repr(cm_knn_glove_train)\n",
    "accuracy_knn_glove_train_tosave = repr(accuracy_knn_glove_train)\n",
    "f1_knn_glove_train_tosave = repr(f1_knn_glove_train)\n",
    "balaccuracy_knn_glove_train_tosave = repr(balaccuracy_knn_glove_train)\n",
    "rocauc_knn_glove_train_tosave = repr(rocauc_knn_glove_train)\n",
    "\n",
    "cm_knn_glove_test_tosave = repr(cm_knn_glove_test)\n",
    "accuracy_knn_glove_test_tosave = repr(accuracy_knn_glove_test)\n",
    "f1_knn_glove_test_tosave = repr(f1_knn_glove_test)\n",
    "balaccuracy_knn_glove_test_tosave = repr(balaccuracy_knn_glove_test)\n",
    "rocauc_knn_glove_test_tosave = repr(rocauc_knn_glove_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/knn_glove_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_knn_glove_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_knn_glove_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_knn_glove_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_knn_glove_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_knn_glove_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_knn_glove_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_knn_glove_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_knn_glove_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_knn_glove_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_knn_glove_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Lfsat69GIoJ"
   },
   "source": [
    "### GloVe + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "rt7c3MSPG3PP",
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.501 total time=  17.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.480 total time=  18.2s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.480 total time=  17.3s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.500 total time=  17.4s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.499 total time=  18.1s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.513 total time=   7.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.494 total time=   7.9s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.503 total time=   7.9s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.493 total time=   7.9s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.507 total time=   7.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.488 total time=   4.0s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.490 total time=   4.1s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.484 total time=   4.1s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.489 total time=   4.0s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.494 total time=   4.0s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.519 total time=   6.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.494 total time=   6.9s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.506 total time=   6.9s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.504 total time=   6.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.517 total time=   6.9s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.498 total time=   4.8s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.473 total time=   4.8s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.490 total time=   4.8s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.486 total time=   4.7s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.484 total time=   4.9s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.510 total time=  11.0s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.492 total time=  10.8s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.494 total time=  11.0s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.485 total time=  10.8s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.521 total time=  10.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.510 total time=  52.7s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.505 total time=  52.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.507 total time=  52.7s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.493 total time=  52.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.516 total time=  52.2s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=7, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.475 total time=  21.5s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=7, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.494 total time=  21.5s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=7, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.495 total time=  21.2s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=7, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.468 total time=  21.0s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=7, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.520 total time=  20.9s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.486 total time= 1.3min\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.494 total time= 1.3min\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.506 total time= 1.3min\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.498 total time= 1.3min\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.493 total time= 1.3min\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.495 total time=   4.0s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.497 total time=   4.0s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.482 total time=   4.0s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.487 total time=   4.0s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.499 total time=   4.0s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.500 total time=   6.6s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.477 total time=   6.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.487 total time=   6.5s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.487 total time=   6.5s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.502 total time=   6.5s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.497 total time=  10.7s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.503 total time=  10.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.499 total time=  10.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.505 total time=  10.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.477 total time=  10.6s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.513 total time=  33.6s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.490 total time=  33.7s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.495 total time=  33.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.497 total time=  33.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.511 total time=  33.8s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.503 total time=  12.7s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.489 total time=  12.8s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.482 total time=  12.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.477 total time=  12.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.516 total time=  12.8s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.490 total time=   2.0s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.476 total time=   2.0s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.486 total time=   2.0s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.483 total time=   2.0s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.500 total time=   2.0s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.432 total time=   6.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.433 total time=   6.7s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.417 total time=   6.5s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.429 total time=   6.5s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.449 total time=   6.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.502 total time=  19.6s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.497 total time=  19.5s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.508 total time=  19.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.495 total time=  19.5s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.517 total time=  19.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.523 total time= 1.1min\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.496 total time= 1.1min\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.510 total time= 1.1min\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.489 total time= 1.1min\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.533 total time= 1.1min\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.478 total time=   5.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.493 total time=   5.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.512 total time=   5.5s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.481 total time=   5.5s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.515 total time=   5.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.464 total time=   2.2s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.475 total time=   2.3s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.470 total time=   2.2s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.453 total time=   2.3s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.475 total time=   2.3s\n",
      "Best parameters: {'subsample': 0.7, 'objective': 'reg:squarederror', 'n_estimators': 500, 'min_child_weight': 3, 'max_depth': 9, 'learning_rate': 0.01, 'colsample_bytree': 0.5} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[6414    6]\n",
      " [  10 5094]] \n",
      "\n",
      "Accuracy: 99.9 \n",
      "\n",
      "F1 Score: 99.8 \n",
      "\n",
      "Balanced accuracy: 99.8 \n",
      "\n",
      "AUC Score: 99.9 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[533 181]\n",
      " [310 243]] \n",
      "\n",
      "Accuracy: 61.2 \n",
      "\n",
      "F1 Score: 49.7 \n",
      "\n",
      "Balanced accuracy: 49.7 \n",
      "\n",
      "AUC Score: 59.3 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = XGBoostClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'learning_rate': [0.01, 0.1, 0.3, 0.5],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.5, 0.7],\n",
    "    'colsample_bytree': [0.5, 0.7],\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'objective': ['reg:squarederror']\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_glove, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_glove, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_xgb_glove_train, accuracy_xgb_glove_train, f1_xgb_glove_train, balaccuracy_xgb_glove_train, rocauc_xgb_glove_train = classifier.evaluate(X_train_embeddings_glove, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_xgb_glove_test, accuracy_xgb_glove_test, f1_xgb_glove_test, balaccuracy_xgb_glove_test, rocauc_xgb_glove_test = classifier.evaluate(X_test_embeddings_glove, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_xgb_glove_train_tosave = repr(cm_xgb_glove_train)\n",
    "accuracy_xgb_glove_train_tosave = repr(accuracy_xgb_glove_train)\n",
    "f1_xgb_glove_train_tosave = repr(f1_xgb_glove_train)\n",
    "balaccuracy_xgb_glove_train_tosave = repr(balaccuracy_xgb_glove_train)\n",
    "rocauc_xgb_glove_train_tosave = repr(rocauc_xgb_glove_train)\n",
    "\n",
    "cm_xgb_glove_test_tosave = repr(cm_xgb_glove_test)\n",
    "accuracy_xgb_glove_test_tosave = repr(accuracy_xgb_glove_test)\n",
    "f1_xgb_glove_test_tosave = repr(f1_xgb_glove_test)\n",
    "balaccuracy_xgb_glove_test_tosave = repr(balaccuracy_xgb_glove_test)\n",
    "rocauc_xgb_glove_test_tosave = repr(rocauc_xgb_glove_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/xgb_glove_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_xgb_glove_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_xgb_glove_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_xgb_glove_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_xgb_glove_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_xgb_glove_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_xgb_glove_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_xgb_glove_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_xgb_glove_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_xgb_glove_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_xgb_glove_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUCEQAQXGIl5"
   },
   "source": [
    "### GloVe + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "GLutlbgvG3f8",
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.426 total time=   0.8s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.418 total time=   0.8s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.435 total time=   0.8s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.409 total time=   0.8s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.425 total time=   0.8s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.391 total time=   0.4s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.414 total time=   0.4s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.391 total time=   0.4s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.405 total time=   0.4s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.412 total time=   0.4s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.481 total time=   2.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.453 total time=   2.0s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.471 total time=   2.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.452 total time=   2.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.471 total time=   2.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.454 total time=   0.2s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.440 total time=   0.2s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.463 total time=   0.2s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.429 total time=   0.2s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.433 total time=   0.2s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.483 total time=   2.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.475 total time=   2.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.482 total time=   2.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.480 total time=   2.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.474 total time=   2.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.413 total time=   0.4s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.394 total time=   0.4s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.408 total time=   0.4s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.415 total time=   0.4s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.440 total time=   0.4s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.386 total time=   0.8s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.418 total time=   0.8s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.407 total time=   0.7s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.390 total time=   0.8s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.413 total time=   0.8s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.492 total time=   2.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.454 total time=   2.0s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.452 total time=   2.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.465 total time=   2.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.514 total time=   2.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.418 total time=   0.4s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.412 total time=   0.4s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.429 total time=   0.4s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.399 total time=   0.4s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.421 total time=   0.4s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.440 total time=   0.5s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.460 total time=   0.5s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.452 total time=   0.5s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.431 total time=   0.5s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.482 total time=   0.5s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.390 total time=   0.4s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.407 total time=   0.4s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.380 total time=   0.4s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.392 total time=   0.4s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.421 total time=   0.4s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.391 total time=   0.8s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.410 total time=   0.8s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.407 total time=   0.8s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.411 total time=   0.8s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.418 total time=   0.8s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.464 total time=   1.5s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.457 total time=   1.5s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.441 total time=   1.5s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.459 total time=   1.5s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.468 total time=   1.5s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.449 total time=   0.7s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.452 total time=   0.7s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.399 total time=   0.7s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.422 total time=   0.7s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.462 total time=   0.7s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.498 total time=   1.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.466 total time=   1.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.472 total time=   1.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.472 total time=   1.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.466 total time=   1.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.447 total time=   0.7s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.440 total time=   0.7s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.421 total time=   0.7s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.435 total time=   0.7s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.433 total time=   0.7s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.460 total time=   2.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.466 total time=   2.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.467 total time=   2.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.480 total time=   2.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.484 total time=   2.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.428 total time=   0.2s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.442 total time=   0.2s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.410 total time=   0.2s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.432 total time=   0.2s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.438 total time=   0.2s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.467 total time=   1.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.463 total time=   1.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.445 total time=   1.0s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.461 total time=   1.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.462 total time=   1.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.458 total time=   1.5s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.462 total time=   1.4s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.431 total time=   1.4s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.426 total time=   1.5s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.448 total time=   1.4s\n",
      "Best parameters: {'n_estimators': 25, 'max_features': 10, 'max_depth': 50, 'bootstrap': False} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[6420    0]\n",
      " [   7 5097]] \n",
      "\n",
      "Accuracy: 99.9 \n",
      "\n",
      "F1 Score: 99.9 \n",
      "\n",
      "Balanced accuracy: 99.9 \n",
      "\n",
      "AUC Score: 99.9 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[508 206]\n",
      " [301 252]] \n",
      "\n",
      "Accuracy: 60.0 \n",
      "\n",
      "F1 Score: 49.9 \n",
      "\n",
      "Balanced accuracy: 49.9 \n",
      "\n",
      "AUC Score: 58.4 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = RFClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'n_estimators': [10, 25], \n",
    "    'max_features': [5, 10],\n",
    "    'max_depth': [10, 50, None], \n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_glove, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_glove, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_rf_glove_train, accuracy_rf_glove_train, f1_rf_glove_train, balaccuracy_rf_glove_train, rocauc_rf_glove_train = classifier.evaluate(X_train_embeddings_glove, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_rf_glove_test, accuracy_rf_glove_test, f1_rf_glove_test, balaccuracy_rf_glove_test, rocauc_rf_glove_test = classifier.evaluate(X_test_embeddings_glove, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_rf_glove_train_tosave = repr(cm_rf_glove_train)\n",
    "accuracy_rf_glove_train_tosave = repr(accuracy_rf_glove_train)\n",
    "f1_rf_glove_train_tosave = repr(f1_rf_glove_train)\n",
    "balaccuracy_rf_glove_train_tosave = repr(balaccuracy_rf_glove_train)\n",
    "rocauc_rf_glove_train_tosave = repr(rocauc_rf_glove_train)\n",
    "\n",
    "cm_rf_glove_test_tosave = repr(cm_rf_glove_test)\n",
    "accuracy_rf_glove_test_tosave = repr(accuracy_rf_glove_test)\n",
    "f1_rf_glove_test_tosave = repr(f1_rf_glove_test)\n",
    "balaccuracy_rf_glove_test_tosave = repr(balaccuracy_rf_glove_test)\n",
    "rocauc_rf_glove_test_tosave = repr(rocauc_rf_glove_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/rf_glove_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_rf_glove_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_rf_glove_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_rf_glove_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_rf_glove_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_rf_glove_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_rf_glove_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_rf_glove_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_rf_glove_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_rf_glove_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_rf_glove_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vldrLVlwGIg9"
   },
   "source": [
    "### GloVe + SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "MqqebdmmG3qI",
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:305: UserWarning: The total space of parameters 16 is smaller than n_iter=20. Running 16 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV 1/5] END ...C=1, gamma=scale, kernel=linear;, score=0.484 total time=  13.0s\n",
      "[CV 2/5] END ...C=1, gamma=scale, kernel=linear;, score=0.484 total time=  13.1s\n",
      "[CV 3/5] END ...C=1, gamma=scale, kernel=linear;, score=0.485 total time=  13.1s\n",
      "[CV 4/5] END ...C=1, gamma=scale, kernel=linear;, score=0.497 total time=  13.0s\n",
      "[CV 5/5] END ...C=1, gamma=scale, kernel=linear;, score=0.504 total time=  13.0s\n",
      "[CV 1/5] END .....C=1, gamma=scale, kernel=poly;, score=0.476 total time=  12.0s\n",
      "[CV 2/5] END .....C=1, gamma=scale, kernel=poly;, score=0.493 total time=  11.9s\n",
      "[CV 3/5] END .....C=1, gamma=scale, kernel=poly;, score=0.478 total time=  11.9s\n",
      "[CV 4/5] END .....C=1, gamma=scale, kernel=poly;, score=0.464 total time=  11.9s\n",
      "[CV 5/5] END .....C=1, gamma=scale, kernel=poly;, score=0.472 total time=  12.0s\n",
      "[CV 1/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.514 total time=  14.4s\n",
      "[CV 2/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.499 total time=  14.4s\n",
      "[CV 3/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.488 total time=  14.3s\n",
      "[CV 4/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.495 total time=  14.2s\n",
      "[CV 5/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.511 total time=  14.6s\n",
      "[CV 1/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.472 total time=   9.3s\n",
      "[CV 2/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.470 total time=  12.7s\n",
      "[CV 3/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.494 total time=   8.9s\n",
      "[CV 4/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.488 total time=   8.9s\n",
      "[CV 5/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.461 total time=   8.5s\n",
      "[CV 1/5] END ..C=10, gamma=scale, kernel=linear;, score=0.489 total time=  23.8s\n",
      "[CV 2/5] END ..C=10, gamma=scale, kernel=linear;, score=0.481 total time=  24.7s\n",
      "[CV 3/5] END ..C=10, gamma=scale, kernel=linear;, score=0.485 total time=  24.3s\n",
      "[CV 4/5] END ..C=10, gamma=scale, kernel=linear;, score=0.492 total time=  24.3s\n",
      "[CV 5/5] END ..C=10, gamma=scale, kernel=linear;, score=0.501 total time=  24.2s\n",
      "[CV 1/5] END ....C=10, gamma=scale, kernel=poly;, score=0.519 total time=  24.7s\n",
      "[CV 2/5] END ....C=10, gamma=scale, kernel=poly;, score=0.524 total time=  23.7s\n",
      "[CV 3/5] END ....C=10, gamma=scale, kernel=poly;, score=0.537 total time=  27.2s\n",
      "[CV 4/5] END ....C=10, gamma=scale, kernel=poly;, score=0.519 total time=  23.9s\n",
      "[CV 5/5] END ....C=10, gamma=scale, kernel=poly;, score=0.537 total time=  25.9s\n",
      "[CV 1/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.526 total time=  25.1s\n",
      "[CV 2/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.530 total time=  25.2s\n",
      "[CV 3/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.529 total time=  26.6s\n",
      "[CV 4/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.531 total time=  26.8s\n",
      "[CV 5/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.539 total time=  25.3s\n",
      "[CV 1/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.460 total time=   7.2s\n",
      "[CV 2/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.483 total time=   7.0s\n",
      "[CV 3/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.496 total time=   7.1s\n",
      "[CV 4/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.483 total time=   7.2s\n",
      "[CV 5/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.476 total time=   7.1s\n",
      "[CV 1/5] END .C=100, gamma=scale, kernel=linear;, score=0.490 total time= 2.0min\n",
      "[CV 2/5] END .C=100, gamma=scale, kernel=linear;, score=0.479 total time= 2.0min\n",
      "[CV 3/5] END .C=100, gamma=scale, kernel=linear;, score=0.487 total time= 1.9min\n",
      "[CV 4/5] END .C=100, gamma=scale, kernel=linear;, score=0.493 total time= 2.0min\n",
      "[CV 5/5] END .C=100, gamma=scale, kernel=linear;, score=0.505 total time= 2.1min\n",
      "[CV 1/5] END ...C=100, gamma=scale, kernel=poly;, score=0.519 total time=  39.0s\n",
      "[CV 2/5] END ...C=100, gamma=scale, kernel=poly;, score=0.514 total time=  41.2s\n",
      "[CV 3/5] END ...C=100, gamma=scale, kernel=poly;, score=0.529 total time=  39.9s\n",
      "[CV 4/5] END ...C=100, gamma=scale, kernel=poly;, score=0.527 total time=  38.1s\n",
      "[CV 5/5] END ...C=100, gamma=scale, kernel=poly;, score=0.539 total time=  42.6s\n",
      "[CV 1/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.519 total time=  54.4s\n",
      "[CV 2/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.520 total time=  52.9s\n",
      "[CV 3/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.538 total time=  53.4s\n",
      "[CV 4/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.535 total time=  52.2s\n",
      "[CV 5/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.537 total time=  55.0s\n",
      "[CV 1/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.456 total time=   6.9s\n",
      "[CV 2/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.458 total time=   7.7s\n",
      "[CV 3/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.494 total time=   6.8s\n",
      "[CV 4/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.476 total time=   7.8s\n",
      "[CV 5/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.477 total time=   7.1s\n",
      "[CV 1/5] END C=1000, gamma=scale, kernel=linear;, score=0.489 total time=16.0min\n",
      "[CV 2/5] END C=1000, gamma=scale, kernel=linear;, score=0.482 total time=18.6min\n",
      "[CV 3/5] END C=1000, gamma=scale, kernel=linear;, score=0.489 total time=16.1min\n",
      "[CV 4/5] END C=1000, gamma=scale, kernel=linear;, score=0.495 total time=16.5min\n",
      "[CV 5/5] END C=1000, gamma=scale, kernel=linear;, score=0.497 total time=17.4min\n",
      "[CV 1/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.525 total time=  38.4s\n",
      "[CV 2/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.516 total time=  41.4s\n",
      "[CV 3/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.523 total time=  38.8s\n",
      "[CV 4/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.521 total time=  38.9s\n",
      "[CV 5/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.541 total time=  40.9s\n",
      "[CV 1/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.519 total time=  54.2s\n",
      "[CV 2/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.519 total time=  54.3s\n",
      "[CV 3/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.536 total time=  53.8s\n",
      "[CV 4/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.535 total time=  54.2s\n",
      "[CV 5/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.533 total time=  55.9s\n",
      "[CV 1/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.458 total time=   6.9s\n",
      "[CV 2/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.457 total time=   8.8s\n",
      "[CV 3/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.492 total time=   6.7s\n",
      "[CV 4/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.472 total time=   7.5s\n",
      "[CV 5/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.475 total time=   6.9s\n",
      "Best parameters: {'kernel': 'rbf', 'gamma': 'scale', 'C': 10} \n",
      "\n",
      "[LibSVM]\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[6247  173]\n",
      " [ 458 4646]] \n",
      "\n",
      "Accuracy: 94.5 \n",
      "\n",
      "F1 Score: 93.6 \n",
      "\n",
      "Balanced accuracy: 93.6 \n",
      "\n",
      "AUC Score: 94.2 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[488 226]\n",
      " [291 262]] \n",
      "\n",
      "Accuracy: 59.2 \n",
      "\n",
      "F1 Score: 50.3 \n",
      "\n",
      "Balanced accuracy: 50.3 \n",
      "\n",
      "AUC Score: 57.9 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = SVClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'C': [1, 10, 100, 1000],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale']\n",
    "    #'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_glove, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_glove, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_svc_glove_train, accuracy_svc_glove_train, f1_svc_glove_train, balaccuracy_svc_glove_train, rocauc_svc_glove_train = classifier.evaluate(X_train_embeddings_glove, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_svc_glove_test, accuracy_svc_glove_test, f1_svc_glove_test, balaccuracy_svc_glove_test, rocauc_svc_glove_test = classifier.evaluate(X_test_embeddings_glove, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_svc_glove_train_tosave = repr(cm_svc_glove_train)\n",
    "accuracy_svc_glove_train_tosave = repr(accuracy_svc_glove_train)\n",
    "f1_svc_glove_train_tosave = repr(f1_svc_glove_train)\n",
    "balaccuracy_svc_glove_train_tosave = repr(balaccuracy_svc_glove_train)\n",
    "rocauc_svc_glove_train_tosave = repr(rocauc_svc_glove_train)\n",
    "\n",
    "cm_svc_glove_test_tosave = repr(cm_svc_glove_test)\n",
    "accuracy_svc_glove_test_tosave = repr(accuracy_svc_glove_test)\n",
    "f1_svc_glove_test_tosave = repr(f1_svc_glove_test)\n",
    "balaccuracy_svc_glove_test_tosave = repr(balaccuracy_svc_glove_test)\n",
    "rocauc_svc_glove_test_tosave = repr(rocauc_svc_glove_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/svc_glove_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_svc_glove_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_svc_glove_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_svc_glove_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_svc_glove_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_svc_glove_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_svc_glove_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_svc_glove_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_svc_glove_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_svc_glove_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_svc_glove_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8IPkDvHGIZw"
   },
   "source": [
    "### GloVe + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "7j_OebkkG32s",
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END ....C=0.21, penalty=l2, solver=sag;, score=0.494 total time=   0.3s\n",
      "[CV 2/5] END ....C=0.21, penalty=l2, solver=sag;, score=0.501 total time=   0.3s\n",
      "[CV 3/5] END ....C=0.21, penalty=l2, solver=sag;, score=0.485 total time=   0.3s\n",
      "[CV 4/5] END ....C=0.21, penalty=l2, solver=sag;, score=0.487 total time=   0.3s\n",
      "[CV 5/5] END ....C=0.21, penalty=l2, solver=sag;, score=0.508 total time=   0.3s\n",
      "[CV 1/5] END C=0.33, penalty=l2, solver=liblinear;, score=0.495 total time=   0.4s\n",
      "[CV 2/5] END C=0.33, penalty=l2, solver=liblinear;, score=0.497 total time=   0.4s\n",
      "[CV 3/5] END C=0.33, penalty=l2, solver=liblinear;, score=0.493 total time=   0.3s\n",
      "[CV 4/5] END C=0.33, penalty=l2, solver=liblinear;, score=0.493 total time=   0.3s\n",
      "[CV 5/5] END C=0.33, penalty=l2, solver=liblinear;, score=0.510 total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ..C=0.12, penalty=none, solver=sag;, score=0.495 total time=   1.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END ..C=0.12, penalty=none, solver=sag;, score=0.500 total time=   1.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END ..C=0.12, penalty=none, solver=sag;, score=0.497 total time=   1.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END ..C=0.12, penalty=none, solver=sag;, score=0.499 total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END ..C=0.12, penalty=none, solver=sag;, score=0.505 total time=   1.1s\n",
      "[CV 1/5] END C=0.54, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.54, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.54, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.54, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.54, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.06, penalty=l2, solver=liblinear;, score=0.496 total time=   0.2s\n",
      "[CV 2/5] END C=0.06, penalty=l2, solver=liblinear;, score=0.484 total time=   0.2s\n",
      "[CV 3/5] END C=0.06, penalty=l2, solver=liblinear;, score=0.478 total time=   0.2s\n",
      "[CV 4/5] END C=0.06, penalty=l2, solver=liblinear;, score=0.481 total time=   0.2s\n",
      "[CV 5/5] END C=0.06, penalty=l2, solver=liblinear;, score=0.501 total time=   0.2s\n",
      "[CV 1/5] END C=0.61, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.61, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.61, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.61, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.61, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ......C=0.58, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ......C=0.58, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ......C=0.58, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ......C=0.58, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ......C=0.58, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.11, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.11, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.11, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.11, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.11, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ....C=0.33, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ....C=0.33, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ....C=0.33, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ....C=0.33, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ....C=0.33, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ......C=0.37, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ......C=0.37, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ......C=0.37, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ......C=0.37, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ......C=0.37, penalty=l1, solver=sag;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ..C=0.6, penalty=none, solver=saga;, score=0.496 total time=   1.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END ..C=0.6, penalty=none, solver=saga;, score=0.500 total time=   1.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END ..C=0.6, penalty=none, solver=saga;, score=0.496 total time=   1.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END ..C=0.6, penalty=none, solver=saga;, score=0.499 total time=   1.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END ..C=0.6, penalty=none, solver=saga;, score=0.505 total time=   1.7s\n",
      "[CV 1/5] END C=0.8300000000000001, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.8300000000000001, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.8300000000000001, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.8300000000000001, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.8300000000000001, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ....C=0.87, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ....C=0.87, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ....C=0.87, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ....C=0.87, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ....C=0.87, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ......C=0.49, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ......C=0.49, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ......C=0.49, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ......C=0.49, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ......C=0.49, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.9, penalty=l2, solver=liblinear;, score=0.489 total time=   0.5s\n",
      "[CV 2/5] END C=0.9, penalty=l2, solver=liblinear;, score=0.499 total time=   0.4s\n",
      "[CV 3/5] END C=0.9, penalty=l2, solver=liblinear;, score=0.493 total time=   0.5s\n",
      "[CV 4/5] END C=0.9, penalty=l2, solver=liblinear;, score=0.500 total time=   0.5s\n",
      "[CV 5/5] END C=0.9, penalty=l2, solver=liblinear;, score=0.506 total time=   0.5s\n",
      "[CV 1/5] END C=0.33, penalty=l1, solver=liblinear;, score=0.493 total time=   0.9s\n",
      "[CV 2/5] END C=0.33, penalty=l1, solver=liblinear;, score=0.492 total time=   0.3s\n",
      "[CV 3/5] END C=0.33, penalty=l1, solver=liblinear;, score=0.486 total time=   0.4s\n",
      "[CV 4/5] END C=0.33, penalty=l1, solver=liblinear;, score=0.475 total time=   0.7s\n",
      "[CV 5/5] END C=0.33, penalty=l1, solver=liblinear;, score=0.502 total time=   0.9s\n",
      "[CV 1/5] END ....C=0.06, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ....C=0.06, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ....C=0.06, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ....C=0.06, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ....C=0.06, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ..C=0.3, penalty=none, solver=saga;, score=0.496 total time=   1.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END ..C=0.3, penalty=none, solver=saga;, score=0.500 total time=   1.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END ..C=0.3, penalty=none, solver=saga;, score=0.496 total time=   1.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END ..C=0.3, penalty=none, solver=saga;, score=0.499 total time=   1.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END ..C=0.3, penalty=none, solver=saga;, score=0.505 total time=   1.7s\n",
      "[CV 1/5] END C=0.62, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.62, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.62, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.62, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.62, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ...C=0.79, penalty=l2, solver=saga;, score=0.490 total time=   1.4s\n",
      "[CV 2/5] END ...C=0.79, penalty=l2, solver=saga;, score=0.499 total time=   1.3s\n",
      "[CV 3/5] END ...C=0.79, penalty=l2, solver=saga;, score=0.494 total time=   1.3s\n",
      "[CV 4/5] END ...C=0.79, penalty=l2, solver=saga;, score=0.500 total time=   1.3s\n",
      "[CV 5/5] END ...C=0.79, penalty=l2, solver=saga;, score=0.507 total time=   1.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "55 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.49483874 0.4977102  0.49924962        nan 0.48815291        nan\n",
      "        nan        nan        nan        nan 0.49919473        nan\n",
      "        nan        nan 0.49754244 0.4895753         nan 0.49919473\n",
      "        nan 0.49819384]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'solver': 'sag', 'penalty': 'none', 'C': 0.12} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[4863 1557]\n",
      " [2624 2480]] \n",
      "\n",
      "Accuracy: 63.7 \n",
      "\n",
      "F1 Score: 54.3 \n",
      "\n",
      "Balanced accuracy: 54.3 \n",
      "\n",
      "AUC Score: 62.2 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[516 198]\n",
      " [307 246]] \n",
      "\n",
      "Accuracy: 60.1 \n",
      "\n",
      "F1 Score: 49.3 \n",
      "\n",
      "Balanced accuracy: 49.3 \n",
      "\n",
      "AUC Score: 58.4 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = LRClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'C' : np.arange(0, 1, 0.01)\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_glove, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_glove, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_lr_glove_train, accuracy_lr_glove_train, f1_lr_glove_train, balaccuracy_lr_glove_train, rocauc_lr_glove_train = classifier.evaluate(X_train_embeddings_glove, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_lr_glove_test, accuracy_lr_glove_test, f1_lr_glove_test, balaccuracy_lr_glove_test, rocauc_lr_glove_test = classifier.evaluate(X_test_embeddings_glove, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_lr_glove_train_tosave = repr(cm_lr_glove_train)\n",
    "accuracy_lr_glove_train_tosave = repr(accuracy_lr_glove_train)\n",
    "f1_lr_glove_train_tosave = repr(f1_lr_glove_train)\n",
    "balaccuracy_lr_glove_train_tosave = repr(balaccuracy_lr_glove_train)\n",
    "rocauc_lr_glove_train_tosave = repr(rocauc_lr_glove_train)\n",
    "\n",
    "cm_lr_glove_test_tosave = repr(cm_lr_glove_test)\n",
    "accuracy_lr_glove_test_tosave = repr(accuracy_lr_glove_test)\n",
    "f1_lr_glove_test_tosave = repr(f1_lr_glove_test)\n",
    "balaccuracy_lr_glove_test_tosave = repr(balaccuracy_lr_glove_test)\n",
    "rocauc_lr_glove_test_tosave = repr(rocauc_lr_glove_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/lr_glove_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_lr_glove_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_lr_glove_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_lr_glove_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_lr_glove_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_lr_glove_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_lr_glove_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_lr_glove_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_lr_glove_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_lr_glove_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_lr_glove_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe + Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Train Loss: 0.6625 | Val Loss: 0.6656 | F1 Score: 0.5401 | Balanced Accuracy: 0.5947 | AUC: 0.5947\n",
      "Epoch 2/15 | Train Loss: 0.6442 | Val Loss: 0.6688 | F1 Score: 0.5408 | Balanced Accuracy: 0.5966 | AUC: 0.5966\n",
      "Epoch 3/15 | Train Loss: 0.6248 | Val Loss: 0.6756 | F1 Score: 0.5421 | Balanced Accuracy: 0.5975 | AUC: 0.5975\n",
      "Epoch 4/15 | Train Loss: 0.6022 | Val Loss: 0.6829 | F1 Score: 0.5474 | Balanced Accuracy: 0.6011 | AUC: 0.6011\n",
      "Epoch 5/15 | Train Loss: 0.5762 | Val Loss: 0.6947 | F1 Score: 0.5529 | Balanced Accuracy: 0.5984 | AUC: 0.5984\n",
      "Epoch 6/15 | Train Loss: 0.5465 | Val Loss: 0.7073 | F1 Score: 0.5572 | Balanced Accuracy: 0.6018 | AUC: 0.6018\n",
      "Early stopping triggered. No improvement in 5 epochs.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[5043 1377]\n",
      " [1380 3724]] \n",
      "\n",
      "Accuracy: 76.1 \n",
      "\n",
      "F1 Score: 73.0 \n",
      "\n",
      "Balanced accuracy: 75.8 \n",
      "\n",
      "AUC Score: 75.8 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[454 260]\n",
      " [239 314]] \n",
      "\n",
      "Accuracy: 60.6 \n",
      "\n",
      "F1 Score: 55.7 \n",
      "\n",
      "Balanced accuracy: 60.2 \n",
      "\n",
      "AUC Score: 60.2 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# SET PARAMETERS\n",
    "dropout_rate = 0.1\n",
    "epochs_number = 15\n",
    "learning_rate = 0.0001\n",
    "early_stopping = 5\n",
    "# --------------------\n",
    "\n",
    "input_dim = X_train_embeddings_glove.shape[1]  # Dimensionality of word embeddings\n",
    "hidden_dim = X_train_embeddings_glove.shape[1]*2  # Number of units in the hidden layer\n",
    "\n",
    "classifier = NeuralNetworkClassifier(input_dim, hidden_dim, dropout=dropout_rate)\n",
    "\n",
    "# Fit the model\n",
    "classifier.fit(X_train_embeddings_glove, y_train, X_test_embeddings_glove, y_test, num_epochs=epochs_number, lr=learning_rate, patience=early_stopping)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_nn_glove_train, accuracy_nn_glove_train, f1_nn_glove_train, balaccuracy_nn_glove_train, rocauc_nn_glove_train = classifier.evaluate(X_train_embeddings_glove, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_nn_glove_test, accuracy_nn_glove_test, f1_nn_glove_test, balaccuracy_nn_glove_test, rocauc_nn_glove_test = classifier.evaluate(X_test_embeddings_glove, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_nn_glove_train_tosave = repr(cm_nn_glove_train)\n",
    "accuracy_nn_glove_train_tosave = repr(accuracy_nn_glove_train)\n",
    "f1_nn_glove_train_tosave = repr(f1_nn_glove_train)\n",
    "balaccuracy_nn_glove_train_tosave = repr(balaccuracy_nn_glove_train)\n",
    "rocauc_nn_glove_train_tosave = repr(rocauc_nn_glove_train)\n",
    "\n",
    "cm_nn_glove_test_tosave = repr(cm_nn_glove_test)\n",
    "accuracy_nn_glove_test_tosave = repr(accuracy_nn_glove_test)\n",
    "f1_nn_glove_test_tosave = repr(f1_nn_glove_test)\n",
    "balaccuracy_nn_glove_test_tosave = repr(balaccuracy_nn_glove_test)\n",
    "rocauc_nn_glove_test_tosave = repr(rocauc_nn_glove_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/nn_glove_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_nn_glove_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_nn_glove_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_nn_glove_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_nn_glove_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_nn_glove_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_nn_glove_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_nn_glove_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_nn_glove_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_nn_glove_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_nn_glove_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec + KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:00:41.587553598Z",
     "start_time": "2023-05-22T01:00:39.813202873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=6, weights=uniform;, score=0.429 total time=   0.2s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=6, weights=uniform;, score=0.380 total time=   0.1s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=6, weights=uniform;, score=0.407 total time=   0.1s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=6, weights=uniform;, score=0.387 total time=   0.1s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=6, weights=uniform;, score=0.419 total time=   0.1s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=2, weights=distance;, score=0.509 total time=   3.0s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=2, weights=distance;, score=0.494 total time=   3.0s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=2, weights=distance;, score=0.508 total time=   3.0s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=2, weights=distance;, score=0.478 total time=   3.0s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=2, weights=distance;, score=0.514 total time=   3.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.518 total time=   0.1s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.493 total time=   0.1s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.517 total time=   0.1s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.482 total time=   0.1s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.505 total time=   0.1s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=7, weights=distance;, score=0.503 total time=   3.0s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=7, weights=distance;, score=0.464 total time=   3.0s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=7, weights=distance;, score=0.490 total time=   3.0s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=7, weights=distance;, score=0.476 total time=   3.0s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=7, weights=distance;, score=0.494 total time=   2.9s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=7, weights=uniform;, score=0.523 total time=   0.1s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=7, weights=uniform;, score=0.467 total time=   0.1s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=7, weights=uniform;, score=0.493 total time=   0.1s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=7, weights=uniform;, score=0.475 total time=   0.1s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=7, weights=uniform;, score=0.500 total time=   0.1s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.526 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.471 total time=   0.1s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.491 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.490 total time=   0.1s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.511 total time=   0.1s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=5, weights=uniform;, score=0.505 total time=   0.1s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=5, weights=uniform;, score=0.477 total time=   0.1s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=5, weights=uniform;, score=0.476 total time=   0.1s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=5, weights=uniform;, score=0.467 total time=   0.1s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=5, weights=uniform;, score=0.497 total time=   0.1s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=6, weights=uniform;, score=0.429 total time=   0.1s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=6, weights=uniform;, score=0.380 total time=   0.1s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=6, weights=uniform;, score=0.407 total time=   0.1s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=6, weights=uniform;, score=0.387 total time=   0.1s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=6, weights=uniform;, score=0.419 total time=   0.1s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=9, weights=distance;, score=0.517 total time=   0.1s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=9, weights=distance;, score=0.467 total time=   0.1s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=9, weights=distance;, score=0.487 total time=   0.1s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=9, weights=distance;, score=0.482 total time=   0.1s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=9, weights=distance;, score=0.495 total time=   0.1s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.400 total time=   0.1s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.368 total time=   0.1s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.393 total time=   0.1s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.366 total time=   0.1s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.391 total time=   0.1s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.519 total time=   0.1s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.479 total time=   0.1s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.488 total time=   0.1s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.486 total time=   0.1s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.505 total time=   0.1s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=6, weights=distance;, score=0.512 total time=   2.9s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=6, weights=distance;, score=0.470 total time=   2.9s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=6, weights=distance;, score=0.500 total time=   2.9s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=6, weights=distance;, score=0.487 total time=   2.9s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=6, weights=distance;, score=0.509 total time=   2.9s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.507 total time=   2.9s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.469 total time=   2.9s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.507 total time=   3.0s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.477 total time=   3.0s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.521 total time=   3.1s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.514 total time=   0.1s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.478 total time=   0.1s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.494 total time=   0.1s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.483 total time=   0.1s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.510 total time=   0.1s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.516 total time=   0.1s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.462 total time=   0.1s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.483 total time=   0.1s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.482 total time=   0.1s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.494 total time=   0.1s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=8, weights=distance;, score=0.510 total time=   3.0s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=8, weights=distance;, score=0.474 total time=   3.0s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=8, weights=distance;, score=0.500 total time=   3.0s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=8, weights=distance;, score=0.489 total time=   3.0s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=8, weights=distance;, score=0.497 total time=   3.0s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=9, weights=distance;, score=0.521 total time=   3.0s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=9, weights=distance;, score=0.475 total time=   3.1s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=9, weights=distance;, score=0.477 total time=   3.0s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=9, weights=distance;, score=0.484 total time=   3.0s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=9, weights=distance;, score=0.508 total time=   3.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=8, weights=uniform;, score=0.439 total time=   0.1s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=8, weights=uniform;, score=0.401 total time=   0.1s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=8, weights=uniform;, score=0.410 total time=   0.1s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=8, weights=uniform;, score=0.403 total time=   0.1s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=8, weights=uniform;, score=0.422 total time=   0.1s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=7, weights=uniform;, score=0.523 total time=   0.1s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=7, weights=uniform;, score=0.467 total time=   0.1s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=7, weights=uniform;, score=0.493 total time=   0.1s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=7, weights=uniform;, score=0.475 total time=   0.1s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=7, weights=uniform;, score=0.500 total time=   0.1s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=2, weights=uniform;, score=0.369 total time=   0.1s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=2, weights=uniform;, score=0.352 total time=   0.1s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=2, weights=uniform;, score=0.365 total time=   0.1s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=2, weights=uniform;, score=0.335 total time=   0.1s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=2, weights=uniform;, score=0.343 total time=   0.1s\n",
      "Best parameters: {'weights': 'distance', 'n_neighbors': 2, 'metric': 'euclidean'} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[6420    0]\n",
      " [  17 5087]] \n",
      "\n",
      "Accuracy: 99.9 \n",
      "\n",
      "F1 Score: 99.8 \n",
      "\n",
      "Balanced accuracy: 99.8 \n",
      "\n",
      "AUC Score: 99.8 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[416 298]\n",
      " [285 268]] \n",
      "\n",
      "Accuracy: 54.0 \n",
      "\n",
      "F1 Score: 47.9 \n",
      "\n",
      "Balanced accuracy: 47.9 \n",
      "\n",
      "AUC Score: 53.4 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = KNNClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'weights' : ['uniform','distance'],\n",
    "    'metric' : ['minkowski','euclidean','manhattan']\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_word2vec, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_word2vec, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_knn_w2v_train, accuracy_knn_w2v_train, f1_knn_w2v_train, balaccuracy_knn_w2v_train, rocauc_knn_w2v_train = classifier.evaluate(X_train_embeddings_word2vec, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_knn_w2v_test, accuracy_knn_w2v_test, f1_knn_w2v_test, balaccuracy_knn_w2v_test, rocauc_knn_w2v_test = classifier.evaluate(X_test_embeddings_word2vec, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_knn_w2v_train_tosave = repr(cm_knn_w2v_train)\n",
    "accuracy_knn_w2v_train_tosave = repr(accuracy_knn_w2v_train)\n",
    "f1_knn_w2v_train_tosave = repr(f1_knn_w2v_train)\n",
    "balaccuracy_knn_w2v_train_tosave = repr(balaccuracy_knn_w2v_train)\n",
    "rocauc_knn_w2v_train_tosave = repr(rocauc_knn_w2v_train)\n",
    "\n",
    "cm_knn_w2v_test_tosave = repr(cm_knn_w2v_test)\n",
    "accuracy_knn_w2v_test_tosave = repr(accuracy_knn_w2v_test)\n",
    "f1_knn_w2v_test_tosave = repr(f1_knn_w2v_test)\n",
    "balaccuracy_knn_w2v_test_tosave = repr(balaccuracy_knn_w2v_test)\n",
    "rocauc_knn_w2v_test_tosave = repr(rocauc_knn_w2v_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/knn_w2v_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_knn_w2v_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_knn_w2v_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_knn_w2v_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_knn_w2v_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_knn_w2v_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_knn_w2v_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_knn_w2v_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_knn_w2v_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_knn_w2v_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_knn_w2v_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:01:42.307248583Z",
     "start_time": "2023-05-22T01:00:41.593893391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.471 total time=   3.7s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.488 total time=   2.4s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.487 total time=   2.4s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.495 total time=   2.4s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.503 total time=   2.4s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.432 total time=   3.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.414 total time=   4.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.400 total time=   4.4s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.397 total time=   4.3s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.428 total time=   4.2s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.464 total time=   5.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.486 total time=   5.9s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.485 total time=   6.4s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.506 total time=   6.3s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.485 total time=   5.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.502 total time=  23.8s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.467 total time=  23.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.491 total time=  23.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.504 total time=  23.1s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.498 total time=  23.4s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.487 total time=  13.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.488 total time=  13.1s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.497 total time= 1.8min\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.477 total time=  11.0s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.510 total time=  11.1s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.506 total time=   3.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.477 total time=   4.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.493 total time=   4.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.468 total time=   4.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.503 total time=   4.7s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.469 total time= 1.2min\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.472 total time= 1.1min\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.480 total time= 1.0min\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.494 total time= 1.0min\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.515 total time= 1.0min\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.493 total time=  26.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.460 total time=  26.0s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.468 total time=  26.2s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.489 total time=  26.2s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.477 total time=  26.2s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.490 total time=   8.3s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.470 total time=   7.9s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.479 total time=   8.1s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.487 total time=   7.9s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.489 total time=   8.0s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.491 total time=  11.0s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.464 total time=  11.9s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.468 total time=  11.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.445 total time=  12.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.493 total time=  11.8s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.504 total time=  35.3s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.496 total time=  36.3s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.495 total time=  36.1s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.469 total time=  35.0s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.521 total time=  34.7s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.491 total time=   3.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.491 total time=   3.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.487 total time=   3.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.474 total time=   3.3s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.513 total time=   3.2s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.472 total time=   4.4s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.496 total time=   4.4s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.486 total time=   4.5s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.491 total time=   4.7s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.503 total time=   4.9s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.464 total time=   2.3s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.470 total time=   2.2s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.462 total time=   2.3s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.458 total time=   2.2s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.484 total time=   2.3s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.464 total time=  11.3s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.496 total time=  10.8s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.472 total time=  10.7s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.476 total time=  10.7s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.472 total time=  10.7s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.489 total time=  19.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.483 total time=  19.9s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.508 total time=  18.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.492 total time=  18.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.518 total time=  18.6s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.505 total time=  20.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.498 total time=  20.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.482 total time=  20.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.485 total time=  20.0s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.505 total time=  20.2s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.368 total time=   2.7s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.346 total time=   2.8s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.363 total time=   2.9s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.353 total time=   3.0s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.360 total time=   2.7s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.475 total time=   9.8s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.451 total time=   9.8s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.458 total time=   9.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.446 total time=   9.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.471 total time=   9.7s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.460 total time=  37.0s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.512 total time=  37.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.494 total time=  36.0s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.491 total time=  35.9s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.497 total time=  35.8s\n",
      "Best parameters: {'subsample': 0.7, 'objective': 'reg:squarederror', 'n_estimators': 200, 'min_child_weight': 5, 'max_depth': 7, 'learning_rate': 0.3, 'colsample_bytree': 0.5} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[6413    7]\n",
      " [   5 5099]] \n",
      "\n",
      "Accuracy: 99.9 \n",
      "\n",
      "F1 Score: 99.9 \n",
      "\n",
      "Balanced accuracy: 99.9 \n",
      "\n",
      "AUC Score: 99.9 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[466 248]\n",
      " [295 258]] \n",
      "\n",
      "Accuracy: 57.1 \n",
      "\n",
      "F1 Score: 48.7 \n",
      "\n",
      "Balanced accuracy: 48.7 \n",
      "\n",
      "AUC Score: 56.0 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = XGBoostClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'learning_rate': [0.01, 0.1, 0.3, 0.5],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.5, 0.7],\n",
    "    'colsample_bytree': [0.5, 0.7],\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'objective': ['reg:squarederror']\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_word2vec, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_word2vec, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_xgb_w2v_train, accuracy_xgb_w2v_train, f1_xgb_w2v_train, balaccuracy_xgb_w2v_train, rocauc_xgb_w2v_train = classifier.evaluate(X_train_embeddings_word2vec, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_xgb_w2v_test, accuracy_xgb_w2v_test, f1_xgb_w2v_test, balaccuracy_xgb_w2v_test, rocauc_xgb_w2v_test = classifier.evaluate(X_test_embeddings_word2vec, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_xgb_w2v_train_tosave = repr(cm_xgb_w2v_train)\n",
    "accuracy_xgb_w2v_train_tosave = repr(accuracy_xgb_w2v_train)\n",
    "f1_xgb_w2v_train_tosave = repr(f1_xgb_w2v_train)\n",
    "balaccuracy_xgb_w2v_train_tosave = repr(balaccuracy_xgb_w2v_train)\n",
    "rocauc_xgb_w2v_train_tosave = repr(rocauc_xgb_w2v_train)\n",
    "\n",
    "cm_xgb_w2v_test_tosave = repr(cm_xgb_w2v_test)\n",
    "accuracy_xgb_w2v_test_tosave = repr(accuracy_xgb_w2v_test)\n",
    "f1_xgb_w2v_test_tosave = repr(f1_xgb_w2v_test)\n",
    "balaccuracy_xgb_w2v_test_tosave = repr(balaccuracy_xgb_w2v_test)\n",
    "rocauc_xgb_w2v_test_tosave = repr(rocauc_xgb_w2v_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/xgb_w2v_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_xgb_w2v_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_xgb_w2v_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_xgb_w2v_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_xgb_w2v_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_xgb_w2v_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_xgb_w2v_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_xgb_w2v_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_xgb_w2v_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_xgb_w2v_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_xgb_w2v_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:01:50.494926763Z",
     "start_time": "2023-05-22T01:01:42.307473981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.446 total time=   1.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.458 total time=   1.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.480 total time=   1.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.422 total time=   1.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.471 total time=   1.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.408 total time=   0.2s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.421 total time=   0.2s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.397 total time=   0.2s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.406 total time=   0.2s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.424 total time=   0.3s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.449 total time=   2.2s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.455 total time=   2.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.474 total time=   2.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.451 total time=   2.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.469 total time=   2.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.421 total time=   0.8s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.386 total time=   0.8s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.372 total time=   0.8s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.393 total time=   0.8s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.404 total time=   0.8s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.474 total time=   2.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.465 total time=   2.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.457 total time=   2.0s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.457 total time=   2.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.494 total time=   2.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.446 total time=   0.5s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.421 total time=   0.5s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.438 total time=   0.5s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.431 total time=   0.5s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.441 total time=   0.5s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.402 total time=   0.4s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.374 total time=   0.4s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.376 total time=   0.4s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.399 total time=   0.4s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.395 total time=   0.4s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.384 total time=   0.4s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.428 total time=   0.4s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.435 total time=   0.4s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.405 total time=   0.4s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.400 total time=   0.4s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.444 total time=   1.5s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.424 total time=   1.5s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.424 total time=   1.5s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.430 total time=   1.5s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.440 total time=   1.5s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.445 total time=   1.5s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.413 total time=   1.5s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.404 total time=   1.5s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.431 total time=   1.4s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.439 total time=   1.6s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.390 total time=   0.8s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.407 total time=   0.8s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.410 total time=   0.8s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.391 total time=   0.8s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.421 total time=   0.8s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.402 total time=   0.8s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.398 total time=   0.8s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.397 total time=   0.8s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.407 total time=   0.8s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.441 total time=   0.8s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.461 total time=   1.2s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.460 total time=   1.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.455 total time=   1.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.455 total time=   1.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.454 total time=   1.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.469 total time=   1.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.449 total time=   1.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.480 total time=   1.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.453 total time=   1.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.458 total time=   1.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.437 total time=   2.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.465 total time=   2.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.473 total time=   2.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.440 total time=   2.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.469 total time=   2.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.412 total time=   0.8s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.376 total time=   0.8s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.393 total time=   0.9s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.393 total time=   0.8s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.427 total time=   0.8s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.444 total time=   0.5s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.405 total time=   0.5s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.448 total time=   0.5s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.454 total time=   0.5s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.447 total time=   0.5s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.457 total time=   2.2s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.465 total time=   2.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.464 total time=   2.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.454 total time=   2.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.458 total time=   2.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.437 total time=   0.3s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.424 total time=   0.3s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.410 total time=   0.3s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.421 total time=   0.2s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.447 total time=   0.2s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.393 total time=   0.7s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.402 total time=   0.7s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.400 total time=   0.7s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.401 total time=   0.7s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.426 total time=   0.7s\n",
      "Best parameters: {'n_estimators': 25, 'max_features': 10, 'max_depth': 50, 'bootstrap': False} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[6416    4]\n",
      " [   8 5096]] \n",
      "\n",
      "Accuracy: 99.9 \n",
      "\n",
      "F1 Score: 99.9 \n",
      "\n",
      "Balanced accuracy: 99.9 \n",
      "\n",
      "AUC Score: 99.9 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[500 214]\n",
      " [320 233]] \n",
      "\n",
      "Accuracy: 57.9 \n",
      "\n",
      "F1 Score: 46.6 \n",
      "\n",
      "Balanced accuracy: 46.6 \n",
      "\n",
      "AUC Score: 56.1 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = RFClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'n_estimators': [10, 25],\n",
    "    'max_features': [5, 10],\n",
    "    'max_depth': [10, 50, None],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_word2vec, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_word2vec, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_rf_w2v_train, accuracy_rf_w2v_train, f1_rf_w2v_train, balaccuracy_rf_w2v_train, rocauc_rf_w2v_train = classifier.evaluate(X_train_embeddings_word2vec, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_rf_w2v_test, accuracy_rf_w2v_test, f1_rf_w2v_test, balaccuracy_rf_w2v_test, rocauc_rf_w2v_test = classifier.evaluate(X_test_embeddings_word2vec, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_rf_w2v_train_tosave = repr(cm_rf_w2v_train)\n",
    "accuracy_rf_w2v_train_tosave = repr(accuracy_rf_w2v_train)\n",
    "f1_rf_w2v_train_tosave = repr(f1_rf_w2v_train)\n",
    "balaccuracy_rf_w2v_train_tosave = repr(balaccuracy_rf_w2v_train)\n",
    "rocauc_rf_w2v_train_tosave = repr(rocauc_rf_w2v_train)\n",
    "\n",
    "cm_rf_w2v_test_tosave = repr(cm_rf_w2v_test)\n",
    "accuracy_rf_w2v_test_tosave = repr(accuracy_rf_w2v_test)\n",
    "f1_rf_w2v_test_tosave = repr(f1_rf_w2v_test)\n",
    "balaccuracy_rf_w2v_test_tosave = repr(balaccuracy_rf_w2v_test)\n",
    "rocauc_rf_w2v_test_tosave = repr(rocauc_rf_w2v_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/rf_w2v_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_rf_w2v_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_rf_w2v_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_rf_w2v_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_rf_w2v_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_rf_w2v_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_rf_w2v_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_rf_w2v_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_rf_w2v_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_rf_w2v_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_rf_w2v_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec + SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:03:31.197761231Z",
     "start_time": "2023-05-22T01:01:50.494732024Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:305: UserWarning: The total space of parameters 16 is smaller than n_iter=20. Running 16 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV 1/5] END ...C=1, gamma=scale, kernel=linear;, score=0.492 total time=  11.8s\n",
      "[CV 2/5] END ...C=1, gamma=scale, kernel=linear;, score=0.486 total time=  11.6s\n",
      "[CV 3/5] END ...C=1, gamma=scale, kernel=linear;, score=0.495 total time=  11.7s\n",
      "[CV 4/5] END ...C=1, gamma=scale, kernel=linear;, score=0.495 total time=  11.4s\n",
      "[CV 5/5] END ...C=1, gamma=scale, kernel=linear;, score=0.498 total time=  11.7s\n",
      "[CV 1/5] END .....C=1, gamma=scale, kernel=poly;, score=0.465 total time=  12.8s\n",
      "[CV 2/5] END .....C=1, gamma=scale, kernel=poly;, score=0.456 total time=  12.7s\n",
      "[CV 3/5] END .....C=1, gamma=scale, kernel=poly;, score=0.450 total time=  12.7s\n",
      "[CV 4/5] END .....C=1, gamma=scale, kernel=poly;, score=0.463 total time=  12.3s\n",
      "[CV 5/5] END .....C=1, gamma=scale, kernel=poly;, score=0.449 total time=  12.4s\n",
      "[CV 1/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.518 total time=  14.7s\n",
      "[CV 2/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.504 total time=  14.9s\n",
      "[CV 3/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.528 total time=  14.8s\n",
      "[CV 4/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.510 total time=  14.8s\n",
      "[CV 5/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.530 total time=  14.9s\n",
      "[CV 1/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.476 total time=  10.5s\n",
      "[CV 2/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.489 total time=  10.7s\n",
      "[CV 3/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.468 total time=  10.5s\n",
      "[CV 4/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.473 total time=  10.7s\n",
      "[CV 5/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.475 total time=  10.6s\n",
      "[CV 1/5] END ..C=10, gamma=scale, kernel=linear;, score=0.494 total time=  15.1s\n",
      "[CV 2/5] END ..C=10, gamma=scale, kernel=linear;, score=0.484 total time=  15.1s\n",
      "[CV 3/5] END ..C=10, gamma=scale, kernel=linear;, score=0.501 total time=  15.3s\n",
      "[CV 4/5] END ..C=10, gamma=scale, kernel=linear;, score=0.503 total time=  15.0s\n",
      "[CV 5/5] END ..C=10, gamma=scale, kernel=linear;, score=0.508 total time=  15.2s\n",
      "[CV 1/5] END ....C=10, gamma=scale, kernel=poly;, score=0.517 total time=  28.3s\n",
      "[CV 2/5] END ....C=10, gamma=scale, kernel=poly;, score=0.510 total time=  28.1s\n",
      "[CV 3/5] END ....C=10, gamma=scale, kernel=poly;, score=0.505 total time=  28.2s\n",
      "[CV 4/5] END ....C=10, gamma=scale, kernel=poly;, score=0.520 total time=  26.9s\n",
      "[CV 5/5] END ....C=10, gamma=scale, kernel=poly;, score=0.532 total time=  28.4s\n",
      "[CV 1/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.524 total time=  33.0s\n",
      "[CV 2/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.523 total time=  37.3s\n",
      "[CV 3/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.512 total time=  35.4s\n",
      "[CV 4/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.537 total time=  35.4s\n",
      "[CV 5/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.541 total time=  35.5s\n",
      "[CV 1/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.471 total time=   7.1s\n",
      "[CV 2/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.481 total time=   7.4s\n",
      "[CV 3/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.470 total time=   7.5s\n",
      "[CV 4/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.459 total time=   7.6s\n",
      "[CV 5/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.466 total time=   7.6s\n",
      "[CV 1/5] END .C=100, gamma=scale, kernel=linear;, score=0.496 total time=  40.3s\n",
      "[CV 2/5] END .C=100, gamma=scale, kernel=linear;, score=0.484 total time=  38.8s\n",
      "[CV 3/5] END .C=100, gamma=scale, kernel=linear;, score=0.501 total time=  39.7s\n",
      "[CV 4/5] END .C=100, gamma=scale, kernel=linear;, score=0.508 total time=  38.7s\n",
      "[CV 5/5] END .C=100, gamma=scale, kernel=linear;, score=0.515 total time=  39.4s\n",
      "[CV 1/5] END ...C=100, gamma=scale, kernel=poly;, score=0.521 total time=  36.2s\n",
      "[CV 2/5] END ...C=100, gamma=scale, kernel=poly;, score=0.514 total time=  34.6s\n",
      "[CV 3/5] END ...C=100, gamma=scale, kernel=poly;, score=0.515 total time=  34.2s\n",
      "[CV 4/5] END ...C=100, gamma=scale, kernel=poly;, score=0.511 total time=  36.3s\n",
      "[CV 5/5] END ...C=100, gamma=scale, kernel=poly;, score=0.544 total time=  35.6s\n",
      "[CV 1/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.528 total time=  53.9s\n",
      "[CV 2/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.520 total time=  53.8s\n",
      "[CV 3/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.522 total time=  52.2s\n",
      "[CV 4/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.518 total time=  53.7s\n",
      "[CV 5/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.547 total time=  55.0s\n",
      "[CV 1/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.468 total time=   6.9s\n",
      "[CV 2/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.474 total time=   7.2s\n",
      "[CV 3/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.475 total time=   7.0s\n",
      "[CV 4/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.460 total time=   7.0s\n",
      "[CV 5/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.465 total time=   7.1s\n",
      "[CV 1/5] END C=1000, gamma=scale, kernel=linear;, score=0.497 total time= 4.7min\n",
      "[CV 2/5] END C=1000, gamma=scale, kernel=linear;, score=0.484 total time= 4.2min\n",
      "[CV 3/5] END C=1000, gamma=scale, kernel=linear;, score=0.503 total time= 4.4min\n",
      "[CV 4/5] END C=1000, gamma=scale, kernel=linear;, score=0.507 total time= 4.0min\n",
      "[CV 5/5] END C=1000, gamma=scale, kernel=linear;, score=0.517 total time= 4.4min\n",
      "[CV 1/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.518 total time=  34.9s\n",
      "[CV 2/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.517 total time=  35.7s\n",
      "[CV 3/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.513 total time=  35.7s\n",
      "[CV 4/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.512 total time=  34.7s\n",
      "[CV 5/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.536 total time=  35.3s\n",
      "[CV 1/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.528 total time=  54.0s\n",
      "[CV 2/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.521 total time=  53.6s\n",
      "[CV 3/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.522 total time=  53.2s\n",
      "[CV 4/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.518 total time=  53.9s\n",
      "[CV 5/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.545 total time=  54.8s\n",
      "[CV 1/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.475 total time=   7.0s\n",
      "[CV 2/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.477 total time=   7.1s\n",
      "[CV 3/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.475 total time=   7.0s\n",
      "[CV 4/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.466 total time=   6.6s\n",
      "[CV 5/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.470 total time=   7.0s\n",
      "Best parameters: {'kernel': 'rbf', 'gamma': 'scale', 'C': 10} \n",
      "\n",
      "[LibSVM]\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[6378   42]\n",
      " [ 120 4984]] \n",
      "\n",
      "Accuracy: 98.6 \n",
      "\n",
      "F1 Score: 98.4 \n",
      "\n",
      "Balanced accuracy: 98.4 \n",
      "\n",
      "AUC Score: 98.5 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[483 231]\n",
      " [270 283]] \n",
      "\n",
      "Accuracy: 60.5 \n",
      "\n",
      "F1 Score: 53.0 \n",
      "\n",
      "Balanced accuracy: 53.0 \n",
      "\n",
      "AUC Score: 59.4 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = SVClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'C': [1, 10, 100, 1000],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale']\n",
    "    #'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_word2vec, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_word2vec, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_svc_w2v_train, accuracy_svc_w2v_train, f1_svc_w2v_train, balaccuracy_svc_w2v_train, rocauc_svc_w2v_train = classifier.evaluate(X_train_embeddings_word2vec, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_svc_w2v_test, accuracy_svc_w2v_test, f1_svc_w2v_test, balaccuracy_svc_w2v_test, rocauc_svc_w2v_test = classifier.evaluate(X_test_embeddings_word2vec, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_svc_w2v_train_tosave = repr(cm_svc_w2v_train)\n",
    "accuracy_svc_w2v_train_tosave = repr(accuracy_svc_w2v_train)\n",
    "f1_svc_w2v_train_tosave = repr(f1_svc_w2v_train)\n",
    "balaccuracy_svc_w2v_train_tosave = repr(balaccuracy_svc_w2v_train)\n",
    "rocauc_svc_w2v_train_tosave = repr(rocauc_svc_w2v_train)\n",
    "\n",
    "cm_svc_w2v_test_tosave = repr(cm_svc_w2v_test)\n",
    "accuracy_svc_w2v_test_tosave = repr(accuracy_svc_w2v_test)\n",
    "f1_svc_w2v_test_tosave = repr(f1_svc_w2v_test)\n",
    "balaccuracy_svc_w2v_test_tosave = repr(balaccuracy_svc_w2v_test)\n",
    "rocauc_svc_w2v_test_tosave = repr(rocauc_svc_w2v_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/svc_w2v_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_svc_w2v_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_svc_w2v_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_svc_w2v_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_svc_w2v_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_svc_w2v_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_svc_w2v_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_svc_w2v_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_svc_w2v_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_svc_w2v_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_svc_w2v_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:03:36.298464070Z",
     "start_time": "2023-05-22T01:03:31.200979988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END C=0.8300000000000001, penalty=l1, solver=saga;, score=0.508 total time=   0.5s\n",
      "[CV 2/5] END C=0.8300000000000001, penalty=l1, solver=saga;, score=0.486 total time=   0.5s\n",
      "[CV 3/5] END C=0.8300000000000001, penalty=l1, solver=saga;, score=0.506 total time=   0.5s\n",
      "[CV 4/5] END C=0.8300000000000001, penalty=l1, solver=saga;, score=0.499 total time=   0.6s\n",
      "[CV 5/5] END C=0.8300000000000001, penalty=l1, solver=saga;, score=0.507 total time=   0.6s\n",
      "[CV 1/5] END ....C=0.81, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ....C=0.81, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ....C=0.81, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ....C=0.81, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ....C=0.81, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.46, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.46, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.46, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.46, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.46, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.99, penalty=l1, solver=liblinear;, score=0.509 total time=   2.0s\n",
      "[CV 2/5] END C=0.99, penalty=l1, solver=liblinear;, score=0.493 total time=   0.2s\n",
      "[CV 3/5] END C=0.99, penalty=l1, solver=liblinear;, score=0.505 total time=   0.2s\n",
      "[CV 4/5] END C=0.99, penalty=l1, solver=liblinear;, score=0.504 total time=   1.9s\n",
      "[CV 5/5] END C=0.99, penalty=l1, solver=liblinear;, score=0.509 total time=   0.5s\n",
      "[CV 1/5] END C=0.98, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.98, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.98, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.98, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.98, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ....C=0.02, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ....C=0.02, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ....C=0.02, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ....C=0.02, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ....C=0.02, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END .C=0.13, penalty=none, solver=saga;, score=0.499 total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END .C=0.13, penalty=none, solver=saga;, score=0.497 total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END .C=0.13, penalty=none, solver=saga;, score=0.505 total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END .C=0.13, penalty=none, solver=saga;, score=0.499 total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END .C=0.13, penalty=none, solver=saga;, score=0.511 total time=   0.6s\n",
      "[CV 1/5] END C=0.61, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.61, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.61, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.61, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.61, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ..C=0.79, penalty=none, solver=sag;, score=0.499 total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END ..C=0.79, penalty=none, solver=sag;, score=0.497 total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END ..C=0.79, penalty=none, solver=sag;, score=0.505 total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END ..C=0.79, penalty=none, solver=sag;, score=0.499 total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END ..C=0.79, penalty=none, solver=sag;, score=0.511 total time=   0.4s\n",
      "[CV 1/5] END C=0.59, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.59, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.59, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.59, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.59, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ...C=0.87, penalty=l2, solver=saga;, score=0.508 total time=   0.4s\n",
      "[CV 2/5] END ...C=0.87, penalty=l2, solver=saga;, score=0.498 total time=   0.4s\n",
      "[CV 3/5] END ...C=0.87, penalty=l2, solver=saga;, score=0.502 total time=   0.4s\n",
      "[CV 4/5] END ...C=0.87, penalty=l2, solver=saga;, score=0.498 total time=   0.4s\n",
      "[CV 5/5] END ...C=0.87, penalty=l2, solver=saga;, score=0.516 total time=   0.4s\n",
      "[CV 1/5] END C=0.81, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.81, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.81, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.81, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.81, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.43, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.43, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.43, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.43, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.43, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.64, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.64, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.64, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.64, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.64, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ......C=0.66, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ......C=0.66, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ......C=0.66, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ......C=0.66, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ......C=0.66, penalty=l1, solver=sag;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ..C=0.49, penalty=none, solver=sag;, score=0.499 total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END ..C=0.49, penalty=none, solver=sag;, score=0.497 total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END ..C=0.49, penalty=none, solver=sag;, score=0.505 total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END ..C=0.49, penalty=none, solver=sag;, score=0.499 total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END ..C=0.49, penalty=none, solver=sag;, score=0.511 total time=   0.4s\n",
      "[CV 1/5] END C=0.17, penalty=none, solver=lbfgs;, score=0.499 total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=0.17, penalty=none, solver=lbfgs;, score=0.497 total time=   0.0s\n",
      "[CV 3/5] END C=0.17, penalty=none, solver=lbfgs;, score=0.505 total time=   0.0s\n",
      "[CV 4/5] END C=0.17, penalty=none, solver=lbfgs;, score=0.499 total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=0.17, penalty=none, solver=lbfgs;, score=0.511 total time=   0.0s\n",
      "[CV 1/5] END ....C=0.67, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ....C=0.67, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ....C=0.67, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ....C=0.67, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ....C=0.67, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=0.53, penalty=none, solver=newton-cg;, score=0.499 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=0.53, penalty=none, solver=newton-cg;, score=0.497 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=0.53, penalty=none, solver=newton-cg;, score=0.505 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=0.53, penalty=none, solver=newton-cg;, score=0.499 total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=0.53, penalty=none, solver=newton-cg;, score=0.511 total time=   0.1s\n",
      "[CV 1/5] END .....C=0.9, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .....C=0.9, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .....C=0.9, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .....C=0.9, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .....C=0.9, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "60 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1048, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 864, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 782, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.50136391        nan        nan 0.50395283        nan        nan\n",
      " 0.50242539        nan 0.50242539        nan 0.50438131        nan\n",
      "        nan        nan        nan 0.50237119 0.50237228        nan\n",
      " 0.50242539        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'solver': 'saga', 'penalty': 'l2', 'C': 0.87} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[4838 1582]\n",
      " [2624 2480]] \n",
      "\n",
      "Accuracy: 63.5 \n",
      "\n",
      "F1 Score: 54.1 \n",
      "\n",
      "Balanced accuracy: 54.1 \n",
      "\n",
      "AUC Score: 62.0 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[540 174]\n",
      " [311 242]] \n",
      "\n",
      "Accuracy: 61.7 \n",
      "\n",
      "F1 Score: 49.9 \n",
      "\n",
      "Balanced accuracy: 49.9 \n",
      "\n",
      "AUC Score: 59.7 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = LRClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'C' : np.arange(0, 1, 0.01)\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_word2vec, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_word2vec, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_lr_w2v_train, accuracy_lr_w2v_train, f1_lr_w2v_train, balaccuracy_lr_w2v_train, rocauc_lr_w2v_train = classifier.evaluate(X_train_embeddings_word2vec, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_lr_w2v_test, accuracy_lr_w2v_test, f1_lr_w2v_test, balaccuracy_lr_w2v_test, rocauc_lr_w2v_test = classifier.evaluate(X_test_embeddings_word2vec, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_lr_w2v_train_tosave = repr(cm_lr_w2v_train)\n",
    "accuracy_lr_w2v_train_tosave = repr(accuracy_lr_w2v_train)\n",
    "f1_lr_w2v_train_tosave = repr(f1_lr_w2v_train)\n",
    "balaccuracy_lr_w2v_train_tosave = repr(balaccuracy_lr_w2v_train)\n",
    "rocauc_lr_w2v_train_tosave = repr(rocauc_lr_w2v_train)\n",
    "\n",
    "cm_lr_w2v_test_tosave = repr(cm_lr_w2v_test)\n",
    "accuracy_lr_w2v_test_tosave = repr(accuracy_lr_w2v_test)\n",
    "f1_lr_w2v_test_tosave = repr(f1_lr_w2v_test)\n",
    "balaccuracy_lr_w2v_test_tosave = repr(balaccuracy_lr_w2v_test)\n",
    "rocauc_lr_w2v_test_tosave = repr(rocauc_lr_w2v_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/lr_w2v_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_lr_w2v_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_lr_w2v_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_lr_w2v_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_lr_w2v_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_lr_w2v_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_lr_w2v_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_lr_w2v_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_lr_w2v_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_lr_w2v_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_lr_w2v_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec + Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:03:42.049533783Z",
     "start_time": "2023-05-22T01:03:36.300816863Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Train Loss: 0.6641 | Val Loss: 0.6602 | F1 Score: 0.5647 | Balanced Accuracy: 0.6123 | AUC: 0.6123\n",
      "Epoch 2/15 | Train Loss: 0.6487 | Val Loss: 0.6617 | F1 Score: 0.5575 | Balanced Accuracy: 0.6076 | AUC: 0.6076\n",
      "Epoch 3/15 | Train Loss: 0.6376 | Val Loss: 0.6652 | F1 Score: 0.5570 | Balanced Accuracy: 0.6115 | AUC: 0.6115\n",
      "Epoch 4/15 | Train Loss: 0.6240 | Val Loss: 0.6709 | F1 Score: 0.5506 | Balanced Accuracy: 0.6065 | AUC: 0.6065\n",
      "Epoch 5/15 | Train Loss: 0.6077 | Val Loss: 0.6777 | F1 Score: 0.5426 | Balanced Accuracy: 0.6011 | AUC: 0.6011\n",
      "Epoch 6/15 | Train Loss: 0.5886 | Val Loss: 0.6861 | F1 Score: 0.5460 | Balanced Accuracy: 0.5990 | AUC: 0.5990\n",
      "Early stopping triggered. No improvement in 5 epochs.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[4710 1710]\n",
      " [1659 3445]] \n",
      "\n",
      "Accuracy: 70.8 \n",
      "\n",
      "F1 Score: 67.2 \n",
      "\n",
      "Balanced accuracy: 70.4 \n",
      "\n",
      "AUC Score: 70.4 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[468 246]\n",
      " [253 300]] \n",
      "\n",
      "Accuracy: 60.6 \n",
      "\n",
      "F1 Score: 54.6 \n",
      "\n",
      "Balanced accuracy: 59.9 \n",
      "\n",
      "AUC Score: 59.9 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# SET PARAMETERS\n",
    "dropout_rate = 0.1\n",
    "epochs_number = 15\n",
    "learning_rate = 0.0001\n",
    "early_stopping = 5\n",
    "# --------------------\n",
    "\n",
    "input_dim = X_train_embeddings_word2vec.shape[1]  # Dimensionality of word embeddings\n",
    "hidden_dim = X_train_embeddings_word2vec.shape[1]*2  # Number of units in the hidden layer\n",
    "\n",
    "classifier = NeuralNetworkClassifier(input_dim, hidden_dim, dropout=dropout_rate)\n",
    "\n",
    "# Fit the model\n",
    "classifier.fit(X_train_embeddings_word2vec, y_train, X_test_embeddings_word2vec, y_test, num_epochs=epochs_number, lr=learning_rate, patience=early_stopping)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_nn_w2v_train, accuracy_nn_w2v_train, f1_nn_w2v_train, balaccuracy_nn_w2v_train, rocauc_nn_w2v_train = classifier.evaluate(X_train_embeddings_word2vec, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_nn_w2v_test, accuracy_nn_w2v_test, f1_nn_w2v_test, balaccuracy_nn_w2v_test, rocauc_nn_w2v_test = classifier.evaluate(X_test_embeddings_word2vec, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_nn_w2v_train_tosave = repr(cm_nn_w2v_train)\n",
    "accuracy_nn_w2v_train_tosave = repr(accuracy_nn_w2v_train)\n",
    "f1_nn_w2v_train_tosave = repr(f1_nn_w2v_train)\n",
    "balaccuracy_nn_w2v_train_tosave = repr(balaccuracy_nn_w2v_train)\n",
    "rocauc_nn_w2v_train_tosave = repr(rocauc_nn_w2v_train)\n",
    "\n",
    "cm_nn_w2v_test_tosave = repr(cm_nn_w2v_test)\n",
    "accuracy_nn_w2v_test_tosave = repr(accuracy_nn_w2v_test)\n",
    "f1_nn_w2v_test_tosave = repr(f1_nn_w2v_test)\n",
    "balaccuracy_nn_w2v_test_tosave = repr(balaccuracy_nn_w2v_test)\n",
    "rocauc_nn_w2v_test_tosave = repr(rocauc_nn_w2v_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/nn_w2v_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_nn_w2v_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_nn_w2v_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_nn_w2v_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_nn_w2v_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_nn_w2v_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_nn_w2v_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_nn_w2v_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_nn_w2v_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_nn_w2v_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_nn_w2v_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT2 + KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:03:45.270796608Z",
     "start_time": "2023-05-22T01:03:42.048900808Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.512 total time=   0.3s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.460 total time=   0.3s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.514 total time=   0.3s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.433 total time=   0.2s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.501 total time=   0.3s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=8, weights=distance;, score=0.502 total time=   7.7s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=8, weights=distance;, score=0.466 total time=   7.5s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=8, weights=distance;, score=0.516 total time=   7.5s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=8, weights=distance;, score=0.431 total time=   7.5s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=8, weights=distance;, score=0.506 total time=   7.5s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.516 total time=   0.2s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.450 total time=   0.2s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.499 total time=   0.3s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.444 total time=   0.2s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.485 total time=   0.3s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.392 total time=   0.3s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.320 total time=   0.3s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.409 total time=   0.3s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.319 total time=   0.3s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.374 total time=   0.3s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=9, weights=uniform;, score=0.487 total time=   0.3s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=9, weights=uniform;, score=0.474 total time=   0.3s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=9, weights=uniform;, score=0.502 total time=   0.3s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=9, weights=uniform;, score=0.431 total time=   0.3s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=9, weights=uniform;, score=0.498 total time=   0.3s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=3, weights=uniform;, score=0.508 total time=   0.3s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=3, weights=uniform;, score=0.441 total time=   0.3s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=3, weights=uniform;, score=0.499 total time=   0.3s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=3, weights=uniform;, score=0.434 total time=   0.3s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=3, weights=uniform;, score=0.488 total time=   0.3s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=8, weights=distance;, score=0.491 total time=   0.3s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=8, weights=distance;, score=0.461 total time=   0.3s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=8, weights=distance;, score=0.516 total time=   0.2s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=8, weights=distance;, score=0.429 total time=   0.3s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=8, weights=distance;, score=0.495 total time=   0.3s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=4, weights=uniform;, score=0.392 total time=   0.3s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=4, weights=uniform;, score=0.320 total time=   0.3s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=4, weights=uniform;, score=0.409 total time=   0.3s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=4, weights=uniform;, score=0.319 total time=   0.3s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=4, weights=uniform;, score=0.374 total time=   0.3s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=9, weights=distance;, score=0.486 total time=   7.5s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=9, weights=distance;, score=0.467 total time=   7.5s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=9, weights=distance;, score=0.510 total time=   7.5s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=9, weights=distance;, score=0.424 total time=   7.5s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=9, weights=distance;, score=0.485 total time=   7.6s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=4, weights=distance;, score=0.516 total time=   0.3s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=4, weights=distance;, score=0.450 total time=   0.3s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=4, weights=distance;, score=0.499 total time=   0.2s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=4, weights=distance;, score=0.444 total time=   0.3s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=4, weights=distance;, score=0.485 total time=   0.3s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.487 total time=   0.3s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.474 total time=   0.3s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.502 total time=   0.3s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.431 total time=   0.3s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.498 total time=   0.3s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=2, weights=uniform;, score=0.339 total time=   0.3s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=2, weights=uniform;, score=0.296 total time=   0.3s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=2, weights=uniform;, score=0.330 total time=   0.3s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=2, weights=uniform;, score=0.290 total time=   0.3s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=2, weights=uniform;, score=0.347 total time=   0.3s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=9, weights=distance;, score=0.488 total time=   0.2s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=9, weights=distance;, score=0.476 total time=   0.3s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=9, weights=distance;, score=0.502 total time=   0.3s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=9, weights=distance;, score=0.432 total time=   0.2s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=9, weights=distance;, score=0.499 total time=   0.2s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=6, weights=uniform;, score=0.421 total time=   7.6s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=6, weights=uniform;, score=0.379 total time=   7.5s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=6, weights=uniform;, score=0.445 total time=   7.6s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=6, weights=uniform;, score=0.327 total time=   7.5s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=6, weights=uniform;, score=0.386 total time=   7.5s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=3, weights=distance;, score=0.497 total time=   7.5s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=3, weights=distance;, score=0.456 total time=   7.5s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=3, weights=distance;, score=0.504 total time=   7.4s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=3, weights=distance;, score=0.428 total time=   7.5s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=3, weights=distance;, score=0.490 total time=   7.5s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=3, weights=uniform;, score=0.497 total time=   7.6s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=3, weights=uniform;, score=0.456 total time=   7.5s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=3, weights=uniform;, score=0.503 total time=   7.5s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=3, weights=uniform;, score=0.427 total time=   7.5s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=3, weights=uniform;, score=0.489 total time=   7.5s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.512 total time=   7.5s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.459 total time=   7.5s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.508 total time=   7.5s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.443 total time=   7.5s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.490 total time=   7.4s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=5, weights=uniform;, score=0.508 total time=   0.3s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=5, weights=uniform;, score=0.457 total time=   0.3s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=5, weights=uniform;, score=0.512 total time=   0.3s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=5, weights=uniform;, score=0.438 total time=   0.3s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=5, weights=uniform;, score=0.501 total time=   0.3s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=7, weights=distance;, score=0.492 total time=   7.5s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=7, weights=distance;, score=0.464 total time=   7.5s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=7, weights=distance;, score=0.513 total time=   7.6s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=7, weights=distance;, score=0.451 total time=   7.5s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=7, weights=distance;, score=0.489 total time=   7.5s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.491 total time=   0.3s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.461 total time=   0.3s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.516 total time=   0.3s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.429 total time=   0.3s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.495 total time=   0.3s\n",
      "Best parameters: {'weights': 'distance', 'n_neighbors': 6, 'metric': 'euclidean'} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[6418    2]\n",
      " [   2 5102]] \n",
      "\n",
      "Accuracy: 100.0 \n",
      "\n",
      "F1 Score: 100.0 \n",
      "\n",
      "Balanced accuracy: 100.0 \n",
      "\n",
      "AUC Score: 100.0 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[483 231]\n",
      " [336 217]] \n",
      "\n",
      "Accuracy: 55.2 \n",
      "\n",
      "F1 Score: 43.4 \n",
      "\n",
      "Balanced accuracy: 43.4 \n",
      "\n",
      "AUC Score: 53.4 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = KNNClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'weights' : ['uniform','distance'],\n",
    "    'metric' : ['minkowski','euclidean','manhattan']\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_gpt2, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_gpt2, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_knn_gpt2_train, accuracy_knn_gpt2_train, f1_knn_gpt2_train, balaccuracy_knn_gpt2_train, rocauc_knn_gpt2_train = classifier.evaluate(X_train_embeddings_gpt2, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_knn_gpt2_test, accuracy_knn_gpt2_test, f1_knn_gpt2_test, balaccuracy_knn_gpt2_test, rocauc_knn_gpt2_test = classifier.evaluate(X_test_embeddings_gpt2, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_knn_gpt2_train_tosave = repr(cm_knn_gpt2_train)\n",
    "accuracy_knn_gpt2_train_tosave = repr(accuracy_knn_gpt2_train)\n",
    "f1_knn_gpt2_train_tosave = repr(f1_knn_gpt2_train)\n",
    "balaccuracy_knn_gpt2_train_tosave = repr(balaccuracy_knn_gpt2_train)\n",
    "rocauc_knn_gpt2_train_tosave = repr(rocauc_knn_gpt2_train)\n",
    "\n",
    "cm_knn_gpt2_test_tosave = repr(cm_knn_gpt2_test)\n",
    "accuracy_knn_gpt2_test_tosave = repr(accuracy_knn_gpt2_test)\n",
    "f1_knn_gpt2_test_tosave = repr(f1_knn_gpt2_test)\n",
    "balaccuracy_knn_gpt2_test_tosave = repr(balaccuracy_knn_gpt2_test)\n",
    "rocauc_knn_gpt2_test_tosave = repr(rocauc_knn_gpt2_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/knn_gpt2_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_knn_gpt2_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_knn_gpt2_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_knn_gpt2_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_knn_gpt2_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_knn_gpt2_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_knn_gpt2_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_knn_gpt2_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_knn_gpt2_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_knn_gpt2_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_knn_gpt2_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT2 + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:05:40.285539512Z",
     "start_time": "2023-05-22T01:03:45.270056163Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.489 total time=  16.6s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.487 total time=  16.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.494 total time=  16.4s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.484 total time=  16.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.497 total time=  16.5s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.491 total time=  13.9s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.512 total time=  13.8s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.494 total time=  13.8s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.490 total time=  13.9s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.492 total time=  13.8s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.384 total time=  10.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.392 total time=  10.5s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.374 total time=  10.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.376 total time=  10.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.361 total time=  10.4s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.477 total time=   9.1s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.490 total time=   8.9s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.484 total time=   8.9s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.468 total time=   8.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.482 total time=   8.9s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.393 total time=  11.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.401 total time=  11.8s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.390 total time=  11.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.379 total time=  11.5s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.382 total time=  11.6s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.492 total time=  50.8s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.479 total time=  50.8s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.483 total time=  50.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.481 total time=  50.4s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.502 total time=  50.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.488 total time=  27.7s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.501 total time=  27.5s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.498 total time=  27.7s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.486 total time=  27.5s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.501 total time=  27.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.465 total time=  52.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.497 total time=  51.8s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.474 total time=  52.1s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.478 total time=  52.3s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.485 total time=  51.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=7, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.482 total time=  19.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=7, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.476 total time=  19.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=7, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.474 total time=  19.7s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=7, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.486 total time=  19.5s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=7, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.499 total time=  19.4s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.490 total time= 2.0min\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.523 total time= 2.0min\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.525 total time= 2.0min\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.512 total time= 2.0min\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.524 total time= 2.0min\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.500 total time=  23.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.500 total time=  23.5s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.506 total time=  23.3s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.492 total time=  23.3s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.501 total time=  23.7s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.491 total time=  43.9s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.463 total time=  42.9s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.473 total time=  43.3s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.490 total time=  43.5s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.495 total time=  43.4s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.485 total time=   7.1s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.496 total time=   6.9s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.489 total time=   7.0s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.489 total time=   7.0s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.505 total time=   6.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.461 total time=  49.6s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.490 total time=  49.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.467 total time=  49.7s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.474 total time=  49.5s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.471 total time=  49.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.466 total time=  12.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.460 total time=  12.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.495 total time=  12.4s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.473 total time=  12.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.504 total time=  12.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.487 total time= 1.1min\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.479 total time= 1.1min\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.474 total time= 1.1min\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.465 total time= 1.1min\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.495 total time= 1.1min\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.506 total time= 1.6min\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.528 total time= 1.6min\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.498 total time= 1.6min\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.500 total time= 1.7min\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.515 total time= 1.6min\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.475 total time=  13.2s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.488 total time=  13.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.447 total time=  13.1s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.491 total time=  13.0s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.488 total time=  12.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=9, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.467 total time=  13.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=9, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.494 total time=  13.1s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=9, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.488 total time=  13.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=9, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.482 total time=  13.0s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=9, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.481 total time=  13.2s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.505 total time= 2.8min\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.506 total time= 2.8min\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.486 total time= 2.8min\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.493 total time= 2.8min\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.527 total time= 2.8min\n",
      "Best parameters: {'subsample': 0.7, 'objective': 'reg:squarederror', 'n_estimators': 500, 'min_child_weight': 3, 'max_depth': 5, 'learning_rate': 0.1, 'colsample_bytree': 0.7} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[6418    2]\n",
      " [   2 5102]] \n",
      "\n",
      "Accuracy: 100.0 \n",
      "\n",
      "F1 Score: 100.0 \n",
      "\n",
      "Balanced accuracy: 100.0 \n",
      "\n",
      "AUC Score: 100.0 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[503 211]\n",
      " [294 259]] \n",
      "\n",
      "Accuracy: 60.1 \n",
      "\n",
      "F1 Score: 50.6 \n",
      "\n",
      "Balanced accuracy: 50.6 \n",
      "\n",
      "AUC Score: 58.6 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = XGBoostClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'learning_rate': [0.01, 0.1, 0.3, 0.5],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.5, 0.7],\n",
    "    'colsample_bytree': [0.5, 0.7],\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'objective': ['reg:squarederror']\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_gpt2, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_gpt2, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_xgb_gpt2_train, accuracy_xgb_gpt2_train, f1_xgb_gpt2_train, balaccuracy_xgb_gpt2_train, rocauc_xgb_gpt2_train = classifier.evaluate(X_train_embeddings_gpt2, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_xgb_gpt2_test, accuracy_xgb_gpt2_test, f1_xgb_gpt2_test, balaccuracy_xgb_gpt2_test, rocauc_xgb_gpt2_test = classifier.evaluate(X_test_embeddings_gpt2, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_xgb_gpt2_train_tosave = repr(cm_xgb_gpt2_train)\n",
    "accuracy_xgb_gpt2_train_tosave = repr(accuracy_xgb_gpt2_train)\n",
    "f1_xgb_gpt2_train_tosave = repr(f1_xgb_gpt2_train)\n",
    "balaccuracy_xgb_gpt2_train_tosave = repr(balaccuracy_xgb_gpt2_train)\n",
    "rocauc_xgb_gpt2_train_tosave = repr(rocauc_xgb_gpt2_train)\n",
    "\n",
    "cm_xgb_gpt2_test_tosave = repr(cm_xgb_gpt2_test)\n",
    "accuracy_xgb_gpt2_test_tosave = repr(accuracy_xgb_gpt2_test)\n",
    "f1_xgb_gpt2_test_tosave = repr(f1_xgb_gpt2_test)\n",
    "balaccuracy_xgb_gpt2_test_tosave = repr(balaccuracy_xgb_gpt2_test)\n",
    "rocauc_xgb_gpt2_test_tosave = repr(rocauc_xgb_gpt2_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/xgb_gpt2_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_xgb_gpt2_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_xgb_gpt2_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_xgb_gpt2_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_xgb_gpt2_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_xgb_gpt2_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_xgb_gpt2_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_xgb_gpt2_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_xgb_gpt2_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_xgb_gpt2_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_xgb_gpt2_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT2 + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:05:49.318675958Z",
     "start_time": "2023-05-22T01:05:40.285007496Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.391 total time=   1.5s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.406 total time=   1.5s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.389 total time=   1.5s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.381 total time=   1.5s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.385 total time=   1.5s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.449 total time=   2.2s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.474 total time=   2.2s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.454 total time=   2.2s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.452 total time=   2.2s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.463 total time=   2.2s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.405 total time=   0.8s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.395 total time=   0.8s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.402 total time=   0.8s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.396 total time=   0.8s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.392 total time=   0.9s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.395 total time=   0.3s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.398 total time=   0.3s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.390 total time=   0.3s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.402 total time=   0.3s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.400 total time=   0.3s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.405 total time=   0.4s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.398 total time=   0.4s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.358 total time=   0.4s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.381 total time=   0.5s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.411 total time=   0.5s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.387 total time=   0.8s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.377 total time=   0.9s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.396 total time=   0.8s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.380 total time=   0.9s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.387 total time=   0.8s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.446 total time=   2.2s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.481 total time=   2.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.471 total time=   2.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.439 total time=   2.2s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.437 total time=   2.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.410 total time=   1.6s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.379 total time=   1.5s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.378 total time=   1.5s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.403 total time=   1.6s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.414 total time=   1.6s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.383 total time=   0.4s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.398 total time=   0.4s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.372 total time=   0.4s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.401 total time=   0.4s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.394 total time=   0.4s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.414 total time=   0.5s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.404 total time=   0.4s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.410 total time=   0.4s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.374 total time=   0.4s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.391 total time=   0.4s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.434 total time=   0.6s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.423 total time=   0.6s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.416 total time=   0.6s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.395 total time=   0.6s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.433 total time=   0.6s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.343 total time=   0.8s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.396 total time=   0.8s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.383 total time=   0.8s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.384 total time=   0.8s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.381 total time=   0.8s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.477 total time=   1.2s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.466 total time=   1.2s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.454 total time=   1.2s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.435 total time=   1.2s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.462 total time=   1.2s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.475 total time=   2.2s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.464 total time=   2.2s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.443 total time=   2.2s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.476 total time=   2.2s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.475 total time=   2.2s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.464 total time=   1.2s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.451 total time=   1.2s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.428 total time=   1.2s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.433 total time=   1.2s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.439 total time=   1.2s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.360 total time=   0.8s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.412 total time=   0.8s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.354 total time=   0.8s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.374 total time=   0.8s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.373 total time=   0.8s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.384 total time=   0.3s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.437 total time=   0.3s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.406 total time=   0.3s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.391 total time=   0.3s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.399 total time=   0.3s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.436 total time=   2.2s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.470 total time=   2.2s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.454 total time=   2.2s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.439 total time=   2.3s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.474 total time=   2.2s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.420 total time=   0.4s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.373 total time=   0.4s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.406 total time=   0.4s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.374 total time=   0.4s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.389 total time=   0.5s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.473 total time=   1.2s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.455 total time=   1.2s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.468 total time=   1.2s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.448 total time=   1.2s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.453 total time=   1.2s\n",
      "Best parameters: {'n_estimators': 25, 'max_features': 10, 'max_depth': None, 'bootstrap': False} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[6420    0]\n",
      " [   4 5100]] \n",
      "\n",
      "Accuracy: 100.0 \n",
      "\n",
      "F1 Score: 100.0 \n",
      "\n",
      "Balanced accuracy: 100.0 \n",
      "\n",
      "AUC Score: 100.0 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[506 208]\n",
      " [314 239]] \n",
      "\n",
      "Accuracy: 58.8 \n",
      "\n",
      "F1 Score: 47.8 \n",
      "\n",
      "Balanced accuracy: 47.8 \n",
      "\n",
      "AUC Score: 57.0 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = RFClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'n_estimators': [10, 25],\n",
    "    'max_features': [5, 10],\n",
    "    'max_depth': [10, 50, None],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_gpt2, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_gpt2, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_rf_gpt2_train, accuracy_rf_gpt2_train, f1_rf_gpt2_train, balaccuracy_rf_gpt2_train, rocauc_rf_gpt2_train = classifier.evaluate(X_train_embeddings_gpt2, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_rf_gpt2_test, accuracy_rf_gpt2_test, f1_rf_gpt2_test, balaccuracy_rf_gpt2_test, rocauc_rf_gpt2_test = classifier.evaluate(X_test_embeddings_gpt2, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_rf_gpt2_train_tosave = repr(cm_rf_gpt2_train)\n",
    "accuracy_rf_gpt2_train_tosave = repr(accuracy_rf_gpt2_train)\n",
    "f1_rf_gpt2_train_tosave = repr(f1_rf_gpt2_train)\n",
    "balaccuracy_rf_gpt2_train_tosave = repr(balaccuracy_rf_gpt2_train)\n",
    "rocauc_rf_gpt2_train_tosave = repr(rocauc_rf_gpt2_train)\n",
    "\n",
    "cm_rf_gpt2_test_tosave = repr(cm_rf_gpt2_test)\n",
    "accuracy_rf_gpt2_test_tosave = repr(accuracy_rf_gpt2_test)\n",
    "f1_rf_gpt2_test_tosave = repr(f1_rf_gpt2_test)\n",
    "balaccuracy_rf_gpt2_test_tosave = repr(balaccuracy_rf_gpt2_test)\n",
    "rocauc_rf_gpt2_test_tosave = repr(rocauc_rf_gpt2_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/rf_gpt2_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_rf_gpt2_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_rf_gpt2_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_rf_gpt2_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_rf_gpt2_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_rf_gpt2_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_rf_gpt2_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_rf_gpt2_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_rf_gpt2_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_rf_gpt2_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_rf_gpt2_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT2 + SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:06:04.032916444Z",
     "start_time": "2023-05-22T01:05:49.324220928Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:305: UserWarning: The total space of parameters 16 is smaller than n_iter=20. Running 16 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV 1/5] END ...C=1, gamma=scale, kernel=linear;, score=0.526 total time=  24.7s\n",
      "[CV 2/5] END ...C=1, gamma=scale, kernel=linear;, score=0.518 total time=  24.9s\n",
      "[CV 3/5] END ...C=1, gamma=scale, kernel=linear;, score=0.506 total time=  24.7s\n",
      "[CV 4/5] END ...C=1, gamma=scale, kernel=linear;, score=0.498 total time=  24.9s\n",
      "[CV 5/5] END ...C=1, gamma=scale, kernel=linear;, score=0.541 total time=  25.0s\n",
      "[CV 1/5] END .....C=1, gamma=scale, kernel=poly;, score=0.514 total time=  25.6s\n",
      "[CV 2/5] END .....C=1, gamma=scale, kernel=poly;, score=0.502 total time=  25.6s\n",
      "[CV 3/5] END .....C=1, gamma=scale, kernel=poly;, score=0.507 total time=  25.6s\n",
      "[CV 4/5] END .....C=1, gamma=scale, kernel=poly;, score=0.494 total time=  25.5s\n",
      "[CV 5/5] END .....C=1, gamma=scale, kernel=poly;, score=0.512 total time=  25.6s\n",
      "[CV 1/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.510 total time=  29.8s\n",
      "[CV 2/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.502 total time=  29.9s\n",
      "[CV 3/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.498 total time=  29.7s\n",
      "[CV 4/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.494 total time=  29.8s\n",
      "[CV 5/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.527 total time=  31.8s\n",
      "[CV 1/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.493 total time=  32.1s\n",
      "[CV 2/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.467 total time=  31.1s\n",
      "[CV 3/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.469 total time=  31.8s\n",
      "[CV 4/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.478 total time=  29.9s\n",
      "[CV 5/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.492 total time=  30.3s\n",
      "[CV 1/5] END ..C=10, gamma=scale, kernel=linear;, score=0.526 total time=  29.8s\n",
      "[CV 2/5] END ..C=10, gamma=scale, kernel=linear;, score=0.515 total time=  30.1s\n",
      "[CV 3/5] END ..C=10, gamma=scale, kernel=linear;, score=0.509 total time=  29.6s\n",
      "[CV 4/5] END ..C=10, gamma=scale, kernel=linear;, score=0.512 total time=  29.4s\n",
      "[CV 5/5] END ..C=10, gamma=scale, kernel=linear;, score=0.546 total time=  30.2s\n",
      "[CV 1/5] END ....C=10, gamma=scale, kernel=poly;, score=0.531 total time=  33.9s\n",
      "[CV 2/5] END ....C=10, gamma=scale, kernel=poly;, score=0.522 total time=  35.6s\n",
      "[CV 3/5] END ....C=10, gamma=scale, kernel=poly;, score=0.517 total time=  33.9s\n",
      "[CV 4/5] END ....C=10, gamma=scale, kernel=poly;, score=0.524 total time=  33.6s\n",
      "[CV 5/5] END ....C=10, gamma=scale, kernel=poly;, score=0.545 total time=  34.3s\n",
      "[CV 1/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.539 total time=  37.0s\n",
      "[CV 2/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.534 total time=  37.4s\n",
      "[CV 3/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.531 total time=  36.8s\n",
      "[CV 4/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.522 total time=  37.0s\n",
      "[CV 5/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.555 total time=  37.7s\n",
      "[CV 1/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.487 total time=  18.7s\n",
      "[CV 2/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.500 total time=  18.8s\n",
      "[CV 3/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.490 total time=  18.7s\n",
      "[CV 4/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.478 total time=  18.6s\n",
      "[CV 5/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.498 total time=  18.9s\n",
      "[CV 1/5] END .C=100, gamma=scale, kernel=linear;, score=0.522 total time= 1.2min\n",
      "[CV 2/5] END .C=100, gamma=scale, kernel=linear;, score=0.509 total time= 1.3min\n",
      "[CV 3/5] END .C=100, gamma=scale, kernel=linear;, score=0.503 total time= 1.2min\n",
      "[CV 4/5] END .C=100, gamma=scale, kernel=linear;, score=0.514 total time= 1.2min\n",
      "[CV 5/5] END .C=100, gamma=scale, kernel=linear;, score=0.546 total time= 1.3min\n",
      "[CV 1/5] END ...C=100, gamma=scale, kernel=poly;, score=0.523 total time= 2.0min\n",
      "[CV 2/5] END ...C=100, gamma=scale, kernel=poly;, score=0.534 total time= 2.1min\n",
      "[CV 3/5] END ...C=100, gamma=scale, kernel=poly;, score=0.535 total time= 2.0min\n",
      "[CV 4/5] END ...C=100, gamma=scale, kernel=poly;, score=0.510 total time= 2.0min\n",
      "[CV 5/5] END ...C=100, gamma=scale, kernel=poly;, score=0.550 total time= 2.0min\n",
      "[CV 1/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.523 total time= 2.3min\n",
      "[CV 2/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.531 total time= 2.4min\n",
      "[CV 3/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.534 total time= 2.3min\n",
      "[CV 4/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.515 total time= 2.3min\n",
      "[CV 5/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.554 total time= 2.4min\n",
      "[CV 1/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.495 total time=  14.9s\n",
      "[CV 2/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.505 total time=  14.8s\n",
      "[CV 3/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.477 total time=  14.6s\n",
      "[CV 4/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.465 total time=  14.6s\n",
      "[CV 5/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.507 total time=  14.9s\n",
      "[CV 1/5] END C=1000, gamma=scale, kernel=linear;, score=0.529 total time= 8.7min\n",
      "[CV 2/5] END C=1000, gamma=scale, kernel=linear;, score=0.515 total time= 8.3min\n",
      "[CV 3/5] END C=1000, gamma=scale, kernel=linear;, score=0.502 total time= 8.7min\n",
      "[CV 4/5] END C=1000, gamma=scale, kernel=linear;, score=0.513 total time= 8.9min\n",
      "[CV 5/5] END C=1000, gamma=scale, kernel=linear;, score=0.541 total time= 8.9min\n",
      "[CV 1/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.519 total time= 2.1min\n",
      "[CV 2/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.534 total time= 2.2min\n",
      "[CV 3/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.539 total time= 2.1min\n",
      "[CV 4/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.508 total time= 2.1min\n",
      "[CV 5/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.542 total time= 2.0min\n",
      "[CV 1/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.518 total time= 2.6min\n",
      "[CV 2/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.536 total time= 2.7min\n",
      "[CV 3/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.536 total time= 2.6min\n",
      "[CV 4/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.517 total time= 2.7min\n",
      "[CV 5/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.547 total time= 2.8min\n",
      "[CV 1/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.496 total time=  14.2s\n",
      "[CV 2/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.503 total time=  14.5s\n",
      "[CV 3/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.476 total time=  14.4s\n",
      "[CV 4/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.467 total time=  14.3s\n",
      "[CV 5/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.506 total time=  14.5s\n",
      "Best parameters: {'kernel': 'rbf', 'gamma': 'scale', 'C': 10} \n",
      "\n",
      "[LibSVM]\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[5967  453]\n",
      " [1238 3866]] \n",
      "\n",
      "Accuracy: 85.3 \n",
      "\n",
      "F1 Score: 82.1 \n",
      "\n",
      "Balanced accuracy: 82.1 \n",
      "\n",
      "AUC Score: 84.3 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[516 198]\n",
      " [288 265]] \n",
      "\n",
      "Accuracy: 61.6 \n",
      "\n",
      "F1 Score: 52.2 \n",
      "\n",
      "Balanced accuracy: 52.2 \n",
      "\n",
      "AUC Score: 60.1 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = SVClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'C': [1, 10, 100, 1000],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale']\n",
    "    #'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_gpt2, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_gpt2, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_svc_gpt2_train, accuracy_svc_gpt2_train, f1_svc_gpt2_train, balaccuracy_svc_gpt2_train, rocauc_svc_gpt2_train = classifier.evaluate(X_train_embeddings_gpt2, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_svc_gpt2_test, accuracy_svc_gpt2_test, f1_svc_gpt2_test, balaccuracy_svc_gpt2_test, rocauc_svc_gpt2_test = classifier.evaluate(X_test_embeddings_gpt2, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_svc_gpt2_train_tosave = repr(cm_svc_gpt2_train)\n",
    "accuracy_svc_gpt2_train_tosave = repr(accuracy_svc_gpt2_train)\n",
    "f1_svc_gpt2_train_tosave = repr(f1_svc_gpt2_train)\n",
    "balaccuracy_svc_gpt2_train_tosave = repr(balaccuracy_svc_gpt2_train)\n",
    "rocauc_svc_gpt2_train_tosave = repr(rocauc_svc_gpt2_train)\n",
    "\n",
    "cm_svc_gpt2_test_tosave = repr(cm_svc_gpt2_test)\n",
    "accuracy_svc_gpt2_test_tosave = repr(accuracy_svc_gpt2_test)\n",
    "f1_svc_gpt2_test_tosave = repr(f1_svc_gpt2_test)\n",
    "balaccuracy_svc_gpt2_test_tosave = repr(balaccuracy_svc_gpt2_test)\n",
    "rocauc_svc_gpt2_test_tosave = repr(rocauc_svc_gpt2_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/svc_gpt2_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_svc_gpt2_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_svc_gpt2_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_svc_gpt2_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_svc_gpt2_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_svc_gpt2_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_svc_gpt2_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_svc_gpt2_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_svc_gpt2_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_svc_gpt2_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_svc_gpt2_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT2 + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:06:07.081711772Z",
     "start_time": "2023-05-22T01:06:04.033203542Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END ...C=0.13, penalty=l1, solver=saga;, score=0.295 total time=   1.9s\n",
      "[CV 2/5] END ...C=0.13, penalty=l1, solver=saga;, score=0.276 total time=   2.8s\n",
      "[CV 3/5] END ...C=0.13, penalty=l1, solver=saga;, score=0.308 total time=   2.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END ...C=0.13, penalty=l1, solver=saga;, score=0.292 total time=   5.5s\n",
      "[CV 5/5] END ...C=0.13, penalty=l1, solver=saga;, score=0.254 total time=   2.7s\n",
      "[CV 1/5] END C=0.63, penalty=l2, solver=newton-cg;, score=0.519 total time=   0.5s\n",
      "[CV 2/5] END C=0.63, penalty=l2, solver=newton-cg;, score=0.523 total time=   0.5s\n",
      "[CV 3/5] END C=0.63, penalty=l2, solver=newton-cg;, score=0.502 total time=   0.5s\n",
      "[CV 4/5] END C=0.63, penalty=l2, solver=newton-cg;, score=0.501 total time=   0.5s\n",
      "[CV 5/5] END C=0.63, penalty=l2, solver=newton-cg;, score=0.536 total time=   0.5s\n",
      "[CV 1/5] END .......C=0.2, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .......C=0.2, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .......C=0.2, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .......C=0.2, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .......C=0.2, penalty=l1, solver=sag;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ..C=0.91, penalty=l2, solver=lbfgs;, score=0.527 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END ..C=0.91, penalty=l2, solver=lbfgs;, score=0.529 total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END ..C=0.91, penalty=l2, solver=lbfgs;, score=0.513 total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END ..C=0.91, penalty=l2, solver=lbfgs;, score=0.509 total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END ..C=0.91, penalty=l2, solver=lbfgs;, score=0.538 total time=   0.3s\n",
      "[CV 1/5] END ....C=0.54, penalty=l2, solver=sag;, score=0.519 total time=   1.0s\n",
      "[CV 2/5] END ....C=0.54, penalty=l2, solver=sag;, score=0.520 total time=   1.2s\n",
      "[CV 3/5] END ....C=0.54, penalty=l2, solver=sag;, score=0.501 total time=   1.4s\n",
      "[CV 4/5] END ....C=0.54, penalty=l2, solver=sag;, score=0.496 total time=   1.4s\n",
      "[CV 5/5] END ....C=0.54, penalty=l2, solver=sag;, score=0.534 total time=   1.3s\n",
      "[CV 1/5] END ...C=0.78, penalty=l2, solver=saga;, score=0.524 total time=   3.4s\n",
      "[CV 2/5] END ...C=0.78, penalty=l2, solver=saga;, score=0.530 total time=   3.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END ...C=0.78, penalty=l2, solver=saga;, score=0.509 total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END ...C=0.78, penalty=l2, solver=saga;, score=0.506 total time=   4.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END ...C=0.78, penalty=l2, solver=saga;, score=0.535 total time=   4.5s\n",
      "[CV 1/5] END C=0.09, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.09, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.09, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.09, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.09, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ......C=0.36, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ......C=0.36, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ......C=0.36, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ......C=0.36, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ......C=0.36, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.76, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.76, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.76, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.76, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.76, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.06, penalty=l1, solver=liblinear;, score=0.015 total time=   0.2s\n",
      "[CV 2/5] END C=0.06, penalty=l1, solver=liblinear;, score=0.002 total time=   0.2s\n",
      "[CV 3/5] END C=0.06, penalty=l1, solver=liblinear;, score=0.017 total time=   0.3s\n",
      "[CV 4/5] END C=0.06, penalty=l1, solver=liblinear;, score=0.004 total time=   0.2s\n",
      "[CV 5/5] END C=0.06, penalty=l1, solver=liblinear;, score=0.002 total time=   0.2s\n",
      "[CV 1/5] END C=0.97, penalty=l2, solver=newton-cg;, score=0.531 total time=   0.6s\n",
      "[CV 2/5] END C=0.97, penalty=l2, solver=newton-cg;, score=0.528 total time=   0.6s\n",
      "[CV 3/5] END C=0.97, penalty=l2, solver=newton-cg;, score=0.513 total time=   0.6s\n",
      "[CV 4/5] END C=0.97, penalty=l2, solver=newton-cg;, score=0.507 total time=   0.6s\n",
      "[CV 5/5] END C=0.97, penalty=l2, solver=newton-cg;, score=0.538 total time=   0.6s\n",
      "[CV 1/5] END C=0.05, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.05, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.05, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.05, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.05, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ..C=0.22, penalty=l2, solver=lbfgs;, score=0.509 total time=   0.2s\n",
      "[CV 2/5] END ..C=0.22, penalty=l2, solver=lbfgs;, score=0.494 total time=   0.2s\n",
      "[CV 3/5] END ..C=0.22, penalty=l2, solver=lbfgs;, score=0.483 total time=   0.2s\n",
      "[CV 4/5] END ..C=0.22, penalty=l2, solver=lbfgs;, score=0.481 total time=   0.2s\n",
      "[CV 5/5] END ..C=0.22, penalty=l2, solver=lbfgs;, score=0.510 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ..C=0.75, penalty=l2, solver=lbfgs;, score=0.525 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END ..C=0.75, penalty=l2, solver=lbfgs;, score=0.526 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END ..C=0.75, penalty=l2, solver=lbfgs;, score=0.508 total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END ..C=0.75, penalty=l2, solver=lbfgs;, score=0.506 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END ..C=0.75, penalty=l2, solver=lbfgs;, score=0.535 total time=   0.2s\n",
      "[CV 1/5] END C=0.04, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.04, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.04, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.04, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.04, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.4, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.4, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.4, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.4, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.4, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END .C=0.76, penalty=none, solver=saga;, score=0.539 total time=   4.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END .C=0.76, penalty=none, solver=saga;, score=0.525 total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END .C=0.76, penalty=none, solver=saga;, score=0.515 total time=   4.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END .C=0.76, penalty=none, solver=saga;, score=0.510 total time=   4.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END .C=0.76, penalty=none, solver=saga;, score=0.552 total time=   4.3s\n",
      "[CV 1/5] END C=0.02, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.02, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.02, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.02, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.02, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.99, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.99, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.99, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.99, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.99, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=0.32, penalty=none, solver=lbfgs;, score=0.534 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=0.32, penalty=none, solver=lbfgs;, score=0.528 total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=0.32, penalty=none, solver=lbfgs;, score=0.505 total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=0.32, penalty=none, solver=lbfgs;, score=0.504 total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "45 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1048, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 864, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 782, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.28484371 0.51629819        nan 0.52316173 0.51407041 0.52085124\n",
      "        nan        nan        nan 0.00812843 0.52335893        nan\n",
      " 0.49524411 0.51975175        nan        nan 0.52820459        nan\n",
      "        nan 0.52452979]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=0.32, penalty=none, solver=lbfgs;, score=0.550 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'solver': 'saga', 'penalty': 'none', 'C': 0.76} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[4922 1498]\n",
      " [2263 2841]] \n",
      "\n",
      "Accuracy: 67.4 \n",
      "\n",
      "F1 Score: 60.2 \n",
      "\n",
      "Balanced accuracy: 60.2 \n",
      "\n",
      "AUC Score: 66.2 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[507 207]\n",
      " [275 278]] \n",
      "\n",
      "Accuracy: 62.0 \n",
      "\n",
      "F1 Score: 53.6 \n",
      "\n",
      "Balanced accuracy: 53.6 \n",
      "\n",
      "AUC Score: 60.6 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = LRClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'C' : np.arange(0, 1, 0.01)\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_gpt2, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_gpt2, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_lr_gpt2_train, accuracy_lr_gpt2_train, f1_lr_gpt2_train, balaccuracy_lr_gpt2_train, rocauc_lr_gpt2_train = classifier.evaluate(X_train_embeddings_gpt2, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_lr_gpt2_test, accuracy_lr_gpt2_test, f1_lr_gpt2_test, balaccuracy_lr_gpt2_test, rocauc_lr_gpt2_test = classifier.evaluate(X_test_embeddings_gpt2, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_lr_gpt2_train_tosave = repr(cm_lr_gpt2_train)\n",
    "accuracy_lr_gpt2_train_tosave = repr(accuracy_lr_gpt2_train)\n",
    "f1_lr_gpt2_train_tosave = repr(f1_lr_gpt2_train)\n",
    "balaccuracy_lr_gpt2_train_tosave = repr(balaccuracy_lr_gpt2_train)\n",
    "rocauc_lr_gpt2_train_tosave = repr(rocauc_lr_gpt2_train)\n",
    "\n",
    "cm_lr_gpt2_test_tosave = repr(cm_lr_gpt2_test)\n",
    "accuracy_lr_gpt2_test_tosave = repr(accuracy_lr_gpt2_test)\n",
    "f1_lr_gpt2_test_tosave = repr(f1_lr_gpt2_test)\n",
    "balaccuracy_lr_gpt2_test_tosave = repr(balaccuracy_lr_gpt2_test)\n",
    "rocauc_lr_gpt2_test_tosave = repr(rocauc_lr_gpt2_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/lr_gpt2_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_lr_gpt2_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_lr_gpt2_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_lr_gpt2_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_lr_gpt2_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_lr_gpt2_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_lr_gpt2_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_lr_gpt2_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_lr_gpt2_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_lr_gpt2_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_lr_gpt2_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT2 + Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:06:42.468684512Z",
     "start_time": "2023-05-22T01:06:07.015363770Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Train Loss: 0.6657 | Val Loss: 0.6535 | F1 Score: 0.5510 | Balanced Accuracy: 0.6060 | AUC: 0.6060\n",
      "Epoch 2/15 | Train Loss: 0.6475 | Val Loss: 0.6540 | F1 Score: 0.5476 | Balanced Accuracy: 0.6052 | AUC: 0.6052\n",
      "Epoch 3/15 | Train Loss: 0.6362 | Val Loss: 0.6575 | F1 Score: 0.5443 | Balanced Accuracy: 0.6015 | AUC: 0.6015\n",
      "Epoch 4/15 | Train Loss: 0.6253 | Val Loss: 0.6622 | F1 Score: 0.5411 | Balanced Accuracy: 0.5961 | AUC: 0.5961\n",
      "Epoch 5/15 | Train Loss: 0.6099 | Val Loss: 0.6699 | F1 Score: 0.5448 | Balanced Accuracy: 0.6022 | AUC: 0.6022\n",
      "Epoch 6/15 | Train Loss: 0.5906 | Val Loss: 0.6821 | F1 Score: 0.5456 | Balanced Accuracy: 0.5966 | AUC: 0.5966\n",
      "Early stopping triggered. No improvement in 5 epochs.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[4683 1737]\n",
      " [1796 3308]] \n",
      "\n",
      "Accuracy: 69.3 \n",
      "\n",
      "F1 Score: 65.2 \n",
      "\n",
      "Balanced accuracy: 68.9 \n",
      "\n",
      "AUC Score: 68.9 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[462 252]\n",
      " [251 302]] \n",
      "\n",
      "Accuracy: 60.3 \n",
      "\n",
      "F1 Score: 54.6 \n",
      "\n",
      "Balanced accuracy: 59.7 \n",
      "\n",
      "AUC Score: 59.7 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# SET PARAMETERS\n",
    "dropout_rate = 0.1\n",
    "epochs_number = 15\n",
    "learning_rate = 0.0001\n",
    "early_stopping = 5\n",
    "# --------------------\n",
    "\n",
    "input_dim = X_train_embeddings_gpt2.shape[1]  # Dimensionality of word embeddings\n",
    "hidden_dim = X_train_embeddings_gpt2.shape[1]*2  # Number of units in the hidden layer\n",
    "\n",
    "classifier = NeuralNetworkClassifier(input_dim, hidden_dim, dropout=dropout_rate)\n",
    "\n",
    "# Fit the model\n",
    "classifier.fit(X_train_embeddings_gpt2, y_train, X_test_embeddings_gpt2, y_test, num_epochs=epochs_number, lr=learning_rate, patience=early_stopping)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_nn_gpt2_train, accuracy_nn_gpt2_train, f1_nn_gpt2_train, balaccuracy_nn_gpt2_train, rocauc_nn_gpt2_train = classifier.evaluate(X_train_embeddings_gpt2, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_nn_gpt2_test, accuracy_nn_gpt2_test, f1_nn_gpt2_test, balaccuracy_nn_gpt2_test, rocauc_nn_gpt2_test = classifier.evaluate(X_test_embeddings_gpt2, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_nn_gpt2_train_tosave = repr(cm_nn_gpt2_train)\n",
    "accuracy_nn_gpt2_train_tosave = repr(accuracy_nn_gpt2_train)\n",
    "f1_nn_gpt2_train_tosave = repr(f1_nn_gpt2_train)\n",
    "balaccuracy_nn_gpt2_train_tosave = repr(balaccuracy_nn_gpt2_train)\n",
    "rocauc_nn_gpt2_train_tosave = repr(rocauc_nn_gpt2_train)\n",
    "\n",
    "cm_nn_gpt2_test_tosave = repr(cm_nn_gpt2_test)\n",
    "accuracy_nn_gpt2_test_tosave = repr(accuracy_nn_gpt2_test)\n",
    "f1_nn_gpt2_test_tosave = repr(f1_nn_gpt2_test)\n",
    "balaccuracy_nn_gpt2_test_tosave = repr(balaccuracy_nn_gpt2_test)\n",
    "rocauc_nn_gpt2_test_tosave = repr(rocauc_nn_gpt2_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/nn_gpt2_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_nn_gpt2_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_nn_gpt2_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_nn_gpt2_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_nn_gpt2_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_nn_gpt2_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_nn_gpt2_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_nn_gpt2_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_nn_gpt2_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_nn_gpt2_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_nn_gpt2_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa + KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:06:44.953606536Z",
     "start_time": "2023-05-22T01:06:42.414864620Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=3, weights=distance;, score=0.517 total time=   7.6s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=3, weights=distance;, score=0.486 total time=   7.6s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=3, weights=distance;, score=0.538 total time=   7.7s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=3, weights=distance;, score=0.493 total time=   7.5s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=3, weights=distance;, score=0.501 total time=   7.5s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.523 total time=   7.5s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.484 total time=   7.6s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.522 total time=   7.6s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.497 total time=   7.5s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.507 total time=   7.7s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.509 total time=   0.3s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.466 total time=   0.3s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.500 total time=   0.3s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.485 total time=   0.3s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.503 total time=   0.4s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=2, weights=uniform;, score=0.360 total time=   0.4s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=2, weights=uniform;, score=0.348 total time=   0.4s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=2, weights=uniform;, score=0.378 total time=   0.3s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=2, weights=uniform;, score=0.377 total time=   0.3s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=2, weights=uniform;, score=0.351 total time=   0.4s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.506 total time=   0.3s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.489 total time=   0.3s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.522 total time=   0.3s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.502 total time=   0.3s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.512 total time=   0.3s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=7, weights=distance;, score=0.510 total time=   7.9s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=7, weights=distance;, score=0.487 total time=   7.5s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=7, weights=distance;, score=0.508 total time=   7.6s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=7, weights=distance;, score=0.496 total time=   7.6s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=7, weights=distance;, score=0.491 total time=   7.5s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.508 total time=   0.3s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.476 total time=   0.3s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.510 total time=   0.3s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.498 total time=   0.3s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.523 total time=   0.3s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=9, weights=distance;, score=0.509 total time=   0.3s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=9, weights=distance;, score=0.470 total time=   0.3s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=9, weights=distance;, score=0.501 total time=   0.3s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=9, weights=distance;, score=0.486 total time=   0.3s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=9, weights=distance;, score=0.503 total time=   0.3s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=8, weights=distance;, score=0.506 total time=   7.5s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=8, weights=distance;, score=0.491 total time=   7.5s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=8, weights=distance;, score=0.516 total time=   7.5s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=8, weights=distance;, score=0.483 total time=   7.5s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=8, weights=distance;, score=0.496 total time=   7.5s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.521 total time=   0.3s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.478 total time=   0.3s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.536 total time=   0.2s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.507 total time=   0.3s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.510 total time=   0.3s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=6, weights=uniform;, score=0.437 total time=   7.5s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=6, weights=uniform;, score=0.411 total time=   7.5s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=6, weights=uniform;, score=0.429 total time=   7.5s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=6, weights=uniform;, score=0.406 total time=   7.5s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=6, weights=uniform;, score=0.437 total time=   7.5s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=8, weights=distance;, score=0.508 total time=   0.3s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=8, weights=distance;, score=0.476 total time=   0.3s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=8, weights=distance;, score=0.510 total time=   0.3s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=8, weights=distance;, score=0.498 total time=   0.3s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=8, weights=distance;, score=0.523 total time=   0.2s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=9, weights=uniform;, score=0.502 total time=   7.6s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=9, weights=uniform;, score=0.480 total time=   7.6s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=9, weights=uniform;, score=0.503 total time=   7.6s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=9, weights=uniform;, score=0.476 total time=   7.6s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=9, weights=uniform;, score=0.495 total time=   7.5s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=2, weights=uniform;, score=0.384 total time=   7.6s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=2, weights=uniform;, score=0.371 total time=   7.5s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=2, weights=uniform;, score=0.396 total time=   7.6s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=2, weights=uniform;, score=0.364 total time=   7.6s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=2, weights=uniform;, score=0.358 total time=   7.5s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.423 total time=   0.3s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.368 total time=   0.3s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.420 total time=   0.3s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.407 total time=   0.3s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.413 total time=   0.3s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.513 total time=   0.3s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.472 total time=   0.3s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.534 total time=   0.3s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.516 total time=   0.3s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.510 total time=   0.3s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=9, weights=uniform;, score=0.509 total time=   0.3s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=9, weights=uniform;, score=0.466 total time=   0.3s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=9, weights=uniform;, score=0.500 total time=   0.3s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=9, weights=uniform;, score=0.485 total time=   0.3s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=9, weights=uniform;, score=0.503 total time=   0.3s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=9, weights=distance;, score=0.509 total time=   0.3s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=9, weights=distance;, score=0.470 total time=   0.3s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=9, weights=distance;, score=0.501 total time=   0.2s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=9, weights=distance;, score=0.486 total time=   0.3s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=9, weights=distance;, score=0.503 total time=   0.3s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=8, weights=uniform;, score=0.441 total time=   0.3s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=8, weights=uniform;, score=0.410 total time=   0.3s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=8, weights=uniform;, score=0.429 total time=   0.3s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=8, weights=uniform;, score=0.415 total time=   0.3s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=8, weights=uniform;, score=0.420 total time=   0.3s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.514 total time=   7.5s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.482 total time=   7.5s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.543 total time=   7.5s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.501 total time=   7.5s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.515 total time=   7.5s\n",
      "Best parameters: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[6420    0]\n",
      " [   4 5100]] \n",
      "\n",
      "Accuracy: 100.0 \n",
      "\n",
      "F1 Score: 100.0 \n",
      "\n",
      "Balanced accuracy: 100.0 \n",
      "\n",
      "AUC Score: 100.0 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[457 257]\n",
      " [289 264]] \n",
      "\n",
      "Accuracy: 56.9 \n",
      "\n",
      "F1 Score: 49.2 \n",
      "\n",
      "Balanced accuracy: 49.2 \n",
      "\n",
      "AUC Score: 55.9 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = KNNClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'weights' : ['uniform','distance'],\n",
    "    'metric' : ['minkowski','euclidean','manhattan']\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_roberta, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_roberta, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_knn_roberta_train, accuracy_knn_roberta_train, f1_knn_roberta_train, balaccuracy_knn_roberta_train, rocauc_knn_roberta_train = classifier.evaluate(X_train_embeddings_roberta, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_knn_roberta_test, accuracy_knn_roberta_test, f1_knn_roberta_test, balaccuracy_knn_roberta_test, rocauc_knn_roberta_test = classifier.evaluate(X_test_embeddings_roberta, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_knn_roberta_train_tosave = repr(cm_knn_roberta_train)\n",
    "accuracy_knn_roberta_train_tosave = repr(accuracy_knn_roberta_train)\n",
    "f1_knn_roberta_train_tosave = repr(f1_knn_roberta_train)\n",
    "balaccuracy_knn_roberta_train_tosave = repr(balaccuracy_knn_roberta_train)\n",
    "rocauc_knn_roberta_train_tosave = repr(rocauc_knn_roberta_train)\n",
    "\n",
    "cm_knn_roberta_test_tosave = repr(cm_knn_roberta_test)\n",
    "accuracy_knn_roberta_test_tosave = repr(accuracy_knn_roberta_test)\n",
    "f1_knn_roberta_test_tosave = repr(f1_knn_roberta_test)\n",
    "balaccuracy_knn_roberta_test_tosave = repr(balaccuracy_knn_roberta_test)\n",
    "rocauc_knn_roberta_test_tosave = repr(rocauc_knn_roberta_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/knn_roberta_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_knn_roberta_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_knn_roberta_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_knn_roberta_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_knn_roberta_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_knn_roberta_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_knn_roberta_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_knn_roberta_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_knn_roberta_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_knn_roberta_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_knn_roberta_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:08:18.508224668Z",
     "start_time": "2023-05-22T01:06:44.935785144Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.499 total time= 1.1min\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.518 total time= 1.1min\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.496 total time= 1.1min\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.514 total time= 1.1min\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.524 total time= 1.1min\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.495 total time=  17.4s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.497 total time=  17.1s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.497 total time=  17.2s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.473 total time=  17.2s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.510 total time=  17.0s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.492 total time=   5.9s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.507 total time=   6.1s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.511 total time=   5.9s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.487 total time=   5.9s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.511 total time=   5.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=9, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.488 total time=  25.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=9, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.494 total time=  25.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=9, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.491 total time=  25.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=9, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.491 total time=  25.4s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=9, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.516 total time=  25.3s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.520 total time=  54.9s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.504 total time=  54.9s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.496 total time=  55.1s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.513 total time=  55.4s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.516 total time=  55.4s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.514 total time=  51.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.483 total time=  50.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.487 total time=  50.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.472 total time=  50.4s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.505 total time=  50.1s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.465 total time=  34.0s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.468 total time=  33.9s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.458 total time=  33.7s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.470 total time=  34.0s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.494 total time=  34.0s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.477 total time=  18.0s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.471 total time=  17.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.475 total time=  17.5s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.451 total time=  17.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.486 total time=  17.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.505 total time=  27.1s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.508 total time=  27.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.492 total time=  27.4s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.491 total time=  27.3s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.520 total time=  27.0s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.503 total time=  13.6s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.480 total time=  13.7s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.493 total time=  13.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.499 total time=  13.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.490 total time=  13.5s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.381 total time=  11.4s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.400 total time=  11.3s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.385 total time=  11.4s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.397 total time=  11.8s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.390 total time=  11.4s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.496 total time= 1.1min\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.510 total time= 1.1min\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.509 total time= 1.1min\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.502 total time= 1.1min\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.525 total time= 1.2min\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.489 total time=  13.7s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.485 total time=  13.7s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.492 total time=  13.7s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.479 total time=  13.8s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.511 total time=  15.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.486 total time= 1.1min\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.506 total time= 1.1min\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.498 total time= 1.1min\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.480 total time= 1.1min\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.517 total time= 1.1min\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.488 total time= 1.9min\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.489 total time= 1.9min\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.489 total time= 1.9min\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.468 total time= 1.9min\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.481 total time= 1.9min\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.494 total time=  11.1s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.497 total time=  11.1s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.498 total time=  11.2s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.491 total time=  11.3s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.522 total time=  11.1s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.496 total time=  53.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.488 total time=  53.7s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.488 total time=  53.3s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.473 total time=  53.8s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.481 total time=  54.5s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.467 total time=  23.2s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.468 total time=  22.0s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.475 total time=  22.3s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.486 total time=  21.9s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.491 total time=  21.9s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.528 total time= 1.6min\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.528 total time= 1.6min\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.494 total time= 1.6min\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.482 total time= 1.6min\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.542 total time= 1.6min\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.514 total time=  34.3s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.502 total time=  36.4s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.514 total time=  33.8s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.484 total time=  33.8s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.525 total time=  33.8s\n",
      "Best parameters: {'subsample': 0.5, 'objective': 'reg:squarederror', 'n_estimators': 500, 'min_child_weight': 5, 'max_depth': 5, 'learning_rate': 0.1, 'colsample_bytree': 0.7} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[6417    3]\n",
      " [   3 5101]] \n",
      "\n",
      "Accuracy: 99.9 \n",
      "\n",
      "F1 Score: 99.9 \n",
      "\n",
      "Balanced accuracy: 99.9 \n",
      "\n",
      "AUC Score: 99.9 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[478 236]\n",
      " [292 261]] \n",
      "\n",
      "Accuracy: 58.3 \n",
      "\n",
      "F1 Score: 49.7 \n",
      "\n",
      "Balanced accuracy: 49.7 \n",
      "\n",
      "AUC Score: 57.1 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = XGBoostClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'learning_rate': [0.01, 0.1, 0.3, 0.5],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.5, 0.7],\n",
    "    'colsample_bytree': [0.5, 0.7],\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'objective': ['reg:squarederror']\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_roberta, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_roberta, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_xgb_roberta_train, accuracy_xgb_roberta_train, f1_xgb_roberta_train, balaccuracy_xgb_roberta_train, rocauc_xgb_roberta_train = classifier.evaluate(X_train_embeddings_roberta, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_xgb_roberta_test, accuracy_xgb_roberta_test, f1_xgb_roberta_test, balaccuracy_xgb_roberta_test, rocauc_xgb_roberta_test = classifier.evaluate(X_test_embeddings_roberta, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_xgb_roberta_train_tosave = repr(cm_xgb_roberta_train)\n",
    "accuracy_xgb_roberta_train_tosave = repr(accuracy_xgb_roberta_train)\n",
    "f1_xgb_roberta_train_tosave = repr(f1_xgb_roberta_train)\n",
    "balaccuracy_xgb_roberta_train_tosave = repr(balaccuracy_xgb_roberta_train)\n",
    "rocauc_xgb_roberta_train_tosave = repr(rocauc_xgb_roberta_train)\n",
    "\n",
    "cm_xgb_roberta_test_tosave = repr(cm_xgb_roberta_test)\n",
    "accuracy_xgb_roberta_test_tosave = repr(accuracy_xgb_roberta_test)\n",
    "f1_xgb_roberta_test_tosave = repr(f1_xgb_roberta_test)\n",
    "balaccuracy_xgb_roberta_test_tosave = repr(balaccuracy_xgb_roberta_test)\n",
    "rocauc_xgb_roberta_test_tosave = repr(rocauc_xgb_roberta_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/xgb_roberta_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_xgb_roberta_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_xgb_roberta_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_xgb_roberta_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_xgb_roberta_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_xgb_roberta_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_xgb_roberta_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_xgb_roberta_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_xgb_roberta_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_xgb_roberta_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_xgb_roberta_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:08:26.902812089Z",
     "start_time": "2023-05-22T01:08:18.482891640Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.407 total time=   0.3s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.400 total time=   0.3s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.404 total time=   0.3s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.404 total time=   0.3s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.414 total time=   0.3s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.454 total time=   1.2s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.469 total time=   1.2s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.449 total time=   1.2s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.417 total time=   1.2s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.453 total time=   1.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.454 total time=   2.2s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.439 total time=   2.3s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.457 total time=   2.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.438 total time=   2.2s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.448 total time=   2.2s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.428 total time=   1.5s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.439 total time=   1.5s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.402 total time=   1.5s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.412 total time=   1.5s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.436 total time=   1.5s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.447 total time=   1.2s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.436 total time=   1.2s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.440 total time=   1.2s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.437 total time=   1.2s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.456 total time=   1.2s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.387 total time=   0.8s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.371 total time=   0.8s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.387 total time=   0.8s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.370 total time=   0.8s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.381 total time=   0.8s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.405 total time=   0.8s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.408 total time=   0.8s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.400 total time=   0.8s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.371 total time=   0.8s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.411 total time=   0.8s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.462 total time=   1.2s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.452 total time=   1.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.475 total time=   1.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.427 total time=   1.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.443 total time=   1.2s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.383 total time=   0.4s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.410 total time=   0.4s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.403 total time=   0.4s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.389 total time=   0.4s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.382 total time=   0.4s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.384 total time=   0.5s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.410 total time=   0.5s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.393 total time=   0.4s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.396 total time=   0.4s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.371 total time=   0.4s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.459 total time=   0.6s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.416 total time=   0.6s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.422 total time=   0.6s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.419 total time=   0.6s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.426 total time=   0.6s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.433 total time=   0.6s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.426 total time=   0.6s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.404 total time=   0.6s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.420 total time=   0.6s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.431 total time=   0.6s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.405 total time=   0.8s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.398 total time=   0.8s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.386 total time=   0.9s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.373 total time=   0.8s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.430 total time=   0.8s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.397 total time=   0.8s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.399 total time=   0.9s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.427 total time=   0.8s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.400 total time=   0.8s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.409 total time=   0.9s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.476 total time=   2.2s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.478 total time=   2.2s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.477 total time=   2.2s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.468 total time=   2.2s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.491 total time=   2.2s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.418 total time=   0.3s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.408 total time=   0.3s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.403 total time=   0.3s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.418 total time=   0.3s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.400 total time=   0.3s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.364 total time=   0.4s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.385 total time=   0.4s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.356 total time=   0.4s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.380 total time=   0.4s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.397 total time=   0.4s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.391 total time=   0.9s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.402 total time=   0.8s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.413 total time=   0.9s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.378 total time=   0.8s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.396 total time=   0.8s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.473 total time=   1.2s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.471 total time=   1.2s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.459 total time=   1.2s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.443 total time=   1.2s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.468 total time=   1.2s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.393 total time=   0.4s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.396 total time=   0.4s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.395 total time=   0.4s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.351 total time=   0.4s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.374 total time=   0.4s\n",
      "Best parameters: {'n_estimators': 25, 'max_features': 10, 'max_depth': 50, 'bootstrap': False} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[6420    0]\n",
      " [   4 5100]] \n",
      "\n",
      "Accuracy: 100.0 \n",
      "\n",
      "F1 Score: 100.0 \n",
      "\n",
      "Balanced accuracy: 100.0 \n",
      "\n",
      "AUC Score: 100.0 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[524 190]\n",
      " [328 225]] \n",
      "\n",
      "Accuracy: 59.1 \n",
      "\n",
      "F1 Score: 46.5 \n",
      "\n",
      "Balanced accuracy: 46.5 \n",
      "\n",
      "AUC Score: 57.0 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = RFClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'n_estimators': [10, 25],\n",
    "    'max_features': [5, 10],\n",
    "    'max_depth': [10, 50, None],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_roberta, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_roberta, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_rf_roberta_train, accuracy_rf_roberta_train, f1_rf_roberta_train, balaccuracy_rf_roberta_train, rocauc_rf_roberta_train = classifier.evaluate(X_train_embeddings_roberta, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_rf_roberta_test, accuracy_rf_roberta_test, f1_rf_roberta_test, balaccuracy_rf_roberta_test, rocauc_rf_roberta_test = classifier.evaluate(X_test_embeddings_roberta, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_rf_roberta_train_tosave = repr(cm_rf_roberta_train)\n",
    "accuracy_rf_roberta_train_tosave = repr(accuracy_rf_roberta_train)\n",
    "f1_rf_roberta_train_tosave = repr(f1_rf_roberta_train)\n",
    "balaccuracy_rf_roberta_train_tosave = repr(balaccuracy_rf_roberta_train)\n",
    "rocauc_rf_roberta_train_tosave = repr(rocauc_rf_roberta_train)\n",
    "\n",
    "cm_rf_roberta_test_tosave = repr(cm_rf_roberta_test)\n",
    "accuracy_rf_roberta_test_tosave = repr(accuracy_rf_roberta_test)\n",
    "f1_rf_roberta_test_tosave = repr(f1_rf_roberta_test)\n",
    "balaccuracy_rf_roberta_test_tosave = repr(balaccuracy_rf_roberta_test)\n",
    "rocauc_rf_roberta_test_tosave = repr(rocauc_rf_roberta_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/rf_roberta_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_rf_roberta_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_rf_roberta_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_rf_roberta_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_rf_roberta_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_rf_roberta_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_rf_roberta_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_rf_roberta_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_rf_roberta_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_rf_roberta_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_rf_roberta_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa + SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:08:39.933979119Z",
     "start_time": "2023-05-22T01:08:26.901157981Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:305: UserWarning: The total space of parameters 16 is smaller than n_iter=20. Running 16 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV 1/5] END ...C=1, gamma=scale, kernel=linear;, score=0.533 total time=  24.6s\n",
      "[CV 2/5] END ...C=1, gamma=scale, kernel=linear;, score=0.515 total time=  24.6s\n",
      "[CV 3/5] END ...C=1, gamma=scale, kernel=linear;, score=0.505 total time=  24.5s\n",
      "[CV 4/5] END ...C=1, gamma=scale, kernel=linear;, score=0.515 total time=  24.4s\n",
      "[CV 5/5] END ...C=1, gamma=scale, kernel=linear;, score=0.532 total time=  24.6s\n",
      "[CV 1/5] END .....C=1, gamma=scale, kernel=poly;, score=0.002 total time=  27.1s\n",
      "[CV 2/5] END .....C=1, gamma=scale, kernel=poly;, score=0.000 total time=  27.1s\n",
      "[CV 3/5] END .....C=1, gamma=scale, kernel=poly;, score=0.000 total time=  27.3s\n",
      "[CV 4/5] END .....C=1, gamma=scale, kernel=poly;, score=0.002 total time=  28.4s\n",
      "[CV 5/5] END .....C=1, gamma=scale, kernel=poly;, score=0.000 total time=  26.9s\n",
      "[CV 1/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.000 total time=  31.4s\n",
      "[CV 2/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.000 total time=  31.4s\n",
      "[CV 3/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.000 total time=  31.1s\n",
      "[CV 4/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.000 total time=  31.6s\n",
      "[CV 5/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.000 total time=  31.7s\n",
      "[CV 1/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.000 total time=  34.5s\n",
      "[CV 2/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.000 total time=  35.4s\n",
      "[CV 3/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.000 total time=  29.3s\n",
      "[CV 4/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.000 total time=  34.3s\n",
      "[CV 5/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.000 total time=  34.5s\n",
      "[CV 1/5] END ..C=10, gamma=scale, kernel=linear;, score=0.549 total time=  29.3s\n",
      "[CV 2/5] END ..C=10, gamma=scale, kernel=linear;, score=0.532 total time=  29.1s\n",
      "[CV 3/5] END ..C=10, gamma=scale, kernel=linear;, score=0.524 total time=  28.8s\n",
      "[CV 4/5] END ..C=10, gamma=scale, kernel=linear;, score=0.516 total time=  29.0s\n",
      "[CV 5/5] END ..C=10, gamma=scale, kernel=linear;, score=0.558 total time=  29.4s\n",
      "[CV 1/5] END ....C=10, gamma=scale, kernel=poly;, score=0.491 total time=  25.7s\n",
      "[CV 2/5] END ....C=10, gamma=scale, kernel=poly;, score=0.471 total time=  25.6s\n",
      "[CV 3/5] END ....C=10, gamma=scale, kernel=poly;, score=0.469 total time=  25.6s\n",
      "[CV 4/5] END ....C=10, gamma=scale, kernel=poly;, score=0.492 total time=  25.5s\n",
      "[CV 5/5] END ....C=10, gamma=scale, kernel=poly;, score=0.501 total time=  26.1s\n",
      "[CV 1/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.490 total time=  30.3s\n",
      "[CV 2/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.454 total time=  30.4s\n",
      "[CV 3/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.463 total time=  30.2s\n",
      "[CV 4/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.479 total time=  30.0s\n",
      "[CV 5/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.477 total time=  30.4s\n",
      "[CV 1/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.442 total time=  37.2s\n",
      "[CV 2/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.407 total time=  37.2s\n",
      "[CV 3/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.405 total time=  26.3s\n",
      "[CV 4/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.415 total time=  37.1s\n",
      "[CV 5/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.419 total time=  37.1s\n",
      "[CV 1/5] END .C=100, gamma=scale, kernel=linear;, score=0.555 total time= 1.2min\n",
      "[CV 2/5] END .C=100, gamma=scale, kernel=linear;, score=0.526 total time= 1.2min\n",
      "[CV 3/5] END .C=100, gamma=scale, kernel=linear;, score=0.530 total time= 1.2min\n",
      "[CV 4/5] END .C=100, gamma=scale, kernel=linear;, score=0.528 total time= 1.2min\n",
      "[CV 5/5] END .C=100, gamma=scale, kernel=linear;, score=0.561 total time= 1.2min\n",
      "[CV 1/5] END ...C=100, gamma=scale, kernel=poly;, score=0.540 total time=  25.3s\n",
      "[CV 2/5] END ...C=100, gamma=scale, kernel=poly;, score=0.521 total time=  25.3s\n",
      "[CV 3/5] END ...C=100, gamma=scale, kernel=poly;, score=0.516 total time=  25.5s\n",
      "[CV 4/5] END ...C=100, gamma=scale, kernel=poly;, score=0.516 total time=  25.1s\n",
      "[CV 5/5] END ...C=100, gamma=scale, kernel=poly;, score=0.541 total time=  25.3s\n",
      "[CV 1/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.543 total time=  29.4s\n",
      "[CV 2/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.519 total time=  29.9s\n",
      "[CV 3/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.509 total time=  29.3s\n",
      "[CV 4/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.520 total time=  29.2s\n",
      "[CV 5/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.542 total time=  29.4s\n",
      "[CV 1/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.472 total time=  15.3s\n",
      "[CV 2/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.508 total time=  15.6s\n",
      "[CV 3/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.427 total time=  17.3s\n",
      "[CV 4/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.481 total time=  15.4s\n",
      "[CV 5/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.501 total time=  15.6s\n",
      "[CV 1/5] END C=1000, gamma=scale, kernel=linear;, score=0.550 total time= 5.9min\n",
      "[CV 2/5] END C=1000, gamma=scale, kernel=linear;, score=0.521 total time= 8.4min\n",
      "[CV 3/5] END C=1000, gamma=scale, kernel=linear;, score=0.535 total time= 6.5min\n",
      "[CV 4/5] END C=1000, gamma=scale, kernel=linear;, score=0.525 total time= 7.0min\n",
      "[CV 5/5] END C=1000, gamma=scale, kernel=linear;, score=0.547 total time= 7.4min\n",
      "[CV 1/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.553 total time=  35.1s\n",
      "[CV 2/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.536 total time=  35.2s\n",
      "[CV 3/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.520 total time=  35.2s\n",
      "[CV 4/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.528 total time=  35.3s\n",
      "[CV 5/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.557 total time=  35.7s\n",
      "[CV 1/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.550 total time=  38.6s\n",
      "[CV 2/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.537 total time=  39.6s\n",
      "[CV 3/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.517 total time=  38.5s\n",
      "[CV 4/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.520 total time=  38.4s\n",
      "[CV 5/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.563 total time=  39.1s\n",
      "[CV 1/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.475 total time=  14.3s\n",
      "[CV 2/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.511 total time=  14.7s\n",
      "[CV 3/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.494 total time=  14.4s\n",
      "[CV 4/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.481 total time=  14.3s\n",
      "[CV 5/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.502 total time=  14.5s\n",
      "Best parameters: {'kernel': 'linear', 'gamma': 'scale', 'C': 100} \n",
      "\n",
      "[LibSVM]\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[4929 1491]\n",
      " [2164 2940]] \n",
      "\n",
      "Accuracy: 68.3 \n",
      "\n",
      "F1 Score: 61.7 \n",
      "\n",
      "Balanced accuracy: 61.7 \n",
      "\n",
      "AUC Score: 67.2 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[503 211]\n",
      " [277 276]] \n",
      "\n",
      "Accuracy: 61.5 \n",
      "\n",
      "F1 Score: 53.1 \n",
      "\n",
      "Balanced accuracy: 53.1 \n",
      "\n",
      "AUC Score: 60.2 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = SVClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'C': [1, 10, 100, 1000],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale']\n",
    "    #'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_roberta, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_roberta, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_svc_roberta_train, accuracy_svc_roberta_train, f1_svc_roberta_train, balaccuracy_svc_roberta_train, rocauc_svc_roberta_train = classifier.evaluate(X_train_embeddings_roberta, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_svc_roberta_test, accuracy_svc_roberta_test, f1_svc_roberta_test, balaccuracy_svc_roberta_test, rocauc_svc_roberta_test = classifier.evaluate(X_test_embeddings_roberta, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_svc_roberta_train_tosave = repr(cm_svc_roberta_train)\n",
    "accuracy_svc_roberta_train_tosave = repr(accuracy_svc_roberta_train)\n",
    "f1_svc_roberta_train_tosave = repr(f1_svc_roberta_train)\n",
    "balaccuracy_svc_roberta_train_tosave = repr(balaccuracy_svc_roberta_train)\n",
    "rocauc_svc_roberta_train_tosave = repr(rocauc_svc_roberta_train)\n",
    "\n",
    "cm_svc_roberta_test_tosave = repr(cm_svc_roberta_test)\n",
    "accuracy_svc_roberta_test_tosave = repr(accuracy_svc_roberta_test)\n",
    "f1_svc_roberta_test_tosave = repr(f1_svc_roberta_test)\n",
    "balaccuracy_svc_roberta_test_tosave = repr(balaccuracy_svc_roberta_test)\n",
    "rocauc_svc_roberta_test_tosave = repr(rocauc_svc_roberta_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/svc_roberta_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_svc_roberta_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_svc_roberta_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_svc_roberta_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_svc_roberta_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_svc_roberta_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_svc_roberta_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_svc_roberta_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_svc_roberta_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_svc_roberta_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_svc_roberta_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:08:50.413713572Z",
     "start_time": "2023-05-22T01:08:39.932072493Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=0.9400000000000001, penalty=l2, solver=lbfgs;, score=0.540 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=0.9400000000000001, penalty=l2, solver=lbfgs;, score=0.513 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=0.9400000000000001, penalty=l2, solver=lbfgs;, score=0.508 total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=0.9400000000000001, penalty=l2, solver=lbfgs;, score=0.501 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=0.9400000000000001, penalty=l2, solver=lbfgs;, score=0.540 total time=   0.3s\n",
      "[CV 1/5] END C=0.12, penalty=l2, solver=liblinear;, score=0.481 total time=   0.6s\n",
      "[CV 2/5] END C=0.12, penalty=l2, solver=liblinear;, score=0.472 total time=   0.5s\n",
      "[CV 3/5] END C=0.12, penalty=l2, solver=liblinear;, score=0.477 total time=   0.6s\n",
      "[CV 4/5] END C=0.12, penalty=l2, solver=liblinear;, score=0.477 total time=   0.6s\n",
      "[CV 5/5] END C=0.12, penalty=l2, solver=liblinear;, score=0.487 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ..C=0.31, penalty=l2, solver=lbfgs;, score=0.518 total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END ..C=0.31, penalty=l2, solver=lbfgs;, score=0.511 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END ..C=0.31, penalty=l2, solver=lbfgs;, score=0.498 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END ..C=0.31, penalty=l2, solver=lbfgs;, score=0.500 total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END ..C=0.31, penalty=l2, solver=lbfgs;, score=0.521 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ..C=0.27, penalty=l2, solver=lbfgs;, score=0.517 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END ..C=0.27, penalty=l2, solver=lbfgs;, score=0.508 total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END ..C=0.27, penalty=l2, solver=lbfgs;, score=0.495 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END ..C=0.27, penalty=l2, solver=lbfgs;, score=0.495 total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END ..C=0.27, penalty=l2, solver=lbfgs;, score=0.520 total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=0.78, penalty=none, solver=lbfgs;, score=0.548 total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=0.78, penalty=none, solver=lbfgs;, score=0.523 total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=0.78, penalty=none, solver=lbfgs;, score=0.521 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=0.78, penalty=none, solver=lbfgs;, score=0.527 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=0.78, penalty=none, solver=lbfgs;, score=0.548 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ..C=0.29, penalty=none, solver=sag;, score=0.546 total time=   3.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END ..C=0.29, penalty=none, solver=sag;, score=0.526 total time=   3.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END ..C=0.29, penalty=none, solver=sag;, score=0.525 total time=   3.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END ..C=0.29, penalty=none, solver=sag;, score=0.517 total time=   3.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END ..C=0.29, penalty=none, solver=sag;, score=0.560 total time=   3.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ..C=0.51, penalty=none, solver=sag;, score=0.547 total time=   3.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END ..C=0.51, penalty=none, solver=sag;, score=0.526 total time=   3.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END ..C=0.51, penalty=none, solver=sag;, score=0.524 total time=   3.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END ..C=0.51, penalty=none, solver=sag;, score=0.518 total time=   3.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END ..C=0.51, penalty=none, solver=sag;, score=0.559 total time=   3.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ....C=0.39, penalty=l2, solver=sag;, score=0.525 total time=   3.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END ....C=0.39, penalty=l2, solver=sag;, score=0.513 total time=   3.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END ....C=0.39, penalty=l2, solver=sag;, score=0.500 total time=   3.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END ....C=0.39, penalty=l2, solver=sag;, score=0.501 total time=   3.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END ....C=0.39, penalty=l2, solver=sag;, score=0.521 total time=   3.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ...C=0.74, penalty=l2, solver=saga;, score=0.545 total time=   4.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END ...C=0.74, penalty=l2, solver=saga;, score=0.519 total time=   4.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END ...C=0.74, penalty=l2, solver=saga;, score=0.505 total time=   4.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END ...C=0.74, penalty=l2, solver=saga;, score=0.508 total time=   4.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END ...C=0.74, penalty=l2, solver=saga;, score=0.540 total time=   4.2s\n",
      "[CV 1/5] END C=0.5, penalty=l2, solver=liblinear;, score=0.539 total time=   0.8s\n",
      "[CV 2/5] END C=0.5, penalty=l2, solver=liblinear;, score=0.515 total time=   0.8s\n",
      "[CV 3/5] END C=0.5, penalty=l2, solver=liblinear;, score=0.501 total time=   0.8s\n",
      "[CV 4/5] END C=0.5, penalty=l2, solver=liblinear;, score=0.505 total time=   0.8s\n",
      "[CV 5/5] END C=0.5, penalty=l2, solver=liblinear;, score=0.533 total time=   0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\utils\\optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=0.15, penalty=none, solver=newton-cg;, score=0.543 total time=  31.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\utils\\optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=0.15, penalty=none, solver=newton-cg;, score=0.537 total time=  26.9s\n",
      "[CV 3/5] END C=0.15, penalty=none, solver=newton-cg;, score=0.526 total time=  33.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\utils\\optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=0.15, penalty=none, solver=newton-cg;, score=0.520 total time=  28.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\utils\\optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=0.15, penalty=none, solver=newton-cg;, score=0.551 total time=  29.8s\n",
      "[CV 1/5] END C=0.01, penalty=l2, solver=liblinear;, score=0.070 total time=   0.4s\n",
      "[CV 2/5] END C=0.01, penalty=l2, solver=liblinear;, score=0.049 total time=   0.4s\n",
      "[CV 3/5] END C=0.01, penalty=l2, solver=liblinear;, score=0.081 total time=   0.4s\n",
      "[CV 4/5] END C=0.01, penalty=l2, solver=liblinear;, score=0.072 total time=   0.4s\n",
      "[CV 5/5] END C=0.01, penalty=l2, solver=liblinear;, score=0.047 total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=0.8200000000000001, penalty=l1, solver=saga;, score=0.520 total time=   6.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=0.8200000000000001, penalty=l1, solver=saga;, score=0.497 total time=   6.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=0.8200000000000001, penalty=l1, solver=saga;, score=0.496 total time=   6.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=0.8200000000000001, penalty=l1, solver=saga;, score=0.500 total time=   6.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=0.8200000000000001, penalty=l1, solver=saga;, score=0.521 total time=   6.2s\n",
      "[CV 1/5] END C=0.61, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.61, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.61, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.61, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.61, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\utils\\optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=0.44, penalty=none, solver=newton-cg;, score=0.543 total time=  31.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\utils\\optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=0.44, penalty=none, solver=newton-cg;, score=0.537 total time=  26.9s\n",
      "[CV 3/5] END C=0.44, penalty=none, solver=newton-cg;, score=0.526 total time=  33.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\utils\\optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=0.44, penalty=none, solver=newton-cg;, score=0.520 total time=  28.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\utils\\optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=0.44, penalty=none, solver=newton-cg;, score=0.551 total time=  29.7s\n",
      "[CV 1/5] END C=0.36, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.36, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.36, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.36, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.36, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.5700000000000001, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.5700000000000001, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.5700000000000001, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.5700000000000001, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.5700000000000001, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.6, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.6, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.6, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.6, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.6, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ..C=0.04, penalty=l2, solver=lbfgs;, score=0.397 total time=   0.2s\n",
      "[CV 2/5] END ..C=0.04, penalty=l2, solver=lbfgs;, score=0.412 total time=   0.2s\n",
      "[CV 3/5] END ..C=0.04, penalty=l2, solver=lbfgs;, score=0.400 total time=   0.2s\n",
      "[CV 4/5] END ..C=0.04, penalty=l2, solver=lbfgs;, score=0.404 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END ..C=0.04, penalty=l2, solver=lbfgs;, score=0.412 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ...C=0.79, penalty=l2, solver=saga;, score=0.544 total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END ...C=0.79, penalty=l2, solver=saga;, score=0.520 total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END ...C=0.79, penalty=l2, solver=saga;, score=0.505 total time=   4.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END ...C=0.79, penalty=l2, solver=saga;, score=0.512 total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "20 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1048, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 864, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 782, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.52045959 0.47892801 0.50960248 0.5067795  0.53334197 0.53477589\n",
      " 0.53468915 0.51200571 0.52352339 0.51860596 0.53546579 0.06403895\n",
      " 0.50681498        nan 0.53546579        nan        nan        nan\n",
      " 0.40509977 0.52502288]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END ...C=0.79, penalty=l2, solver=saga;, score=0.544 total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\utils\\optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'solver': 'newton-cg', 'penalty': 'none', 'C': 0.15} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[4897 1523]\n",
      " [2137 2967]] \n",
      "\n",
      "Accuracy: 68.2 \n",
      "\n",
      "F1 Score: 61.9 \n",
      "\n",
      "Balanced accuracy: 61.9 \n",
      "\n",
      "AUC Score: 67.2 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[487 227]\n",
      " [279 274]] \n",
      "\n",
      "Accuracy: 60.1 \n",
      "\n",
      "F1 Score: 52.0 \n",
      "\n",
      "Balanced accuracy: 52.0 \n",
      "\n",
      "AUC Score: 58.9 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wesub\\anaconda3\\lib\\site-packages\\sklearn\\utils\\optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = LRClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'C' : np.arange(0, 1, 0.01)\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_roberta, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_roberta, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_lr_roberta_train, accuracy_lr_roberta_train, f1_lr_roberta_train, balaccuracy_lr_roberta_train, rocauc_lr_roberta_train = classifier.evaluate(X_train_embeddings_roberta, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_lr_roberta_test, accuracy_lr_roberta_test, f1_lr_roberta_test, balaccuracy_lr_roberta_test, rocauc_lr_roberta_test = classifier.evaluate(X_test_embeddings_roberta, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_lr_roberta_train_tosave = repr(cm_lr_roberta_train)\n",
    "accuracy_lr_roberta_train_tosave = repr(accuracy_lr_roberta_train)\n",
    "f1_lr_roberta_train_tosave = repr(f1_lr_roberta_train)\n",
    "balaccuracy_lr_roberta_train_tosave = repr(balaccuracy_lr_roberta_train)\n",
    "rocauc_lr_roberta_train_tosave = repr(rocauc_lr_roberta_train)\n",
    "\n",
    "cm_lr_roberta_test_tosave = repr(cm_lr_roberta_test)\n",
    "accuracy_lr_roberta_test_tosave = repr(accuracy_lr_roberta_test)\n",
    "f1_lr_roberta_test_tosave = repr(f1_lr_roberta_test)\n",
    "balaccuracy_lr_roberta_test_tosave = repr(balaccuracy_lr_roberta_test)\n",
    "rocauc_lr_roberta_test_tosave = repr(rocauc_lr_roberta_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/lr_roberta_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_lr_roberta_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_lr_roberta_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_lr_roberta_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_lr_roberta_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_lr_roberta_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_lr_roberta_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_lr_roberta_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_lr_roberta_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_lr_roberta_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_lr_roberta_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa + Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:09:38.464715395Z",
     "start_time": "2023-05-22T01:08:50.416675061Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Train Loss: 0.6733 | Val Loss: 0.6550 | F1 Score: 0.5367 | Balanced Accuracy: 0.5967 | AUC: 0.5967\n",
      "Epoch 2/15 | Train Loss: 0.6576 | Val Loss: 0.6509 | F1 Score: 0.5446 | Balanced Accuracy: 0.6039 | AUC: 0.6039\n",
      "Epoch 3/15 | Train Loss: 0.6518 | Val Loss: 0.6494 | F1 Score: 0.5424 | Balanced Accuracy: 0.6027 | AUC: 0.6027\n",
      "Epoch 4/15 | Train Loss: 0.6490 | Val Loss: 0.6488 | F1 Score: 0.5577 | Balanced Accuracy: 0.6134 | AUC: 0.6134\n",
      "Epoch 5/15 | Train Loss: 0.6456 | Val Loss: 0.6478 | F1 Score: 0.5441 | Balanced Accuracy: 0.6089 | AUC: 0.6089\n",
      "Epoch 6/15 | Train Loss: 0.6435 | Val Loss: 0.6477 | F1 Score: 0.5546 | Balanced Accuracy: 0.6138 | AUC: 0.6138\n",
      "Epoch 7/15 | Train Loss: 0.6412 | Val Loss: 0.6481 | F1 Score: 0.5646 | Balanced Accuracy: 0.6191 | AUC: 0.6191\n",
      "Epoch 8/15 | Train Loss: 0.6384 | Val Loss: 0.6483 | F1 Score: 0.5622 | Balanced Accuracy: 0.6168 | AUC: 0.6168\n",
      "Epoch 9/15 | Train Loss: 0.6354 | Val Loss: 0.6477 | F1 Score: 0.5581 | Balanced Accuracy: 0.6175 | AUC: 0.6175\n",
      "Epoch 10/15 | Train Loss: 0.6330 | Val Loss: 0.6486 | F1 Score: 0.5530 | Balanced Accuracy: 0.6134 | AUC: 0.6134\n",
      "Epoch 11/15 | Train Loss: 0.6296 | Val Loss: 0.6495 | F1 Score: 0.5535 | Balanced Accuracy: 0.6141 | AUC: 0.6141\n",
      "Epoch 12/15 | Train Loss: 0.6267 | Val Loss: 0.6511 | F1 Score: 0.5551 | Balanced Accuracy: 0.6145 | AUC: 0.6145\n",
      "Epoch 13/15 | Train Loss: 0.6223 | Val Loss: 0.6518 | F1 Score: 0.5487 | Balanced Accuracy: 0.6123 | AUC: 0.6123\n",
      "Epoch 14/15 | Train Loss: 0.6179 | Val Loss: 0.6538 | F1 Score: 0.5475 | Balanced Accuracy: 0.6097 | AUC: 0.6097\n",
      "Early stopping triggered. No improvement in 5 epochs.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[4507 1913]\n",
      " [2113 2991]] \n",
      "\n",
      "Accuracy: 65.1 \n",
      "\n",
      "F1 Score: 59.8 \n",
      "\n",
      "Balanced accuracy: 64.4 \n",
      "\n",
      "AUC Score: 64.4 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[495 219]\n",
      " [262 291]] \n",
      "\n",
      "Accuracy: 62.0 \n",
      "\n",
      "F1 Score: 54.8 \n",
      "\n",
      "Balanced accuracy: 61.0 \n",
      "\n",
      "AUC Score: 61.0 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# SET PARAMETERS\n",
    "dropout_rate = 0.1\n",
    "epochs_number = 15\n",
    "learning_rate = 0.0001\n",
    "early_stopping = 5\n",
    "# --------------------\n",
    "\n",
    "input_dim = X_train_embeddings_roberta.shape[1]  # Dimensionality of word embeddings\n",
    "hidden_dim = X_train_embeddings_roberta.shape[1]*2  # Number of units in the hidden layer\n",
    "\n",
    "classifier = NeuralNetworkClassifier(input_dim, hidden_dim, dropout=dropout_rate)\n",
    "\n",
    "# Fit the model\n",
    "classifier.fit(X_train_embeddings_roberta, y_train, X_test_embeddings_roberta, y_test, num_epochs=epochs_number, lr=learning_rate, patience=early_stopping)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_nn_roberta_train, accuracy_nn_roberta_train, f1_nn_roberta_train, balaccuracy_nn_roberta_train, rocauc_nn_roberta_train = classifier.evaluate(X_train_embeddings_roberta, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_nn_roberta_test, accuracy_nn_roberta_test, f1_nn_roberta_test, balaccuracy_nn_roberta_test, rocauc_nn_roberta_test = classifier.evaluate(X_test_embeddings_roberta, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm_nn_roberta_train_tosave = repr(cm_nn_roberta_train)\n",
    "accuracy_nn_roberta_train_tosave = repr(accuracy_nn_roberta_train)\n",
    "f1_nn_roberta_train_tosave = repr(f1_nn_roberta_train)\n",
    "balaccuracy_nn_roberta_train_tosave = repr(balaccuracy_nn_roberta_train)\n",
    "rocauc_nn_roberta_train_tosave = repr(rocauc_nn_roberta_train)\n",
    "\n",
    "cm_nn_roberta_test_tosave = repr(cm_nn_roberta_test)\n",
    "accuracy_nn_roberta_test_tosave = repr(accuracy_nn_roberta_test)\n",
    "f1_nn_roberta_test_tosave = repr(f1_nn_roberta_test)\n",
    "balaccuracy_nn_roberta_test_tosave = repr(balaccuracy_nn_roberta_test)\n",
    "rocauc_nn_roberta_test_tosave = repr(rocauc_nn_roberta_test)\n",
    "\n",
    "\n",
    "file = open(\"results_split/nn_roberta_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".txt\", \"w\")\n",
    "\n",
    "file.write(\"Train CM = \"+cm_nn_roberta_train_tosave + \"\\n\" +\n",
    "           \"Train Accuracy = \" + accuracy_nn_roberta_train_tosave + \"\\n\" +\n",
    "           \"Train F1 = \"+f1_nn_roberta_train_tosave + \"\\n\"+\n",
    "           \"Train Balanced Accuracy = \"+balaccuracy_nn_roberta_train_tosave + \"\\n\" +\n",
    "           \"Train ROCAUC = \"+rocauc_nn_roberta_train_tosave + \"\\n\"+\n",
    "           \"Test CM = \"+cm_nn_roberta_test_tosave + \"\\n\"+\n",
    "           \"Test Accuracy = \"+accuracy_nn_roberta_test_tosave + \"\\n\"+\n",
    "           \"Test F1 = \"+f1_nn_roberta_test_tosave + \"\\n\"+\n",
    "           \"Test Balanced Accuracy = \"+balaccuracy_nn_roberta_test_tosave + \"\\n\"+\n",
    "           \"Test ROCAUC = \"+rocauc_nn_roberta_test_tosave)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:09:38.465052113Z",
     "start_time": "2023-05-22T01:09:38.428136128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model                      Accuracy    F1-Score    Balanced Accuracy    ROC AUC \n",
      "\n",
      " LR+GloVe                       64.6        78.5                 50         50   \n",
      "\n",
      " KNN+GloVe                      60.8        73.8                 50.6       50.6 \n",
      "\n",
      " SVC+GloVe                      64.6        78.5                 50         50   \n",
      "\n",
      " RandomForest+GloVe             61.7        75.9                 48.8       48.8 \n",
      "\n",
      " XGBoost+GloVe                  63.4        77.1                 50.3       50.3 \n",
      "\n",
      " NeuralNetwork+GloVe            62.4        74.4                 53.2       53.2 \n",
      "\n",
      " LR+Word2Vec                    64.6        78.5                 50         50   \n",
      "\n",
      " KNN+Word2Vec                   60.7        73.8                 50.4       50.4 \n",
      "\n",
      " SVC+Word2Vec                   64          77.9                 49.9       49.9 \n",
      "\n",
      " RandomForest + Word2Vec        61.5        74.6                 50.7       50.7 \n",
      "\n",
      " XGBoost+Word2Vec               64.1        77.8                 50.3       50.3 \n",
      "\n",
      " NeuralNetwork+Word2Vec         63.9        77.3                 51.1       51.1 \n",
      "\n",
      " LR+BERT                        58.6        70.5                 51.2       51.2 \n",
      "\n",
      " KNN+BERT                       58.4        71.7                 48.9       48.9 \n",
      "\n",
      " SVC+BERT                       64.6        78.5                 50         50   \n",
      "\n",
      " RandomForest+BERT              64.7        78                   51.5       51.5 \n",
      "\n",
      " XGBoost+BERT                   64.2        78.2                 49.8       49.8 \n",
      "\n",
      " NeuralNetwork+BERT             59          71                   51.2       51.2 \n",
      "\n",
      " LR+RoBERTa                     64.6        78.5                 50         50   \n",
      "\n",
      " KNN+RoBERTa                    58.8        72.2                 48.9       48.9 \n",
      "\n",
      " SVC+RoBERTa                    64.6        78.5                 50         50   \n",
      "\n",
      " RandomForest+RoBERTa           63.7        77.5                 50         50   \n",
      "\n",
      " XGBoost+RoBERTa                64.7        78.5                 50.2       50.2 \n",
      "\n",
      " NeuralNetwork+RoBERTa          63.4        77.2                 50         50   \n",
      "\n",
      " LR+GPT2                        64.6        78.5                 50         50   \n",
      "\n",
      " KNN+GPT2                       58.7        70.9                 50.9       50.9 \n",
      "\n",
      " SVC+GPT2                       64.6        78.5                 50         50   \n",
      "\n",
      " RandomForest+GPT2              62.9        77                   49.2       49.2 \n",
      "\n",
      " XGBoost+GPT2                   64.3        78.1                 50.4       50.4 \n",
      "\n",
      " NeuralNetwork+GPT2             61.4        74.1                 51.6       51.6 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create data\n",
    "data = [[\"LR+GloVe\", accuracy_lr_glove_test, f1_lr_glove_test, balaccuracy_lr_glove_test, rocauc_lr_glove_test],\n",
    "        [\"KNN+GloVe\", accuracy_knn_glove_test, f1_knn_glove_test, balaccuracy_knn_glove_test, rocauc_knn_glove_test],\n",
    "        [\"SVC+GloVe\", accuracy_svc_glove_test, f1_svc_glove_test, balaccuracy_svc_glove_test, rocauc_svc_glove_test],\n",
    "        [\"RandomForest+GloVe\", accuracy_rf_glove_test, f1_rf_glove_test, balaccuracy_rf_glove_test, rocauc_rf_glove_test],\n",
    "        [\"XGBoost+GloVe\", accuracy_xgb_glove_test, f1_xgb_glove_test, balaccuracy_xgb_glove_test, rocauc_xgb_glove_test],\n",
    "        [\"NeuralNetwork+GloVe\", accuracy_nn_glove_test, f1_nn_glove_test, balaccuracy_nn_glove_test, rocauc_nn_glove_test],\n",
    "        [\"LR+Word2Vec\", accuracy_lr_w2v_test, f1_lr_w2v_test, balaccuracy_lr_w2v_test, rocauc_lr_w2v_test],\n",
    "        [\"KNN+Word2Vec\", accuracy_knn_w2v_test, f1_knn_w2v_test, balaccuracy_knn_w2v_test, rocauc_knn_w2v_test],\n",
    "        [\"SVC+Word2Vec\", accuracy_svc_w2v_test, f1_svc_w2v_test, balaccuracy_svc_w2v_test, rocauc_svc_w2v_test],\n",
    "        [\"RandomForest+Word2Vec\", accuracy_rf_w2v_test, f1_rf_w2v_test, balaccuracy_rf_w2v_test, rocauc_rf_w2v_test],\n",
    "        [\"XGBoost+Word2Vec\", accuracy_xgb_w2v_test, f1_xgb_w2v_test, balaccuracy_xgb_w2v_test, rocauc_xgb_w2v_test],\n",
    "        [\"NeuralNetwork+Word2Vec\", accuracy_nn_w2v_test, f1_nn_w2v_test, balaccuracy_nn_w2v_test, rocauc_nn_w2v_test],\n",
    "        [\"LR+BERT\", accuracy_lr_bert_test, f1_lr_bert_test, balaccuracy_lr_bert_test, rocauc_lr_bert_test],\n",
    "        [\"KNN+BERT\", accuracy_knn_bert_test, f1_knn_bert_test, balaccuracy_knn_bert_test, rocauc_knn_bert_test],\n",
    "        [\"SVC+BERT\", accuracy_svc_bert_test, f1_svc_bert_test, balaccuracy_svc_bert_test, rocauc_svc_bert_test],\n",
    "        [\"RandomForest+BERT\", accuracy_rf_bert_test, f1_rf_bert_test, balaccuracy_rf_bert_test, rocauc_rf_bert_test],\n",
    "        [\"XGBoost+BERT\", accuracy_xgb_bert_test, f1_xgb_bert_test, balaccuracy_xgb_bert_test, rocauc_xgb_bert_test],\n",
    "        [\"NeuralNetwork+BERT\", accuracy_nn_bert_test, f1_nn_bert_test, balaccuracy_nn_bert_test, rocauc_nn_bert_test],\n",
    "        [\"LR+RoBERTa\", accuracy_lr_roberta_test, f1_lr_roberta_test, balaccuracy_lr_roberta_test, rocauc_lr_roberta_test],\n",
    "        [\"KNN+RoBERTa\", accuracy_knn_roberta_test, f1_knn_roberta_test, balaccuracy_knn_roberta_test, rocauc_knn_roberta_test],\n",
    "        [\"SVC+RoBERTa\", accuracy_svc_roberta_test, f1_svc_roberta_test, balaccuracy_svc_roberta_test, rocauc_svc_roberta_test],\n",
    "        [\"RandomForest+RoBERTa\", accuracy_rf_roberta_test, f1_rf_roberta_test, balaccuracy_rf_roberta_test, rocauc_rf_roberta_test],\n",
    "        [\"XGBoost+RoBERTa\", accuracy_xgb_roberta_test, f1_xgb_roberta_test, balaccuracy_xgb_roberta_test, rocauc_xgb_roberta_test],\n",
    "        [\"NeuralNetwork+RoBERTa\", accuracy_nn_roberta_test, f1_nn_roberta_test, balaccuracy_nn_roberta_test, rocauc_nn_roberta_test],\n",
    "        [\"LR+GPT2\", accuracy_lr_gpt2_test, f1_lr_gpt2_test, balaccuracy_lr_gpt2_test, rocauc_lr_gpt2_test],\n",
    "        [\"KNN+GPT2\", accuracy_knn_gpt2_test, f1_knn_gpt2_test, balaccuracy_knn_gpt2_test, rocauc_knn_gpt2_test],\n",
    "        [\"SVC+GPT2\", accuracy_svc_gpt2_test, f1_svc_gpt2_test, balaccuracy_svc_gpt2_test, rocauc_svc_gpt2_test],\n",
    "        [\"RandomForest+GPT2\", accuracy_rf_gpt2_test, f1_rf_gpt2_test, balaccuracy_rf_gpt2_test, rocauc_rf_gpt2_test],\n",
    "        [\"XGBoost+GPT2\", accuracy_xgb_gpt2_test, f1_xgb_gpt2_test, balaccuracy_xgb_gpt2_test, rocauc_xgb_gpt2_test],\n",
    "        [\"NeuralNetwork+GPT2\", accuracy_nn_gpt2_test, f1_nn_gpt2_test, balaccuracy_nn_gpt2_test, rocauc_nn_gpt2_test]]\n",
    "  \n",
    "#define header names\n",
    "col_names = [\"Model\", \"Accuracy\", \"F1-Score\", \"Balanced Accuracy\", \"ROC AUC\"]\n",
    "\n",
    "#save results to csv\n",
    "if fast:\n",
    "    with open(\"results_fast_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(col_names)\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "else:\n",
    "    with open(\"results_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(col_names)\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "#display table\n",
    "print(tabulate(data, headers=col_names, tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:10:06.779483117Z",
     "start_time": "2023-05-22T01:10:06.709782207Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\hline\n",
      " Model                   &   Accuracy &   F1-Score &   Balanced Accuracy &   ROC AUC \\\\\n",
      "\\hline\n",
      " LR+GloVe                &       64.6 &       78.5 &                50   &      50   \\\\\n",
      " KNN+GloVe               &       60.8 &       73.8 &                50.6 &      50.6 \\\\\n",
      " SVC+GloVe               &       64.6 &       78.5 &                50   &      50   \\\\\n",
      " RandomForest+GloVe      &       61.7 &       75.9 &                48.8 &      48.8 \\\\\n",
      " XGBoost+GloVe           &       63.4 &       77.1 &                50.3 &      50.3 \\\\\n",
      " NeuralNetwork+GloVe     &       62.4 &       74.4 &                53.2 &      53.2 \\\\\n",
      " LR+Word2Vec             &       64.6 &       78.5 &                50   &      50   \\\\\n",
      " KNN+Word2Vec            &       60.7 &       73.8 &                50.4 &      50.4 \\\\\n",
      " SVC+Word2Vec            &       64   &       77.9 &                49.9 &      49.9 \\\\\n",
      " RandomForest + Word2Vec &       61.5 &       74.6 &                50.7 &      50.7 \\\\\n",
      " XGBoost+Word2Vec        &       64.1 &       77.8 &                50.3 &      50.3 \\\\\n",
      " NeuralNetwork+Word2Vec  &       63.9 &       77.3 &                51.1 &      51.1 \\\\\n",
      " LR+BERT                 &       58.6 &       70.5 &                51.2 &      51.2 \\\\\n",
      " KNN+BERT                &       58.4 &       71.7 &                48.9 &      48.9 \\\\\n",
      " SVC+BERT                &       64.6 &       78.5 &                50   &      50   \\\\\n",
      " RandomForest+BERT       &       64.7 &       78   &                51.5 &      51.5 \\\\\n",
      " XGBoost+BERT            &       64.2 &       78.2 &                49.8 &      49.8 \\\\\n",
      " NeuralNetwork+BERT      &       59   &       71   &                51.2 &      51.2 \\\\\n",
      " LR+RoBERTa              &       64.6 &       78.5 &                50   &      50   \\\\\n",
      " KNN+RoBERTa             &       58.8 &       72.2 &                48.9 &      48.9 \\\\\n",
      " SVC+RoBERTa             &       64.6 &       78.5 &                50   &      50   \\\\\n",
      " RandomForest+RoBERTa    &       63.7 &       77.5 &                50   &      50   \\\\\n",
      " XGBoost+RoBERTa         &       64.7 &       78.5 &                50.2 &      50.2 \\\\\n",
      " NeuralNetwork+RoBERTa   &       63.4 &       77.2 &                50   &      50   \\\\\n",
      " LR+GPT2                 &       64.6 &       78.5 &                50   &      50   \\\\\n",
      " KNN+GPT2                &       58.7 &       70.9 &                50.9 &      50.9 \\\\\n",
      " SVC+GPT2                &       64.6 &       78.5 &                50   &      50   \\\\\n",
      " RandomForest+GPT2       &       62.9 &       77   &                49.2 &      49.2 \\\\\n",
      " XGBoost+GPT2            &       64.3 &       78.1 &                50.4 &      50.4 \\\\\n",
      " NeuralNetwork+GPT2      &       61.4 &       74.1 &                51.6 &      51.6 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "#print(tabulate(data, headers=col_names, tablefmt=\"latex\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T00:11:44.648347738Z",
     "start_time": "2023-05-08T00:11:44.628206561Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "\n",
      " Model       Accuracy    F1-Score \n",
      "\n",
      " GloVe           53.4        33.2 \n",
      "\n",
      " Word2Vec        59          40.7 \n",
      "\n",
      " BERT            52.8        34.6 \n",
      "\n",
      " RoBERTa         51.9        38   \n",
      "\n",
      " GPT2            55.2        37.6 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #create data\n",
    "# lr_results = [\n",
    "#     [\"GloVe\", accuracy_lr_glove_test, f1_lr_glove_test],\n",
    "#     [\"Word2Vec\", accuracy_lr_w2v_test, f1_lr_w2v_test],\n",
    "#     [\"BERT\", accuracy_lr_bert_test, f1_lr_bert_test],\n",
    "#     [\"RoBERTa\", accuracy_lr_roberta_test, f1_lr_roberta_test],\n",
    "#     [\"GPT2\", accuracy_lr_gpt2_test, f1_lr_gpt2_test]\n",
    "# ]\n",
    "#\n",
    "# #define header names\n",
    "# col_names = [\"Model\", \"Accuracy\", \"F1-Score\"]\n",
    "#\n",
    "# #display table\n",
    "# print(\"Logistic Regression\")\n",
    "# # print(tabulate(lr_results, headers=col_names, tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T00:11:45.623006697Z",
     "start_time": "2023-05-08T00:11:45.607642365Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "\n",
      " Model       Accuracy    F1-Score \n",
      "\n",
      " GloVe           53.7        34.7 \n",
      "\n",
      " Word2Vec        55.5        40   \n",
      "\n",
      " BERT            53.1        35.7 \n",
      "\n",
      " RoBERTa         52.8        38.2 \n",
      "\n",
      " GPT2            48.5        46.6 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #create data\n",
    "# knn_results = [\n",
    "#     [\"GloVe\", accuracy_knn_glove_test, f1_knn_glove_test],\n",
    "#     [\"Word2Vec\", accuracy_knn_w2v_test, f1_knn_w2v_test],\n",
    "#     [\"BERT\", accuracy_knn_bert_test, f1_knn_bert_test],\n",
    "#     [\"RoBERTa\", accuracy_knn_roberta_test, f1_knn_roberta_test],\n",
    "#     [\"GPT2\", accuracy_knn_gpt2_test, f1_knn_gpt2_test]\n",
    "# ]\n",
    "#\n",
    "# #define header names\n",
    "# col_names = [\"Model\", \"Accuracy\", \"F1-Score\"]\n",
    "#\n",
    "# #display table\n",
    "# print(\"KNN\")\n",
    "# print(tabulate(knn_results, headers=col_names, tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T00:11:46.045799687Z",
     "start_time": "2023-05-08T00:11:46.031440509Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classifier\n",
      "\n",
      " Model       Accuracy    F1-Score \n",
      "\n",
      " GloVe           56.1        34.4 \n",
      "\n",
      " Word2Vec        59.3        39.2 \n",
      "\n",
      " BERT            50.4        33   \n",
      "\n",
      " RoBERTa         52.9        36   \n",
      "\n",
      " GPT2            55.6        35.7 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #create data\n",
    "# svc_results = [\n",
    "#     [\"GloVe\", accuracy_svc_glove_test, f1_svc_glove_test],\n",
    "#     [\"Word2Vec\", accuracy_svc_w2v_test, f1_svc_w2v_test],\n",
    "#     [\"BERT\", accuracy_svc_bert_test, f1_svc_bert_test],\n",
    "#     [\"RoBERTa\", accuracy_svc_roberta_test, f1_svc_roberta_test],\n",
    "#     [\"GPT2\", accuracy_svc_gpt2_test, f1_svc_gpt2_test]\n",
    "# ]\n",
    "#\n",
    "# #define header names\n",
    "# col_names = [\"Model\", \"Accuracy\", \"F1-Score\"]\n",
    "#\n",
    "# #display table\n",
    "# print(\"SVM Classifier\")\n",
    "# print(tabulate(svc_results, headers=col_names, tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T00:11:46.310402046Z",
     "start_time": "2023-05-08T00:11:46.299016397Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "\n",
      " Model       Accuracy    F1-Score \n",
      "\n",
      " GloVe           61.2        22.9 \n",
      "\n",
      " Word2Vec        62.9        29.9 \n",
      "\n",
      " BERT            57.9        22.4 \n",
      "\n",
      " RoBERTa         61.4        19.7 \n",
      "\n",
      " GPT2            58.2        17.2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #create data\n",
    "# rf_results = [\n",
    "#     [\"GloVe\", accuracy_rf_glove_test, f1_rf_glove_test],\n",
    "#     [\"Word2Vec\", accuracy_rf_w2v_test, f1_rf_w2v_test],\n",
    "#     [\"BERT\", accuracy_rf_bert_test, f1_rf_bert_test],\n",
    "#     [\"RoBERTa\", accuracy_rf_roberta_test, f1_rf_roberta_test],\n",
    "#     [\"GPT2\", accuracy_rf_gpt2_test, f1_rf_gpt2_test]\n",
    "# ]\n",
    "#\n",
    "# #define header names\n",
    "# col_names = [\"Model\", \"Accuracy\", \"F1-Score\"]\n",
    "#\n",
    "# #display table\n",
    "# print(\"Random Forest\")\n",
    "# print(tabulate(rf_results, headers=col_names, tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T00:11:46.663167632Z",
     "start_time": "2023-05-08T00:11:46.636014024Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "\n",
      " Model       Accuracy    F1-Score \n",
      "\n",
      " GloVe           53          36.4 \n",
      "\n",
      " Word2Vec        56.3        38.2 \n",
      "\n",
      " BERT            52.3        34.8 \n",
      "\n",
      " RoBERTa         53.9        36.2 \n",
      "\n",
      " GPT2            52.6        35.9 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #create data\n",
    "# xgb_results = [\n",
    "#     [\"GloVe\", accuracy_xgb_glove_test, f1_xgb_glove_test],\n",
    "#     [\"Word2Vec\", accuracy_xgb_w2v_test, f1_xgb_w2v_test],\n",
    "#     [\"BERT\", accuracy_xgb_bert_test, f1_xgb_bert_test],\n",
    "#     [\"RoBERTa\", accuracy_xgb_roberta_test, f1_xgb_roberta_test],\n",
    "#     [\"GPT2\", accuracy_xgb_gpt2_test, f1_xgb_gpt2_test]\n",
    "# ]\n",
    "#\n",
    "# #define header names\n",
    "# col_names = [\"Model\", \"Accuracy\", \"F1-Score\"]\n",
    "#\n",
    "# #display table\n",
    "# print(\"XGBoost\")\n",
    "# print(tabulate(xgb_results, headers=col_names, tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T00:11:47.147217610Z",
     "start_time": "2023-05-08T00:11:47.088596164Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork\n",
      "\n",
      " Model       Accuracy    F1-Score \n",
      "\n",
      " GloVe           52.5        37.8 \n",
      "\n",
      " Word2Vec        60.1        40   \n",
      "\n",
      " BERT            59.6        17.9 \n",
      "\n",
      " RoBERTa         62.5         9.5 \n",
      "\n",
      " GPT2            56.4        29   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create data\n",
    "nn_results = [\n",
    "    [\"GloVe\", accuracy_nn_glove_test, f1_nn_glove_test],\n",
    "    [\"Word2Vec\", accuracy_nn_w2v_test, f1_nn_w2v_test],\n",
    "    [\"BERT\", accuracy_nn_bert_test, f1_nn_bert_test],\n",
    "    [\"RoBERTa\", accuracy_nn_roberta_test, f1_nn_roberta_test],\n",
    "    [\"GPT2\", accuracy_nn_gpt2_test, f1_nn_gpt2_test]\n",
    "]\n",
    "\n",
    "#define header names\n",
    "col_names = [\"Model\", \"Accuracy\", \"F1-Score\"]\n",
    "\n",
    "#display table\n",
    "print(\"NeuralNetwork\")\n",
    "print(tabulate(nn_results, headers=col_names, tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24t1eSrlFLK6"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
