{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpFId1mjCySe"
   },
   "source": [
    "# Word Embeddings + various classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TCU7NPLUuQrU",
    "ExecuteTime": {
     "end_time": "2023-05-21T18:04:30.804880425Z",
     "start_time": "2023-05-21T18:04:23.361536713Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 20:04:25.298653: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import gensim\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "import time\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "from tabulate import tabulate\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, balanced_accuracy_score, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../data/liar_dataset/train.tsv', sep=\"\\t\", usecols=[1, 2], names = [\"label\", \"text\"])\n",
    "test = pd.read_csv('../../data/liar_dataset/test.tsv', sep=\"\\t\", usecols=[1, 2], names = [\"label\", \"text\"])\n",
    "valid = pd.read_csv('../../data/liar_dataset/valid.tsv', sep=\"\\t\", usecols=[1, 2], names = [\"label\", \"text\"])\n",
    "\n",
    "train = pd.concat([train, valid])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-21T18:04:30.862649279Z",
     "start_time": "2023-05-21T18:04:30.802707624Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "half-true      2362\nfalse          2258\nmostly-true    2213\nbarely-true    1891\ntrue           1845\npants-fire      955\nName: label, dtype: int64"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-21T18:04:31.933002088Z",
     "start_time": "2023-05-21T18:04:31.892012746Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "convert_text_labels = lambda x: 0 if x in ['true', 'mostly-true'] else 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-21T18:04:34.356071222Z",
     "start_time": "2023-05-21T18:04:34.336019526Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train['label'] = train['label'].apply(convert_text_labels)\n",
    "test['label'] = test['label'].apply(convert_text_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-21T18:04:35.055740583Z",
     "start_time": "2023-05-21T18:04:35.004757336Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "1    7466\n0    4058\nName: label, dtype: int64"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-21T18:04:35.746082650Z",
     "start_time": "2023-05-21T18:04:35.711085438Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(train['label'].value_counts().index, train['label'].value_counts().values)\n",
    "\n",
    "plt.xticks(train['label'].value_counts().index)\n",
    "\n",
    "plt.title('Train')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-21T18:04:36.312169920Z",
     "start_time": "2023-05-21T18:04:36.075492959Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "1    818\n0    449\nName: label, dtype: int64"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['label'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-21T18:04:39.088720749Z",
     "start_time": "2023-05-21T18:04:39.062746188Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(test['label'].value_counts().index, test['label'].value_counts().values)\n",
    "\n",
    "plt.xticks(test['label'].value_counts().index)\n",
    "\n",
    "plt.title('Test')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-21T18:04:39.466001984Z",
     "start_time": "2023-05-21T18:04:39.284692714Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "count    11524.000000\nmean       106.895783\nstd         58.415051\nmin         11.000000\n25%         73.000000\n50%         99.000000\n75%        133.000000\nmax       3192.000000\nName: text, dtype: float64"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text'].str.len().describe()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-21T18:04:42.257060032Z",
     "start_time": "2023-05-21T18:04:42.234563132Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "count    1267.000000\nmean      109.578532\nstd        98.031030\nmin        12.000000\n25%        74.000000\n50%        98.000000\n75%       133.000000\nmax      2941.000000\nName: text, dtype: float64"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['text'].str.len().describe()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-21T18:04:42.508084999Z",
     "start_time": "2023-05-21T18:04:42.484494089Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qaFYAFhluSCU",
    "outputId": "cf3c01ae-ba06-4501-d53b-8926d079f411",
    "ExecuteTime": {
     "end_time": "2023-05-21T18:04:42.732735806Z",
     "start_time": "2023-05-21T18:04:42.659577064Z"
    }
   },
   "outputs": [],
   "source": [
    "# # OLD data\n",
    "# fake = pd.read_csv('../../data/Fake.csv')\n",
    "# true = pd.read_csv('../../data/True.csv')\n",
    "#\n",
    "# fake[\"label\"] = 0\n",
    "# true[\"label\"] = 1\n",
    "#\n",
    "# df = pd.concat([fake, true], ignore_index = True)\n",
    "#\n",
    "# df['text'] = df['title'] + \" \" + df['text']\n",
    "# df.drop(columns=['title', 'date', 'subject'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ieOrxB_AuTwA",
    "outputId": "f5f4e1d6-b001-4dab-b81f-7b08125dcdfc",
    "ExecuteTime": {
     "end_time": "2023-05-21T18:04:44.636472631Z",
     "start_time": "2023-05-21T18:04:43.539625897Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/szymon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 868 ms, sys: 6.27 ms, total: 875 ms\n",
      "Wall time: 1.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)\n",
    "\n",
    "\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    text = re.sub('\\[[^]]*\\]', '', text)\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop:\n",
    "            final_text.append(i.strip())\n",
    "    return \" \".join(final_text)\n",
    "    \n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "\n",
    "train['text']=train['text'].apply(denoise_text)\n",
    "test['text']=test['text'].apply(denoise_text)\n",
    "\n",
    "train.to_csv(\"../../data/train.csv\", index=False)\n",
    "test.to_csv(\"../../data/test.csv\", index=False)\n",
    "\n",
    "\n",
    "X_train = train['text'].tolist()\n",
    "y_train = train['label'].tolist()\n",
    "with open(\"../../data/X_train\", \"wb\") as fp:\n",
    "    pickle.dump(X_train, fp)\n",
    "with open(\"../../data/y_train\", \"wb\") as fp:\n",
    "    pickle.dump(y_train, fp)\n",
    "\n",
    "X_test = test['text'].tolist()\n",
    "y_test = test['label'].tolist()\n",
    "with open(\"../../data/X_test\", \"wb\") as fp:\n",
    "    pickle.dump(X_test, fp)\n",
    "with open(\"../../data/y_test\", \"wb\") as fp:\n",
    "    pickle.dump(y_test, fp)\n",
    "\n",
    "\n",
    "train_small = train.sample(frac=1).reset_index(drop=True)[:1000]\n",
    "train_small.to_csv(\"../../data/train_small.csv\", index=False)\n",
    "\n",
    "X_train_small = train_small['text'].tolist()\n",
    "y_train_small = train_small['label'].tolist()\n",
    "with open(\"../../data/small/X_train\", \"wb\") as fp:\n",
    "    pickle.dump(X_train_small, fp)\n",
    "with open(\"../../data/small/y_train\", \"wb\") as fp:\n",
    "    pickle.dump(y_train_small, fp)\n",
    "with open(\"../../data/small/X_test\", \"wb\") as fp:\n",
    "    pickle.dump(X_test, fp)\n",
    "with open(\"../../data/small/y_test\", \"wb\") as fp:\n",
    "    pickle.dump(y_test, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQMXCPQ97YRZ"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIG7v_fk7jbE"
   },
   "source": [
    "Reduce dataset for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "3OAKnGLyuVS0",
    "ExecuteTime": {
     "end_time": "2023-05-15T23:34:50.461705265Z",
     "start_time": "2023-05-15T23:34:50.448253114Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_original = df.copy()\n",
    "# df = df.sample(frac=1).reset_index(drop=True)[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oA_tL8Sg7VWO"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-lrFVWwGiwt"
   },
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:04:48.388539746Z",
     "start_time": "2023-05-21T18:04:48.308663595Z"
    }
   },
   "outputs": [],
   "source": [
    "redo_embedding = False # recalculate embeddings\n",
    "fast = True # True if use reduced dataset (1000 obs) vs. False if full dataset (40000 obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:04:54.097801242Z",
     "start_time": "2023-05-21T18:04:54.014147963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 2.85 ms, total: 2.85 ms\n",
      "Wall time: 2.33 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load data\n",
    "\n",
    "# OLD approach ------------------------\n",
    "# if fast:\n",
    "#     df_original = df.copy()\n",
    "#     df = df.sample(frac=1).reset_index(drop=True)[:1000]\n",
    "#\n",
    "# if redo_embedding:\n",
    "#     X = df['text'].tolist()\n",
    "#     y = df['label'].tolist()\n",
    "#\n",
    "#     with open(\"X\", \"wb\") as fp:\n",
    "#       pickle.dump(X, fp)\n",
    "#     with open(\"y\", \"wb\") as fp:\n",
    "#       pickle.dump(y, fp)\n",
    "# elif fast:\n",
    "#     with open(\"../../data/small/X\", \"rb\") as fp:\n",
    "#       X = pickle.load(fp)\n",
    "#     with open(\"../../data/small/y\", \"rb\") as fp:\n",
    "#       y = pickle.load(fp)\n",
    "# else:\n",
    "#     with open(\"../../data/X\", \"rb\") as fp:\n",
    "#       X = pickle.load(fp)\n",
    "#     with open(\"../../data/y\", \"rb\") as fp:\n",
    "#       y = pickle.load(fp)\n",
    "#\n",
    "# if redo_embedding:\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "#     with open(\"X_train\", \"wb\") as fp:\n",
    "#       pickle.dump(X_train, fp)\n",
    "#     with open(\"X_test\", \"wb\") as fp:\n",
    "#       pickle.dump(X_test, fp)\n",
    "#     with open(\"y_train\", \"wb\") as fp:\n",
    "#       pickle.dump(y_train, fp)\n",
    "#     with open(\"y_test\", \"wb\") as fp:\n",
    "#       pickle.dump(y_test, fp)\n",
    "# elif fast:\n",
    "#     with open(\"../../data/small/X_train\", \"rb\") as fp:\n",
    "#       X_train = pickle.load(fp)\n",
    "#     with open(\"../../data/small/X_test\", \"rb\") as fp:\n",
    "#       X_test = pickle.load(fp)\n",
    "#     with open(\"../../data/small/y_train\", \"rb\") as fp:\n",
    "#       y_train = pickle.load(fp)\n",
    "#     with open(\"../../data/small/y_test\", \"rb\") as fp:\n",
    "#       y_test = pickle.load(fp)\n",
    "# else:\n",
    "#     with open(\"../../data/X_train\", \"rb\") as fp:\n",
    "#       X_train = pickle.load(fp)\n",
    "#     with open(\"../../data/X_test\", \"rb\") as fp:\n",
    "#       X_test = pickle.load(fp)\n",
    "#     with open(\"../../data/y_train\", \"rb\") as fp:\n",
    "#       y_train = pickle.load(fp)\n",
    "#     with open(\"../../data/y_test\", \"rb\") as fp:\n",
    "#       y_test = pickle.load(fp)\n",
    "\n",
    "\n",
    "# NEW approach ------------------------\n",
    "if fast:\n",
    "    with open(\"../../data/small/X_train\", \"rb\") as fp:\n",
    "      X_train = pickle.load(fp)\n",
    "    with open(\"../../data/small/X_test\", \"rb\") as fp:\n",
    "      X_test = pickle.load(fp)\n",
    "    with open(\"../../data/small/y_train\", \"rb\") as fp:\n",
    "      y_train = pickle.load(fp)\n",
    "    with open(\"../../data/small/y_test\", \"rb\") as fp:\n",
    "      y_test = pickle.load(fp)\n",
    "else:\n",
    "    with open(\"../../data/X_train\", \"rb\") as fp:\n",
    "      X_train = pickle.load(fp)\n",
    "    with open(\"../../data/X_test\", \"rb\") as fp:\n",
    "      X_test = pickle.load(fp)\n",
    "    with open(\"../../data/y_train\", \"rb\") as fp:\n",
    "      y_train = pickle.load(fp)\n",
    "    with open(\"../../data/y_test\", \"rb\") as fp:\n",
    "      y_test = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqQEmeGREcUg"
   },
   "source": [
    "## BERT Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pbSRaebguY2a",
    "outputId": "a3fc1906-e2a4-4a92-8d1c-a656f2fdd828",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-05-21T18:05:08.290366743Z",
     "start_time": "2023-05-21T18:05:07.977706368Z"
    }
   },
   "outputs": [],
   "source": [
    "if redo_embedding:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    bert = AutoModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    bert.to(device)\n",
    "\n",
    "    def _get_bert_embedding(text):\n",
    "        input_ids = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=512)\n",
    "        input_ids = np.array(input_ids)\n",
    "        input_ids = np.expand_dims(input_ids, axis=0)\n",
    "        input_ids = torch.tensor(input_ids).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = bert(input_ids)\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            last_hidden_state = last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "        return last_hidden_state\n",
    "\n",
    "    print(\"TRAIN\")\n",
    "    X_train_embeddings = []\n",
    "    for text in tqdm(X_train):\n",
    "        embedding = _get_bert_embedding(text)\n",
    "        X_train_embeddings.append(embedding)\n",
    "    X_train_embeddings = np.array(X_train_embeddings)\n",
    "    X_train_embeddings_bert = np.squeeze(X_train_embeddings, axis=1)\n",
    "\n",
    "    print(\"TEST\")\n",
    "    X_test_embeddings = []\n",
    "    for text in tqdm(X_test):\n",
    "        embedding = _get_bert_embedding(text)\n",
    "        X_test_embeddings.append(embedding)\n",
    "    X_test_embeddings = np.array(X_test_embeddings)\n",
    "    X_test_embeddings_bert = np.squeeze(X_test_embeddings, axis=1)\n",
    "    \n",
    "    if fast:\n",
    "        pd.DataFrame(X_train_embeddings_bert).to_csv(\"../../data/small/embeddings/X_train_embeddings_bert_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "        pd.DataFrame(X_test_embeddings_bert).to_csv(\"../../data/small/embeddings/X_test_embeddings_bert_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "    else:\n",
    "        pd.DataFrame(X_train_embeddings_bert).to_csv(\"../../data/embeddings/X_train_embeddings_bert_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "        pd.DataFrame(X_test_embeddings_bert).to_csv(\"../../data/embeddings/X_test_embeddings_bert_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "    \n",
    "elif fast:\n",
    "    X_train_embeddings_bert = pd.read_csv('../../data/small/embeddings/X_train_embeddings_bert.csv', sep=',', header=None).values\n",
    "    X_test_embeddings_bert = pd.read_csv('../../data/small/embeddings/X_test_embeddings_bert.csv', sep=',', header=None).values\n",
    "else:\n",
    "    X_train_embeddings_bert = pd.read_csv('../../data/embeddings/X_train_embeddings_bert.csv', sep=',', header=None).values\n",
    "    X_test_embeddings_bert = pd.read_csv('../../data/embeddings/X_test_embeddings_bert.csv', sep=',', header=None).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhGA1YV7GaKD"
   },
   "source": [
    "## GloVe Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:05:08.811252051Z",
     "start_time": "2023-05-21T18:05:08.645898992Z"
    }
   },
   "outputs": [],
   "source": [
    "if redo_embedding:\n",
    "\n",
    "    def load_glove_embeddings(filename):\n",
    "        embeddings_index = {}\n",
    "        with open(filename) as f:\n",
    "            for line in tqdm(f):\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                if len(values[1:]) == 300:\n",
    "                    coefs = np.asarray(values[1:], dtype='float32')\n",
    "                    embeddings_index[word] = coefs\n",
    "        return embeddings_index\n",
    "\n",
    "    glove_embeddings = load_glove_embeddings('../../glove/glove.840B.300d.txt')\n",
    "\n",
    "    def text_to_glove_embeddings(text, embeddings_index, embedding_dim):\n",
    "        embeddings = []\n",
    "        for sentence in text:\n",
    "            sentence_embeddings = []\n",
    "            for word in sentence.split():\n",
    "                if word in embeddings_index:\n",
    "                    sentence_embeddings.append(embeddings_index[word])\n",
    "            if len(sentence_embeddings) > 0:\n",
    "                embeddings.append(np.mean(sentence_embeddings, axis=0))\n",
    "            else:\n",
    "                embeddings.append(np.zeros(embedding_dim))\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    X_train_embeddings_glove = text_to_glove_embeddings(X_train, glove_embeddings, embedding_dim=300)\n",
    "    X_test_embeddings_glove = text_to_glove_embeddings(X_test, glove_embeddings, embedding_dim=300)\n",
    "    \n",
    "    if fast:\n",
    "        pd.DataFrame(X_train_embeddings_glove).to_csv(\"../../data/small/embeddings/X_train_embeddings_glove_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "        pd.DataFrame(X_test_embeddings_glove).to_csv(\"../../data/small/embeddings/X_test_embeddings_glove_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "    else:\n",
    "        pd.DataFrame(X_train_embeddings_glove).to_csv(\"../../data/embeddings/X_train_embeddings_glove_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "        pd.DataFrame(X_test_embeddings_glove).to_csv(\"../../data/embeddings/X_test_embeddings_glove_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "\n",
    "elif fast:\n",
    "    X_train_embeddings_glove = pd.read_csv('../../data/small/embeddings/X_train_embeddings_glove.csv', sep=',', header=None).values\n",
    "    X_test_embeddings_glove = pd.read_csv('../../data/small/embeddings/X_test_embeddings_glove.csv', sep=',', header=None).values\n",
    "\n",
    "else:\n",
    "    X_train_embeddings_glove = pd.read_csv('../../data/embeddings/X_train_embeddings_glove.csv', sep=',', header=None).values\n",
    "    X_test_embeddings_glove = pd.read_csv('../../data/embeddings/X_test_embeddings_glove.csv', sep=',', header=None).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "if redo_embedding:\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format('../../word2vec/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "def get_word2vec_embeddings(text):\n",
    "    embeddings = []\n",
    "    for sentence in tqdm(text):\n",
    "        tokens = sentence.split()\n",
    "        doc_vecs = [model[token] for token in tokens if token in model.key_to_index]\n",
    "        if len(doc_vecs) > 0:\n",
    "            doc_vec = np.mean(doc_vecs, axis=0)\n",
    "            embeddings.append(doc_vec)\n",
    "        else:\n",
    "            embeddings.append([0] * 300) # if vocabulary does not exist in Word2Vec append a vector of zeros\n",
    "    return np.array(embeddings)\n",
    "\n",
    "\n",
    "if redo_embedding:\n",
    "    # noinspection PyUnboundLocalVariable\n",
    "    X_train_embeddings_word2vec = get_word2vec_embeddings(X_train)\n",
    "    X_test_embeddings_word2vec = get_word2vec_embeddings(X_test)\n",
    "\n",
    "    if fast:\n",
    "        pd.DataFrame(X_train_embeddings_word2vec).to_csv(\"../../data/small/embeddings/X_train_embeddings_word2vec_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "        pd.DataFrame(X_test_embeddings_word2vec).to_csv(\"../../data/small/embeddings/X_test_embeddings_word2vec_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "    else:\n",
    "        pd.DataFrame(X_train_embeddings_word2vec).to_csv(\"../../data/embeddings/X_train_embeddings_word2vec_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "        pd.DataFrame(X_test_embeddings_word2vec).to_csv(\"../../data/embeddings/X_test_embeddings_word2vec_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "\n",
    "elif fast:\n",
    "    X_train_embeddings_word2vec = pd.read_csv('../../data/small/embeddings/X_train_embeddings_word2vec.csv', sep=',', header=None).values\n",
    "    X_test_embeddings_word2vec = pd.read_csv('../../data/small/embeddings/X_test_embeddings_word2vec.csv', sep=',', header=None).values\n",
    "\n",
    "else:\n",
    "    X_train_embeddings_word2vec = pd.read_csv('../../data/embeddings/X_train_embeddings_word2vec.csv', sep=',', header=None).values\n",
    "    X_test_embeddings_word2vec = pd.read_csv('../../data/embeddings/X_test_embeddings_word2vec.csv', sep=',', header=None).values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-21T18:05:09.690006240Z",
     "start_time": "2023-05-21T18:05:09.528408841Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GPT2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "if redo_embedding:\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\", output_hidden_states=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    gpt2.to(device)\n",
    "\n",
    "    def _get_gpt2_embedding(text):\n",
    "        input_ids = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=1024)\n",
    "        input_ids = torch.tensor(input_ids).unsqueeze(0).to(device).long()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = gpt2.transformer.wte(input_ids)\n",
    "            mean_embedding = embeddings.mean(dim=1)\n",
    "        #     outputs = bert(input_ids)\n",
    "        #     last_hidden_state = outputs.last_hidden_state\n",
    "        #     last_hidden_state = last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "            #vector = gpt2.transformer.wte.weight[input_ids,:]\n",
    "        mean_embedding = mean_embedding.cpu().numpy()\n",
    "        return mean_embedding\n",
    "\n",
    "    print(\"TRAIN\")\n",
    "    X_train_embeddings = []\n",
    "    for text in tqdm(X_train):\n",
    "        embedding = _get_gpt2_embedding(text)\n",
    "        X_train_embeddings.append(embedding)\n",
    "    X_train_embeddings = np.array(X_train_embeddings)\n",
    "    X_train_embeddings_gpt2 = np.squeeze(X_train_embeddings, axis=1)\n",
    "\n",
    "    print(\"TEST\")\n",
    "    X_test_embeddings = []\n",
    "    for text in tqdm(X_test):\n",
    "        embedding = _get_gpt2_embedding(text)\n",
    "        X_test_embeddings.append(embedding)\n",
    "    X_test_embeddings = np.array(X_test_embeddings)\n",
    "    X_test_embeddings_gpt2 = np.squeeze(X_test_embeddings, axis=1)\n",
    "\n",
    "    if fast:\n",
    "        pd.DataFrame(X_train_embeddings_gpt2).to_csv(\"../../data/small/embeddings/X_train_embeddings_gpt2_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "        pd.DataFrame(X_test_embeddings_gpt2).to_csv(\"../../data/small/embeddings/X_test_embeddings_gpt2_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "    else:\n",
    "        pd.DataFrame(X_train_embeddings_gpt2).to_csv(\"../../data/embeddings/X_train_embeddings_gpt2_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "        pd.DataFrame(X_test_embeddings_gpt2).to_csv(\"../../data/embeddings/X_test_embeddings_gpt2_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "\n",
    "elif fast:\n",
    "    X_train_embeddings_gpt2 = pd.read_csv('../../data/small/embeddings/X_train_embeddings_gpt2.csv', sep=',', header=None).values\n",
    "    X_test_embeddings_gpt2 = pd.read_csv('../../data/small/embeddings/X_test_embeddings_gpt2.csv', sep=',', header=None).values\n",
    "else:\n",
    "    X_train_embeddings_gpt2 = pd.read_csv('../../data/embeddings/X_train_embeddings_gpt2.csv', sep=',', header=None).values\n",
    "    X_test_embeddings_gpt2 = pd.read_csv('../../data/embeddings/X_test_embeddings_gpt2.csv', sep=',', header=None).values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-21T18:05:10.469448118Z",
     "start_time": "2023-05-21T18:05:10.215262321Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RoBERTa"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "if redo_embedding:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "    roberta = AutoModel.from_pretrained(\"roberta-base\", output_hidden_states=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    roberta.to(device)\n",
    "\n",
    "    def _get_roberta_embedding(text):\n",
    "        input_ids = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=512)\n",
    "        input_ids = np.array(input_ids)\n",
    "        input_ids = np.expand_dims(input_ids, axis=0)\n",
    "        input_ids = torch.tensor(input_ids).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = roberta(input_ids)\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            last_hidden_state = last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "        return last_hidden_state\n",
    "\n",
    "    print(\"TRAIN\")\n",
    "    X_train_embeddings = []\n",
    "    for text in tqdm(X_train):\n",
    "        embedding = _get_roberta_embedding(text)\n",
    "        X_train_embeddings.append(embedding)\n",
    "    X_train_embeddings = np.array(X_train_embeddings)\n",
    "    X_train_embeddings_roberta = np.squeeze(X_train_embeddings, axis=1)\n",
    "\n",
    "    print(\"TEST\")\n",
    "    X_test_embeddings = []\n",
    "    for text in tqdm(X_test):\n",
    "        embedding = _get_roberta_embedding(text)\n",
    "        X_test_embeddings.append(embedding)\n",
    "    X_test_embeddings = np.array(X_test_embeddings)\n",
    "    X_test_embeddings_roberta = np.squeeze(X_test_embeddings, axis=1)\n",
    "\n",
    "    if fast:\n",
    "        pd.DataFrame(X_train_embeddings_roberta).to_csv(\"../../data/small/embeddings/X_train_embeddings_roberta_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "        pd.DataFrame(X_test_embeddings_roberta).to_csv(\"../../data/small/embeddings/X_test_embeddings_roberta_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "    else:\n",
    "        pd.DataFrame(X_train_embeddings_roberta).to_csv(\"../../data/embeddings/X_train_embeddings_roberta_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "        pd.DataFrame(X_test_embeddings_roberta).to_csv(\"../../data/embeddings/X_test_embeddings_roberta_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", index=False, header=False)\n",
    "\n",
    "elif fast:\n",
    "    X_train_embeddings_roberta = pd.read_csv('../../data/small/embeddings/X_train_embeddings_roberta.csv', sep=',', header=None).values\n",
    "    X_test_embeddings_roberta = pd.read_csv('../../data/small/embeddings/X_test_embeddings_roberta.csv', sep=',', header=None).values\n",
    "else:\n",
    "    X_train_embeddings_roberta = pd.read_csv('../../data/embeddings/X_train_embeddings_roberta.csv', sep=',', header=None).values\n",
    "    X_test_embeddings_roberta = pd.read_csv('../../data/embeddings/X_test_embeddings_roberta.csv', sep=',', header=None).values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-21T18:05:11.323881974Z",
     "start_time": "2023-05-21T18:05:11.057400065Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMvpJtV-EiIo"
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:05:16.720976073Z",
     "start_time": "2023-05-21T18:05:16.690832646Z"
    }
   },
   "outputs": [],
   "source": [
    "class KNNClassifier:\n",
    "    def __init__(self, n_neighbors=2, weights='uniform', metric='minkowski'):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.weights = weights\n",
    "        self.metric = metric\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model = KNeighborsClassifier(n_neighbors=self.n_neighbors, weights=self.weights, metric=self.metric)\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        if self.model is None:\n",
    "          raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        return y_pred\n",
    "\n",
    "    def randomized_search(self, X_train, y_train, param_distributions, cv=5, n_iter=20):\n",
    "        self.model = KNeighborsClassifier()\n",
    "        random_search = RandomizedSearchCV(self.model, param_distributions=param_distributions, cv=cv, n_iter=n_iter, scoring=\"f1\", verbose=3)\n",
    "        random_search.fit(X_train, y_train)\n",
    "\n",
    "        print('Best parameters:', random_search.best_params_, '\\n')\n",
    "        self.n_neighbors = random_search.best_params_['n_neighbors']\n",
    "        self.weights = random_search.best_params_['weights']\n",
    "        self.metric = random_search.best_params_['metric']\n",
    "\n",
    "        self.model = KNeighborsClassifier(n_neighbors=self.n_neighbors, weights=self.weights, metric=self.metric)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        if self.model is None:\n",
    "              raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "        y_pred = self.predict(X_test)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        accuracy = round(accuracy_score(y_test, y_pred)*100,1)\n",
    "        f1 = round(f1_score(y_test, y_pred)*100,1)\n",
    "        balanced_accuracy = round(balanced_accuracy_score(y_test, y_pred)*100,1)\n",
    "        auc = round(roc_auc_score(y_test, y_pred)*100, 1)\n",
    "\n",
    "        print('\\nConfusion matrix\\n', cm, '\\n')\n",
    "        print('Accuracy:', accuracy, '\\n')\n",
    "        print('F1 Score:', f1, '\\n')\n",
    "        print('Balanced accuracy:', f1, '\\n')\n",
    "        print('AUC Score:', auc, '\\n')\n",
    "\n",
    "        return cm, accuracy, f1, balanced_accuracy, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:05:17.104409136Z",
     "start_time": "2023-05-21T18:05:17.087125770Z"
    }
   },
   "outputs": [],
   "source": [
    "class XGBoostClassifier:\n",
    "    def __init__(self, learning_rate=0.1, max_depth=5, min_child_weight=1, subsample=0.5, colsample_bytree=0.5, n_estimators=100, objective='req:squarederror'):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.subsample = subsample\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.n_estimators = n_estimators\n",
    "        self.objective = objective\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model = xgb.XGBClassifier(learning_rate=self.learning_rate, max_depth=self.max_depth, min_child_weight=self.min_child_weight, subsample=self.subsample, colsample_bytree=self.colsample_bytree, n_estimators=self.n_estimators, objective=self.objective)\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        if self.model is None:\n",
    "          raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "        y_pred = self.model.predict(X_test)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def randomized_search(self, X_train, y_train, param_distributions, cv=5, n_iter=20):\n",
    "        self.model = xgb.XGBClassifier()\n",
    "        random_search = RandomizedSearchCV(self.model, param_distributions=param_distributions, cv=cv, n_iter=n_iter, scoring=\"f1\", verbose=3)\n",
    "        random_search.fit(X_train, y_train)\n",
    "\n",
    "        print('Best parameters:', random_search.best_params_, '\\n')\n",
    "        self.learning_rate = random_search.best_params_['learning_rate']\n",
    "        self.max_depth = random_search.best_params_['max_depth']\n",
    "        self.min_child_weight = random_search.best_params_['min_child_weight']\n",
    "        self.subsample = random_search.best_params_['subsample']\n",
    "        self.colsample_bytree = random_search.best_params_['colsample_bytree']\n",
    "        self.n_estimators = random_search.best_params_['n_estimators']\n",
    "        self.objective = random_search.best_params_['objective']\n",
    "\n",
    "        self.model = xgb.XGBClassifier(learning_rate=self.learning_rate, max_depth=self.max_depth, min_child_weight=self.min_child_weight, subsample=self.subsample, colsample_bytree=self.colsample_bytree, n_estimators=self.n_estimators, objective=self.objective)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        if self.model is None:\n",
    "              raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "        y_pred = self.predict(X_test)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        accuracy = round(accuracy_score(y_test, y_pred)*100,1)\n",
    "        f1 = round(f1_score(y_test, y_pred)*100,1)\n",
    "        balanced_accuracy = round(balanced_accuracy_score(y_test, y_pred)*100,1)\n",
    "        auc = round(roc_auc_score(y_test, y_pred)*100, 1)\n",
    "\n",
    "        print('\\nConfusion matrix\\n', cm, '\\n')\n",
    "        print('Accuracy:', accuracy, '\\n')\n",
    "        print('F1 Score:', f1, '\\n')\n",
    "        print('Balanced accuracy:', f1, '\\n')\n",
    "        print('AUC Score:', auc, '\\n')\n",
    "\n",
    "        return cm, accuracy, f1, balanced_accuracy, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:05:17.890694996Z",
     "start_time": "2023-05-21T18:05:17.815589158Z"
    }
   },
   "outputs": [],
   "source": [
    "class RFClassifier:\n",
    "    def __init__(self, n_estimators=100, max_features='sqrt', max_depth='none', bootstrap=True):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.max_depth = max_depth\n",
    "        self.bootstrap = bootstrap\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model = RandomForestClassifier(n_estimators=self.n_estimators, max_features=self.max_features, max_depth=self.max_depth, bootstrap=self.bootstrap, verbose=True)\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        if self.model is None:\n",
    "          raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "        y_pred = self.model.predict(X_test)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def randomized_search(self, X_train, y_train, param_distributions, cv=5, n_iter=20):\n",
    "        self.model = RandomForestClassifier()\n",
    "        random_search = RandomizedSearchCV(self.model, param_distributions=param_distributions, cv=cv, n_iter=n_iter, scoring=\"f1\", verbose=3)\n",
    "        random_search.fit(X_train, y_train)\n",
    "\n",
    "        print('Best parameters:', random_search.best_params_, '\\n')\n",
    "        self.n_estimators = random_search.best_params_['n_estimators']\n",
    "        self.max_features = random_search.best_params_['max_features']\n",
    "        self.max_depth = random_search.best_params_['max_depth']\n",
    "        self.bootstrap = random_search.best_params_['bootstrap']\n",
    "\n",
    "        self.model = RandomForestClassifier(n_estimators=self.n_estimators, max_features=self.max_features, max_depth=self.max_depth, bootstrap=self.bootstrap)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        if self.model is None:\n",
    "              raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "        y_pred = self.predict(X_test)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        accuracy = round(accuracy_score(y_test, y_pred)*100,1)\n",
    "        f1 = round(f1_score(y_test, y_pred)*100,1)\n",
    "        balanced_accuracy = round(balanced_accuracy_score(y_test, y_pred)*100,1)\n",
    "        auc = round(roc_auc_score(y_test, y_pred)*100, 1)\n",
    "\n",
    "        print('\\nConfusion matrix\\n', cm, '\\n')\n",
    "        print('Accuracy:', accuracy, '\\n')\n",
    "        print('F1 Score:', f1, '\\n')\n",
    "        print('Balanced accuracy:', f1, '\\n')\n",
    "        print('AUC Score:', auc, '\\n')\n",
    "\n",
    "        return cm, accuracy, f1, balanced_accuracy, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:05:18.557517295Z",
     "start_time": "2023-05-21T18:05:18.477738622Z"
    }
   },
   "outputs": [],
   "source": [
    "class SVClassifier:\n",
    "    def __init__(self, C = 1, kernel='linear', gamma = 0.2):\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.gamma = gamma\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model = svm.SVC(C=self.C, kernel=self.kernel, gamma=self.gamma, verbose=True)\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        if self.model is None:\n",
    "          raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "        y_pred = self.model.predict(X_test)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def randomized_search(self, X_train, y_train, param_distributions, cv=5, n_iter=20):\n",
    "        self.model = svm.SVC()\n",
    "        random_search = RandomizedSearchCV(self.model, param_distributions=param_distributions, cv=cv, n_iter=n_iter, scoring=\"f1\", verbose=3)\n",
    "        random_search.fit(X_train, y_train)\n",
    "\n",
    "        print('Best parameters:', random_search.best_params_, '\\n')\n",
    "        self.C = random_search.best_params_['C']\n",
    "        self.kernel = random_search.best_params_['kernel']\n",
    "        self.gamma = random_search.best_params_['gamma']\n",
    "\n",
    "        self.model = svm.SVC(C=self.C, kernel=self.kernel, gamma=self.gamma)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        if self.model is None:\n",
    "              raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "        y_pred = self.predict(X_test)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        accuracy = round(accuracy_score(y_test, y_pred)*100,1)\n",
    "        f1 = round(f1_score(y_test, y_pred)*100,1)\n",
    "        balanced_accuracy = round(balanced_accuracy_score(y_test, y_pred)*100,1)\n",
    "        auc = round(roc_auc_score(y_test, y_pred)*100, 1)\n",
    "\n",
    "        print('\\nConfusion matrix\\n', cm, '\\n')\n",
    "        print('Accuracy:', accuracy, '\\n')\n",
    "        print('F1 Score:', f1, '\\n')\n",
    "        print('Balanced accuracy:', f1, '\\n')\n",
    "        print('AUC Score:', auc, '\\n')\n",
    "\n",
    "        return cm, accuracy, f1, balanced_accuracy, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:05:19.341597108Z",
     "start_time": "2023-05-21T18:05:19.263656336Z"
    }
   },
   "outputs": [],
   "source": [
    "class LRClassifier:\n",
    "    def __init__(self, penalty = 'l2', solver = 'libinear', C = 0.5):\n",
    "        self.penalty = penalty\n",
    "        self.solver = solver\n",
    "        self.C = C\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model = LogisticRegression(penalty=self.penalty, solver=self.solver, C=self.C)\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        if self.model is None:\n",
    "          raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "        y_pred = self.model.predict(X_test)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def randomized_search(self, X_train, y_train, param_distributions, cv=5, n_iter=20):\n",
    "        self.model = LogisticRegression()\n",
    "        random_search = RandomizedSearchCV(self.model, param_distributions=param_distributions, cv=cv, n_iter=n_iter, scoring=\"f1\", verbose=3)\n",
    "        random_search.fit(X_train, y_train)\n",
    "\n",
    "        print('Best parameters:', random_search.best_params_, '\\n')\n",
    "        self.penalty = random_search.best_params_['penalty']\n",
    "        self.solver = random_search.best_params_['solver']\n",
    "        self.C = random_search.best_params_['C']\n",
    "\n",
    "        self.model = LogisticRegression(penalty=self.penalty, solver=self.solver, C=self.C)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        if self.model is None:\n",
    "              raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "        y_pred = self.predict(X_test)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        accuracy = round(accuracy_score(y_test, y_pred)*100,1)\n",
    "        f1 = round(f1_score(y_test, y_pred)*100,1)\n",
    "        balanced_accuracy = round(balanced_accuracy_score(y_test, y_pred)*100,1)\n",
    "        auc = round(roc_auc_score(y_test, y_pred)*100, 1)\n",
    "\n",
    "        print('\\nConfusion matrix\\n', cm, '\\n')\n",
    "        print('Accuracy:', accuracy, '\\n')\n",
    "        print('F1 Score:', f1, '\\n')\n",
    "        print('Balanced accuracy:', f1, '\\n')\n",
    "        print('AUC Score:', auc, '\\n')\n",
    "\n",
    "        return cm, accuracy, f1, balanced_accuracy, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "class FakeNewsClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, dropout):\n",
    "        super(FakeNewsClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.relu(self.fc1(x))\n",
    "        hidden = self.dropout(hidden)\n",
    "        output = self.fc2(hidden)\n",
    "        return output\n",
    "\n",
    "class NeuralNetworkClassifier:\n",
    "    def __init__(self, input_dim, hidden_dim, dropout, output_dim=2):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val, patience, num_epochs=10, lr=0.001):\n",
    "        self.model = None\n",
    "\n",
    "        self.model = FakeNewsClassifier(self.input_dim, self.hidden_dim, self.output_dim, self.dropout)\n",
    "        self.model = self.model.float()\n",
    "\n",
    "        class_counts = np.bincount(y_train)\n",
    "        num_classes = len(class_counts)\n",
    "        class_weights = torch.tensor(class_counts.sum() / (num_classes * class_counts), dtype=torch.float)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        early_stop_counter = 0\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        f1_scores = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = 0.0\n",
    "\n",
    "            self.model.train()\n",
    "\n",
    "            for embedding, label in zip(X_train, y_train):\n",
    "                embedding_tensor = torch.from_numpy(embedding).float().unsqueeze(0)\n",
    "                label_tensor = torch.tensor([label])\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(embedding_tensor)\n",
    "                loss = criterion(outputs, label_tensor)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = train_loss / len(X_train)\n",
    "\n",
    "            self.model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0.0\n",
    "\n",
    "                for embedding, label in zip(X_val, y_val):\n",
    "                    embedding_tensor = torch.from_numpy(embedding).float().unsqueeze(0)\n",
    "                    label_tensor = torch.tensor([label])\n",
    "                    outputs = self.model(embedding_tensor)\n",
    "                    loss = criterion(outputs, label_tensor)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                avg_val_loss = val_loss / len(X_val)\n",
    "\n",
    "                train_losses.append(avg_train_loss)\n",
    "                val_losses.append(avg_val_loss)\n",
    "\n",
    "                y_pred = self.predict(X_val)\n",
    "                f1 = f1_score(y_val, y_pred)\n",
    "                f1_scores.append(f1)\n",
    "\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | F1 Score: {f1:.4f} | Balanced Accuracy: {balanced_accuracy_score(y_val, y_pred):.4f} | AUC: {roc_auc_score(y_val, y_pred):.4f}\")\n",
    "\n",
    "                if avg_val_loss < best_loss:\n",
    "                    best_loss = avg_val_loss\n",
    "                    early_stop_counter = 0\n",
    "                else:\n",
    "                    early_stop_counter += 1\n",
    "                    if early_stop_counter >= patience:\n",
    "                        print(f\"Early stopping triggered. No improvement in {patience} epochs.\")\n",
    "                        break\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        epochs = np.arange(1, len(train_losses) + 1)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, train_losses, label='Train Loss')\n",
    "        plt.plot(epochs, val_losses, label='Val Loss')\n",
    "        plt.plot(epochs, f1_scores, label='F1 Score')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')  # Adding the y-axis label\n",
    "        plt.legend()  # Adding the legend\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "        test_inputs = torch.from_numpy(X_test).float()\n",
    "        predictions = self.model(test_inputs)\n",
    "        predicted_classes = torch.argmax(predictions, dim=1)\n",
    "        predicted_classes = predicted_classes.numpy()\n",
    "        return predicted_classes\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "        y_pred = self.predict(X_test)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        accuracy = round(accuracy_score(y_test, y_pred) * 100, 1)\n",
    "        f1 = round(f1_score(y_test, y_pred) * 100, 1)\n",
    "        balanced_accuracy = round(balanced_accuracy_score(y_test, y_pred) * 100, 1)\n",
    "        auc = round(roc_auc_score(y_test, y_pred) * 100, 1)\n",
    "\n",
    "        print('\\nConfusion matrix\\n', cm, '\\n')\n",
    "        print('Accuracy:', accuracy, '\\n')\n",
    "        print('F1 Score:', f1, '\\n')\n",
    "        print('Balanced accuracy:', balanced_accuracy, '\\n')\n",
    "        print('AUC Score:', auc, '\\n')\n",
    "\n",
    "        return cm, accuracy, f1, balanced_accuracy, auc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T00:08:34.790707339Z",
     "start_time": "2023-05-22T00:08:34.715377702Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "OLD NN Wrapper"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# class FakeNewsClassifier(nn.Module):\n",
    "#     def __init__(self, embedding_dim, hidden_dim, output_dim):\n",
    "#         super(FakeNewsClassifier, self).__init__()\n",
    "#         self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "#         self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "#         self.relu = nn.ReLU()\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         hidden = self.relu(self.fc1(x))\n",
    "#         output = self.fc2(hidden)\n",
    "#         return output\n",
    "#\n",
    "# class NeuralNetworkClassifier:\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim=2):\n",
    "#         self.input_dim = input_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.output_dim = output_dim\n",
    "#         self.model = None\n",
    "#\n",
    "#     def fit(self, X_train, y_train, num_epochs=10, lr=0.001):\n",
    "#         self.model = FakeNewsClassifier(self.input_dim, self.hidden_dim, self.output_dim)\n",
    "#         self.model = self.model.double()\n",
    "#\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#         optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "#\n",
    "#         for _ in tqdm(range(num_epochs), desc=\"Epoch\"):\n",
    "#             for embedding, label in zip(X_train, y_train):\n",
    "#                 embedding_tensor = torch.from_numpy(embedding).double().unsqueeze(0)\n",
    "#                 label_tensor = torch.tensor([label])\n",
    "#                 optimizer.zero_grad()\n",
    "#                 outputs = self.model(embedding_tensor)\n",
    "#                 loss = criterion(outputs, label_tensor)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#\n",
    "#     def predict(self, X_test):\n",
    "#         if self.model is None:\n",
    "#             raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "#         test_inputs = torch.from_numpy(X_test).double()\n",
    "#         predictions = self.model(test_inputs)\n",
    "#         predicted_classes = torch.argmax(predictions, dim=1)\n",
    "#         predicted_classes = predicted_classes.numpy()\n",
    "#         return predicted_classes\n",
    "#\n",
    "#     def evaluate(self, X_test, y_test):\n",
    "#         if self.model is None:\n",
    "#             raise ValueError(\"The model has not been trained yet. Please call 'fit' first.\")\n",
    "#         y_pred = self.predict(X_test)\n",
    "#\n",
    "#         cm = confusion_matrix(y_test, y_pred)\n",
    "#         accuracy = round(accuracy_score(y_test, y_pred)*100, 1)\n",
    "#         f1 = round(f1_score(y_test, y_pred)*100, 1)\n",
    "#         balanced_accuracy = round(balanced_accuracy_score(y_test, y_pred)*100,1)\n",
    "#         auc = round(roc_auc_score(y_test, y_pred)*100, 1)\n",
    "#\n",
    "#         print('\\nConfusion matrix\\n', cm, '\\n')\n",
    "#         print('Accuracy:', accuracy, '\\n')\n",
    "#         print('F1 Score:', f1, '\\n')\n",
    "#         print('Balanced accuracy:', f1, '\\n')\n",
    "#         print('AUC Score:', auc, '\\n')\n",
    "#\n",
    "#         return cm, accuracy, f1, balanced_accuracy, auc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-21T18:05:20.764158061Z",
     "start_time": "2023-05-21T18:05:20.741575830Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlaJrgisGNfg"
   },
   "source": [
    "## BERT embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMOZaIWp0_oy"
   },
   "source": [
    "### BERT + KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "7HzI3FV7zy6t",
    "outputId": "8ca1830c-a309-4f27-f6dd-cee648ee0978",
    "ExecuteTime": {
     "end_time": "2023-05-22T00:49:50.509862180Z",
     "start_time": "2023-05-22T00:49:48.474138779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=7, weights=distance;, score=0.726 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=7, weights=distance;, score=0.740 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=7, weights=distance;, score=0.722 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=7, weights=distance;, score=0.705 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=7, weights=distance;, score=0.716 total time=   0.0s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=6, weights=distance;, score=0.727 total time=   0.1s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=6, weights=distance;, score=0.741 total time=   0.1s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=6, weights=distance;, score=0.686 total time=   0.1s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=6, weights=distance;, score=0.690 total time=   0.1s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=6, weights=distance;, score=0.687 total time=   0.1s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.728 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.744 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.715 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.723 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.696 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=7, weights=uniform;, score=0.726 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=7, weights=uniform;, score=0.740 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=7, weights=uniform;, score=0.722 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=7, weights=uniform;, score=0.705 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=7, weights=uniform;, score=0.716 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=5, weights=distance;, score=0.725 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=5, weights=distance;, score=0.740 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=5, weights=distance;, score=0.681 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=5, weights=distance;, score=0.683 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=5, weights=distance;, score=0.715 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=3, weights=distance;, score=0.721 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=3, weights=distance;, score=0.740 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=3, weights=distance;, score=0.664 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=3, weights=distance;, score=0.679 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=3, weights=distance;, score=0.712 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=3, weights=uniform;, score=0.721 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=3, weights=uniform;, score=0.740 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=3, weights=uniform;, score=0.664 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=3, weights=uniform;, score=0.679 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=3, weights=uniform;, score=0.712 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=6, weights=uniform;, score=0.679 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=6, weights=uniform;, score=0.669 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=6, weights=uniform;, score=0.649 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=6, weights=uniform;, score=0.599 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=6, weights=uniform;, score=0.649 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.721 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.740 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.664 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.679 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.712 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=6, weights=distance;, score=0.711 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=6, weights=distance;, score=0.734 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=6, weights=distance;, score=0.695 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=6, weights=distance;, score=0.695 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=6, weights=distance;, score=0.690 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.711 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.734 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.695 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.695 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.690 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=3, weights=distance;, score=0.721 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=3, weights=distance;, score=0.740 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=3, weights=distance;, score=0.664 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=3, weights=distance;, score=0.679 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=3, weights=distance;, score=0.712 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=7, weights=uniform;, score=0.726 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=7, weights=uniform;, score=0.740 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=7, weights=uniform;, score=0.722 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=7, weights=uniform;, score=0.705 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=7, weights=uniform;, score=0.716 total time=   0.0s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.689 total time=   0.1s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.733 total time=   0.1s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.662 total time=   0.1s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.690 total time=   0.1s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.692 total time=   0.1s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=7, weights=distance;, score=0.726 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=7, weights=distance;, score=0.740 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=7, weights=distance;, score=0.722 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=7, weights=distance;, score=0.705 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=7, weights=distance;, score=0.716 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=2, weights=uniform;, score=0.516 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=2, weights=uniform;, score=0.586 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=2, weights=uniform;, score=0.581 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=2, weights=uniform;, score=0.549 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=2, weights=uniform;, score=0.552 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=5, weights=uniform;, score=0.725 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=5, weights=uniform;, score=0.740 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=5, weights=uniform;, score=0.681 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=5, weights=uniform;, score=0.683 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=5, weights=uniform;, score=0.715 total time=   0.0s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.710 total time=   0.1s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.742 total time=   0.1s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.702 total time=   0.1s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.710 total time=   0.1s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.703 total time=   0.1s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=5, weights=uniform;, score=0.725 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=5, weights=uniform;, score=0.740 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=5, weights=uniform;, score=0.681 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=5, weights=uniform;, score=0.683 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=5, weights=uniform;, score=0.715 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.717 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.730 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.687 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.676 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.698 total time=   0.0s\n",
      "Best parameters: {'weights': 'distance', 'n_neighbors': 7, 'metric': 'euclidean'} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[355   0]\n",
      " [  0 645]] \n",
      "\n",
      "Accuracy: 100.0 \n",
      "\n",
      "F1 Score: 100.0 \n",
      "\n",
      "Balanced accuracy: 100.0 \n",
      "\n",
      "AUC Score: 100.0 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[ 74 375]\n",
      " [152 666]] \n",
      "\n",
      "Accuracy: 58.4 \n",
      "\n",
      "F1 Score: 71.7 \n",
      "\n",
      "Balanced accuracy: 71.7 \n",
      "\n",
      "AUC Score: 48.9 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = KNNClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'weights' : ['uniform','distance'],\n",
    "    'metric' : ['minkowski','euclidean','manhattan']\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_bert, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_bert, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_knn_bert_train, accuracy_knn_bert_train, f1_knn_bert_train, balaccuracy_knn_bert_train, rocauc_knn_bert_train = classifier.evaluate(X_train_embeddings_bert, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_knn_bert_test, accuracy_knn_bert_test, f1_knn_bert_test, balaccuracy_knn_bert_test, rocauc_knn_bert_test = classifier.evaluate(X_test_embeddings_bert, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bg4FzG2htmwr"
   },
   "source": [
    "### BERT + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "psJLbuc_win3",
    "outputId": "554936c2-e2c5-4b6f-df80-f9e4054f0dbe",
    "ExecuteTime": {
     "end_time": "2023-05-22T00:51:37.481027768Z",
     "start_time": "2023-05-22T00:50:00.083466545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.632 total time=   2.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.672 total time=   1.0s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.695 total time=   1.9s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.657 total time=   1.0s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.703 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.707 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.633 total time=   1.0s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.679 total time=   1.5s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.677 total time=   2.4s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.662 total time=   2.2s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.777 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.776 total time=   0.5s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.780 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.773 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.776 total time=   0.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=7, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.659 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=7, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.604 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=7, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.647 total time=   1.0s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=7, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.623 total time=   0.9s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=7, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.647 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.748 total time=   2.2s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.738 total time=   2.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.755 total time=   2.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.763 total time=   2.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.777 total time=   2.2s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.684 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.644 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.646 total time=   0.4s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.679 total time=   0.4s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.634 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.758 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.766 total time=   0.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.779 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.773 total time=   0.3s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.753 total time=   0.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.772 total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.747 total time=   1.1s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.784 total time=   1.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.770 total time=   1.1s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.772 total time=   1.2s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.709 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.718 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.696 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.725 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.683 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.746 total time=   0.2s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.652 total time=   0.2s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.725 total time=   0.2s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.701 total time=   0.2s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.700 total time=   0.2s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=9, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.667 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=9, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.659 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=9, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.649 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=9, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.674 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=9, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.687 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.672 total time=   1.8s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.667 total time=   1.9s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.664 total time=   1.7s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.613 total time=   1.8s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.676 total time=   1.7s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.607 total time=   1.1s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.643 total time=   1.1s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.620 total time=   1.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.682 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.609 total time=   1.1s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.740 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.698 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.735 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.647 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.724 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.784 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.770 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.783 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.769 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.785 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.659 total time=   1.5s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.565 total time=   1.5s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.630 total time=   1.5s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.631 total time=   1.5s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.631 total time=   1.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.738 total time=   1.8s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.719 total time=   1.7s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.737 total time=   1.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.739 total time=   1.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.717 total time=   1.8s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.769 total time=   1.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.751 total time=   1.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.755 total time=   1.4s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.756 total time=   1.4s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.759 total time=   1.4s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.638 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.576 total time=   0.3s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.652 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.686 total time=   0.3s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.644 total time=   0.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.780 total time=   0.2s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.780 total time=   0.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.780 total time=   0.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.784 total time=   0.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.780 total time=   0.2s\n",
      "Best parameters: {'subsample': 0.7, 'objective': 'reg:squarederror', 'n_estimators': 50, 'min_child_weight': 3, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.5} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[ 33 322]\n",
      " [  0 645]] \n",
      "\n",
      "Accuracy: 67.8 \n",
      "\n",
      "F1 Score: 80.0 \n",
      "\n",
      "Balanced accuracy: 80.0 \n",
      "\n",
      "AUC Score: 54.6 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[  1 448]\n",
      " [  5 813]] \n",
      "\n",
      "Accuracy: 64.2 \n",
      "\n",
      "F1 Score: 78.2 \n",
      "\n",
      "Balanced accuracy: 78.2 \n",
      "\n",
      "AUC Score: 49.8 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = XGBoostClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'learning_rate': [0.01, 0.1, 0.3, 0.5],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.5, 0.7],\n",
    "    'colsample_bytree': [0.5, 0.7],\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'objective': ['reg:squarederror']\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_bert, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_bert, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_xgb_bert_train, accuracy_xgb_bert_train, f1_xgb_bert_train, balaccuracy_xgb_bert_train, rocauc_xgb_bert_train = classifier.evaluate(X_train_embeddings_bert, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_xgb_bert_test, accuracy_xgb_bert_test, f1_xgb_bert_test, balaccuracy_xgb_bert_test, rocauc_xgb_bert_test = classifier.evaluate(X_test_embeddings_bert, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekftGhJPFFFX"
   },
   "source": [
    "### BERT + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ks6w5IOrJYG",
    "outputId": "0341c3cf-975e-4e0c-8a26-01064b132ae1",
    "ExecuteTime": {
     "end_time": "2023-05-22T00:51:45.712531157Z",
     "start_time": "2023-05-22T00:51:37.481573654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.769 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.772 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.749 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.750 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.721 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.740 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.726 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.715 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.737 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.738 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.770 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.768 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.766 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.765 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.776 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.699 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.713 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.695 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.723 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.760 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.727 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.727 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.764 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.759 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.766 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.748 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.765 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.786 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.767 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.689 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.636 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.649 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.694 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.701 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.746 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.738 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.746 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.766 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.752 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.752 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.709 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.770 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.752 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.741 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.755 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.717 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.792 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.732 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.744 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.773 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.762 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.773 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.751 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.763 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.677 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.654 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.716 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.667 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.708 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.739 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.687 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.745 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.749 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.764 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.642 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.606 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.681 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.734 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.615 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.735 total time=   0.2s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.744 total time=   0.2s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.741 total time=   0.2s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.745 total time=   0.2s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.719 total time=   0.2s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.759 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.732 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.750 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.739 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.735 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.593 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.657 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.706 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.679 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.632 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.657 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.619 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.715 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.700 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.714 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.759 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.726 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.742 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.751 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.760 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.687 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.629 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.654 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.708 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.708 total time=   0.0s\n",
      "Best parameters: {'n_estimators': 25, 'max_features': 5, 'max_depth': 10, 'bootstrap': False} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[345  10]\n",
      " [  0 645]] \n",
      "\n",
      "Accuracy: 99.0 \n",
      "\n",
      "F1 Score: 99.2 \n",
      "\n",
      "Balanced accuracy: 99.2 \n",
      "\n",
      "AUC Score: 98.6 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[ 28 421]\n",
      " [ 26 792]] \n",
      "\n",
      "Accuracy: 64.7 \n",
      "\n",
      "F1 Score: 78.0 \n",
      "\n",
      "Balanced accuracy: 78.0 \n",
      "\n",
      "AUC Score: 51.5 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = RFClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'n_estimators': [10, 25], \n",
    "    'max_features': [5, 10],\n",
    "    'max_depth': [10, 50, None], \n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_bert, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_bert, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_rf_bert_train, accuracy_rf_bert_train, f1_rf_bert_train, balaccuracy_rf_bert_train, rocauc_rf_bert_train = classifier.evaluate(X_train_embeddings_bert, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_rf_bert_test, accuracy_rf_bert_test, f1_rf_bert_test, balaccuracy_rf_bert_test, rocauc_rf_bert_test = classifier.evaluate(X_test_embeddings_bert, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbX9j48J1a9j"
   },
   "source": [
    "### BERT + SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fSTFuYyH63Mk",
    "outputId": "6ba15a33-8e98-4a4e-9113-8c771dc0dede",
    "ExecuteTime": {
     "end_time": "2023-05-22T00:51:59.785062028Z",
     "start_time": "2023-05-22T00:51:45.714415762Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:292: UserWarning: The total space of parameters 16 is smaller than n_iter=20. Running 16 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV 1/5] END ...C=1, gamma=scale, kernel=linear;, score=0.619 total time=   0.2s\n",
      "[CV 2/5] END ...C=1, gamma=scale, kernel=linear;, score=0.584 total time=   0.2s\n",
      "[CV 3/5] END ...C=1, gamma=scale, kernel=linear;, score=0.651 total time=   0.2s\n",
      "[CV 4/5] END ...C=1, gamma=scale, kernel=linear;, score=0.689 total time=   0.2s\n",
      "[CV 5/5] END ...C=1, gamma=scale, kernel=linear;, score=0.664 total time=   0.2s\n",
      "[CV 1/5] END .....C=1, gamma=scale, kernel=poly;, score=0.784 total time=   0.1s\n",
      "[CV 2/5] END .....C=1, gamma=scale, kernel=poly;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END .....C=1, gamma=scale, kernel=poly;, score=0.784 total time=   0.1s\n",
      "[CV 4/5] END .....C=1, gamma=scale, kernel=poly;, score=0.784 total time=   0.1s\n",
      "[CV 5/5] END .....C=1, gamma=scale, kernel=poly;, score=0.784 total time=   0.1s\n",
      "[CV 1/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.784 total time=   0.2s\n",
      "[CV 2/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.784 total time=   0.2s\n",
      "[CV 3/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.784 total time=   0.2s\n",
      "[CV 4/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.784 total time=   0.2s\n",
      "[CV 5/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.784 total time=   0.2s\n",
      "[CV 1/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.784 total time=   0.2s\n",
      "[CV 2/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.784 total time=   0.1s\n",
      "[CV 4/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.784 total time=   0.1s\n",
      "[CV 5/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.784 total time=   0.1s\n",
      "[CV 1/5] END ..C=10, gamma=scale, kernel=linear;, score=0.632 total time=   0.3s\n",
      "[CV 2/5] END ..C=10, gamma=scale, kernel=linear;, score=0.545 total time=   0.2s\n",
      "[CV 3/5] END ..C=10, gamma=scale, kernel=linear;, score=0.612 total time=   0.3s\n",
      "[CV 4/5] END ..C=10, gamma=scale, kernel=linear;, score=0.672 total time=   0.3s\n",
      "[CV 5/5] END ..C=10, gamma=scale, kernel=linear;, score=0.681 total time=   0.2s\n",
      "[CV 1/5] END ....C=10, gamma=scale, kernel=poly;, score=0.654 total time=   0.1s\n",
      "[CV 2/5] END ....C=10, gamma=scale, kernel=poly;, score=0.627 total time=   0.2s\n",
      "[CV 3/5] END ....C=10, gamma=scale, kernel=poly;, score=0.667 total time=   0.1s\n",
      "[CV 4/5] END ....C=10, gamma=scale, kernel=poly;, score=0.679 total time=   0.1s\n",
      "[CV 5/5] END ....C=10, gamma=scale, kernel=poly;, score=0.640 total time=   0.1s\n",
      "[CV 1/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.703 total time=   0.2s\n",
      "[CV 2/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.640 total time=   0.2s\n",
      "[CV 3/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.674 total time=   0.2s\n",
      "[CV 4/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.734 total time=   0.2s\n",
      "[CV 5/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.669 total time=   0.2s\n",
      "[CV 1/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.756 total time=   0.1s\n",
      "[CV 2/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.706 total time=   0.1s\n",
      "[CV 3/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.737 total time=   0.1s\n",
      "[CV 4/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.774 total time=   0.1s\n",
      "[CV 5/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.756 total time=   0.1s\n",
      "[CV 1/5] END .C=100, gamma=scale, kernel=linear;, score=0.632 total time=   0.2s\n",
      "[CV 2/5] END .C=100, gamma=scale, kernel=linear;, score=0.545 total time=   0.2s\n",
      "[CV 3/5] END .C=100, gamma=scale, kernel=linear;, score=0.612 total time=   0.2s\n",
      "[CV 4/5] END .C=100, gamma=scale, kernel=linear;, score=0.672 total time=   0.3s\n",
      "[CV 5/5] END .C=100, gamma=scale, kernel=linear;, score=0.681 total time=   0.2s\n",
      "[CV 1/5] END ...C=100, gamma=scale, kernel=poly;, score=0.615 total time=   0.1s\n",
      "[CV 2/5] END ...C=100, gamma=scale, kernel=poly;, score=0.611 total time=   0.1s\n",
      "[CV 3/5] END ...C=100, gamma=scale, kernel=poly;, score=0.662 total time=   0.1s\n",
      "[CV 4/5] END ...C=100, gamma=scale, kernel=poly;, score=0.652 total time=   0.1s\n",
      "[CV 5/5] END ...C=100, gamma=scale, kernel=poly;, score=0.654 total time=   0.1s\n",
      "[CV 1/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.631 total time=   0.2s\n",
      "[CV 2/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.608 total time=   0.2s\n",
      "[CV 3/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.654 total time=   0.2s\n",
      "[CV 4/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.652 total time=   0.2s\n",
      "[CV 5/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.647 total time=   0.2s\n",
      "[CV 1/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.653 total time=   0.1s\n",
      "[CV 2/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.616 total time=   0.1s\n",
      "[CV 3/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.669 total time=   0.1s\n",
      "[CV 4/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.715 total time=   0.1s\n",
      "[CV 5/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.631 total time=   0.1s\n",
      "[CV 1/5] END C=1000, gamma=scale, kernel=linear;, score=0.632 total time=   0.3s\n",
      "[CV 2/5] END C=1000, gamma=scale, kernel=linear;, score=0.545 total time=   0.2s\n",
      "[CV 3/5] END C=1000, gamma=scale, kernel=linear;, score=0.612 total time=   0.2s\n",
      "[CV 4/5] END C=1000, gamma=scale, kernel=linear;, score=0.672 total time=   0.3s\n",
      "[CV 5/5] END C=1000, gamma=scale, kernel=linear;, score=0.681 total time=   0.3s\n",
      "[CV 1/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.615 total time=   0.1s\n",
      "[CV 2/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.611 total time=   0.1s\n",
      "[CV 3/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.662 total time=   0.1s\n",
      "[CV 4/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.652 total time=   0.1s\n",
      "[CV 5/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.654 total time=   0.1s\n",
      "[CV 1/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.631 total time=   0.2s\n",
      "[CV 2/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.608 total time=   0.2s\n",
      "[CV 3/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.654 total time=   0.2s\n",
      "[CV 4/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.652 total time=   0.2s\n",
      "[CV 5/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.647 total time=   0.2s\n",
      "[CV 1/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.608 total time=   0.1s\n",
      "[CV 2/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.591 total time=   0.1s\n",
      "[CV 3/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.654 total time=   0.1s\n",
      "[CV 4/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.689 total time=   0.1s\n",
      "[CV 5/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.615 total time=   0.1s\n",
      "Best parameters: {'kernel': 'poly', 'gamma': 'scale', 'C': 1} \n",
      "\n",
      "[LibSVM].*\n",
      "optimization finished, #iter = 1581\n",
      "obj = -646.370537, rho = 0.766021\n",
      "nSV = 877, nBSV = 527\n",
      "Total nSV = 877\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[  4 351]\n",
      " [  0 645]] \n",
      "\n",
      "Accuracy: 64.9 \n",
      "\n",
      "F1 Score: 78.6 \n",
      "\n",
      "Balanced accuracy: 78.6 \n",
      "\n",
      "AUC Score: 50.6 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[  0 449]\n",
      " [  0 818]] \n",
      "\n",
      "Accuracy: 64.6 \n",
      "\n",
      "F1 Score: 78.5 \n",
      "\n",
      "Balanced accuracy: 78.5 \n",
      "\n",
      "AUC Score: 50.0 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = SVClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'C': [1, 10, 100, 1000],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale']\n",
    "    #'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_bert, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_bert, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_svc_bert_train, accuracy_svc_bert_train, f1_svc_bert_train, balaccuracy_svc_bert_train, rocauc_svc_bert_train = classifier.evaluate(X_train_embeddings_bert, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_svc_bert_test, accuracy_svc_bert_test, f1_svc_bert_test, balaccuracy_svc_bert_test, rocauc_svc_bert_test = classifier.evaluate(X_test_embeddings_bert, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYoz0OB11ayT"
   },
   "source": [
    "### BERT + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YijN0v7JCFbG",
    "outputId": "4bf95058-f264-43c2-e84d-1572b4814428",
    "ExecuteTime": {
     "end_time": "2023-05-22T00:52:12.423855070Z",
     "start_time": "2023-05-22T00:51:59.790075409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END C=0.64, penalty=l2, solver=newton-cg;, score=0.644 total time=   0.2s\n",
      "[CV 2/5] END C=0.64, penalty=l2, solver=newton-cg;, score=0.571 total time=   0.3s\n",
      "[CV 3/5] END C=0.64, penalty=l2, solver=newton-cg;, score=0.652 total time=   0.3s\n",
      "[CV 4/5] END C=0.64, penalty=l2, solver=newton-cg;, score=0.721 total time=   0.2s\n",
      "[CV 5/5] END C=0.64, penalty=l2, solver=newton-cg;, score=0.664 total time=   0.3s\n",
      "[CV 1/5] END C=0.52, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.52, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.52, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.52, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.52, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.98, penalty=none, solver=lbfgs;, score=0.622 total time=   0.0s\n",
      "[CV 2/5] END C=0.98, penalty=none, solver=lbfgs;, score=0.540 total time=   0.1s\n",
      "[CV 3/5] END C=0.98, penalty=none, solver=lbfgs;, score=0.630 total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=0.98, penalty=none, solver=lbfgs;, score=0.672 total time=   0.1s\n",
      "[CV 5/5] END C=0.98, penalty=none, solver=lbfgs;, score=0.636 total time=   0.1s\n",
      "[CV 1/5] END ..C=0.59, penalty=l2, solver=lbfgs;, score=0.647 total time=   0.0s\n",
      "[CV 2/5] END ..C=0.59, penalty=l2, solver=lbfgs;, score=0.573 total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END ..C=0.59, penalty=l2, solver=lbfgs;, score=0.657 total time=   0.1s\n",
      "[CV 4/5] END ..C=0.59, penalty=l2, solver=lbfgs;, score=0.721 total time=   0.1s\n",
      "[CV 5/5] END ..C=0.59, penalty=l2, solver=lbfgs;, score=0.664 total time=   0.1s\n",
      "[CV 1/5] END C=0.89, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.89, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.89, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.89, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.89, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.2, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.2, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.2, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.2, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.2, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.07, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.07, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.07, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.07, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.07, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END .C=0.45, penalty=none, solver=saga;, score=0.641 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END .C=0.45, penalty=none, solver=saga;, score=0.557 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END .C=0.45, penalty=none, solver=saga;, score=0.631 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END .C=0.45, penalty=none, solver=saga;, score=0.712 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END .C=0.45, penalty=none, solver=saga;, score=0.642 total time=   0.5s\n",
      "[CV 1/5] END C=0.1, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.1, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.1, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.1, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.1, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ...C=0.37, penalty=l2, solver=saga;, score=0.672 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END ...C=0.37, penalty=l2, solver=saga;, score=0.589 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END ...C=0.37, penalty=l2, solver=saga;, score=0.662 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END ...C=0.37, penalty=l2, solver=saga;, score=0.728 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END ...C=0.37, penalty=l2, solver=saga;, score=0.679 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ...C=0.93, penalty=l2, solver=saga;, score=0.647 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END ...C=0.93, penalty=l2, solver=saga;, score=0.564 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END ...C=0.93, penalty=l2, solver=saga;, score=0.652 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END ...C=0.93, penalty=l2, solver=saga;, score=0.721 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END ...C=0.93, penalty=l2, solver=saga;, score=0.662 total time=   0.5s\n",
      "[CV 1/5] END C=0.84, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.84, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.84, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.84, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.84, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.07, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.07, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.07, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.07, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.07, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=0.8200000000000001, penalty=none, solver=sag;, score=0.628 total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=0.8200000000000001, penalty=none, solver=sag;, score=0.520 total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=0.8200000000000001, penalty=none, solver=sag;, score=0.638 total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=0.8200000000000001, penalty=none, solver=sag;, score=0.696 total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=0.8200000000000001, penalty=none, solver=sag;, score=0.644 total time=   0.4s\n",
      "[CV 1/5] END ..C=0.46, penalty=l2, solver=lbfgs;, score=0.654 total time=   0.1s\n",
      "[CV 2/5] END ..C=0.46, penalty=l2, solver=lbfgs;, score=0.594 total time=   0.1s\n",
      "[CV 3/5] END ..C=0.46, penalty=l2, solver=lbfgs;, score=0.659 total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END ..C=0.46, penalty=l2, solver=lbfgs;, score=0.721 total time=   0.1s\n",
      "[CV 5/5] END ..C=0.46, penalty=l2, solver=lbfgs;, score=0.669 total time=   0.1s\n",
      "[CV 1/5] END C=0.76, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.76, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.76, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.76, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.76, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.14, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.14, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.14, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.14, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.14, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.13, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.13, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.13, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.13, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.13, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.31, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.31, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.31, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.31, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.31, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ..C=0.44, penalty=l2, solver=lbfgs;, score=0.657 total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "55 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1471, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.65043227        nan 0.6200009  0.65219201        nan        nan\n",
      "        nan 0.63653271        nan 0.66569268 0.64896824        nan\n",
      "        nan 0.62539509 0.65948642        nan        nan        nan\n",
      "        nan 0.6609862 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END ..C=0.44, penalty=l2, solver=lbfgs;, score=0.594 total time=   0.1s\n",
      "[CV 3/5] END ..C=0.44, penalty=l2, solver=lbfgs;, score=0.664 total time=   0.1s\n",
      "[CV 4/5] END ..C=0.44, penalty=l2, solver=lbfgs;, score=0.721 total time=   0.1s\n",
      "[CV 5/5] END ..C=0.44, penalty=l2, solver=lbfgs;, score=0.669 total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'solver': 'saga', 'penalty': 'l2', 'C': 0.37} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[234 121]\n",
      " [ 32 613]] \n",
      "\n",
      "Accuracy: 84.7 \n",
      "\n",
      "F1 Score: 88.9 \n",
      "\n",
      "Balanced accuracy: 88.9 \n",
      "\n",
      "AUC Score: 80.5 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[116 333]\n",
      " [191 627]] \n",
      "\n",
      "Accuracy: 58.6 \n",
      "\n",
      "F1 Score: 70.5 \n",
      "\n",
      "Balanced accuracy: 70.5 \n",
      "\n",
      "AUC Score: 51.2 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = LRClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'C' : np.arange(0, 1, 0.01)\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_bert, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_bert, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_lr_bert_train, accuracy_lr_bert_train, f1_lr_bert_train, balaccuracy_lr_bert_train, rocauc_lr_bert_train = classifier.evaluate(X_train_embeddings_bert, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_lr_bert_test, accuracy_lr_bert_test, f1_lr_bert_test, balaccuracy_lr_bert_test, rocauc_lr_bert_test = classifier.evaluate(X_test_embeddings_bert, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### BERT + Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Train Loss: 0.6776 | Val Loss: 0.6470 | F1 Score: 0.7842 | Balanced Accuracy: 0.5010 | AUC: 0.5010\n",
      "Epoch 2/15 | Train Loss: 0.6413 | Val Loss: 0.6506 | F1 Score: 0.7721 | Balanced Accuracy: 0.4994 | AUC: 0.4994\n",
      "Epoch 3/15 | Train Loss: 0.6171 | Val Loss: 0.6578 | F1 Score: 0.7656 | Balanced Accuracy: 0.5001 | AUC: 0.5001\n",
      "Epoch 4/15 | Train Loss: 0.5876 | Val Loss: 0.6700 | F1 Score: 0.7459 | Balanced Accuracy: 0.5041 | AUC: 0.5041\n",
      "Epoch 5/15 | Train Loss: 0.5454 | Val Loss: 0.6847 | F1 Score: 0.7267 | Balanced Accuracy: 0.5024 | AUC: 0.5024\n",
      "Epoch 6/15 | Train Loss: 0.4935 | Val Loss: 0.7091 | F1 Score: 0.7101 | Balanced Accuracy: 0.5119 | AUC: 0.5119\n",
      "Early stopping triggered. No improvement in 5 epochs.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1200x400 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[219 136]\n",
      " [ 40 605]] \n",
      "\n",
      "Accuracy: 82.4 \n",
      "\n",
      "F1 Score: 87.3 \n",
      "\n",
      "Balanced accuracy: 77.7 \n",
      "\n",
      "AUC Score: 77.7 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[110 339]\n",
      " [181 637]] \n",
      "\n",
      "Accuracy: 59.0 \n",
      "\n",
      "F1 Score: 71.0 \n",
      "\n",
      "Balanced accuracy: 51.2 \n",
      "\n",
      "AUC Score: 51.2 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# SET PARAMETERS\n",
    "dropout_rate = 0.1\n",
    "epochs_number = 15\n",
    "learning_rate = 0.0001\n",
    "early_stopping = 5\n",
    "# --------------------\n",
    "\n",
    "input_dim = X_train_embeddings_bert.shape[1]  # Dimensionality of word embeddings\n",
    "hidden_dim = X_train_embeddings_bert.shape[1]*2  # Number of units in the hidden layer\n",
    "\n",
    "classifier = NeuralNetworkClassifier(input_dim, hidden_dim, dropout=dropout_rate)\n",
    "\n",
    "# Fit the model\n",
    "classifier.fit(X_train_embeddings_bert, y_train, X_test_embeddings_bert, y_test, num_epochs=epochs_number, lr=learning_rate, patience=early_stopping)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_nn_bert_train, accuracy_nn_bert_train, f1_nn_bert_train, balaccuracy_nn_bert_train, rocauc_nn_bert_train = classifier.evaluate(X_train_embeddings_bert, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_nn_bert_test, accuracy_nn_bert_test, f1_nn_bert_test, balaccuracy_nn_bert_test, rocauc_nn_bert_test = classifier.evaluate(X_test_embeddings_bert, y_test)\n",
    "print('_______________________________________________________________________')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T00:52:42.639907106Z",
     "start_time": "2023-05-22T00:52:12.424528764Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uSk7WKhGB1v"
   },
   "source": [
    "## GloVe embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1D_jiR2GI7H"
   },
   "source": [
    "### GloVe + KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "id": "LA5swddzG28v",
    "ExecuteTime": {
     "end_time": "2023-05-22T00:53:06.860567452Z",
     "start_time": "2023-05-22T00:53:05.036758479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.692 total time=   0.0s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.679 total time=   0.0s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.664 total time=   0.0s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.651 total time=   0.0s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.664 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=5, weights=uniform;, score=0.706 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=5, weights=uniform;, score=0.693 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=5, weights=uniform;, score=0.674 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=5, weights=uniform;, score=0.671 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=5, weights=uniform;, score=0.693 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.646 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.575 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.585 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.561 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.567 total time=   0.0s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=9, weights=distance;, score=0.742 total time=   0.0s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=9, weights=distance;, score=0.697 total time=   0.0s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=9, weights=distance;, score=0.713 total time=   0.0s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=9, weights=distance;, score=0.725 total time=   0.0s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=9, weights=distance;, score=0.712 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=5, weights=distance;, score=0.706 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=5, weights=distance;, score=0.693 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=5, weights=distance;, score=0.674 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=5, weights=distance;, score=0.671 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=5, weights=distance;, score=0.693 total time=   0.0s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=5, weights=distance;, score=0.711 total time=   0.0s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=5, weights=distance;, score=0.691 total time=   0.0s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=5, weights=distance;, score=0.674 total time=   0.0s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=5, weights=distance;, score=0.652 total time=   0.0s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=5, weights=distance;, score=0.693 total time=   0.0s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=7, weights=distance;, score=0.716 total time=   0.0s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=7, weights=distance;, score=0.685 total time=   0.0s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=7, weights=distance;, score=0.707 total time=   0.0s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=7, weights=distance;, score=0.701 total time=   0.0s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=7, weights=distance;, score=0.678 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.636 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.646 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.636 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.617 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.654 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.700 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.681 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.659 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.649 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.679 total time=   0.0s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=9, weights=uniform;, score=0.742 total time=   0.0s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=9, weights=uniform;, score=0.697 total time=   0.0s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=9, weights=uniform;, score=0.713 total time=   0.0s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=9, weights=uniform;, score=0.725 total time=   0.0s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=9, weights=uniform;, score=0.712 total time=   0.0s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.711 total time=   0.0s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.691 total time=   0.0s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.674 total time=   0.0s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.652 total time=   0.0s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.693 total time=   0.0s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=2, weights=uniform;, score=0.500 total time=   0.0s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=2, weights=uniform;, score=0.527 total time=   0.0s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=2, weights=uniform;, score=0.491 total time=   0.0s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=2, weights=uniform;, score=0.465 total time=   0.0s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=2, weights=uniform;, score=0.464 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=7, weights=distance;, score=0.735 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=7, weights=distance;, score=0.671 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=7, weights=distance;, score=0.702 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=7, weights=distance;, score=0.715 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=7, weights=distance;, score=0.703 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=3, weights=uniform;, score=0.700 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=3, weights=uniform;, score=0.681 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=3, weights=uniform;, score=0.659 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=3, weights=uniform;, score=0.649 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=3, weights=uniform;, score=0.679 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=8, weights=uniform;, score=0.678 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=8, weights=uniform;, score=0.659 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=8, weights=uniform;, score=0.664 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=8, weights=uniform;, score=0.642 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=8, weights=uniform;, score=0.674 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=6, weights=uniform;, score=0.664 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=6, weights=uniform;, score=0.654 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=6, weights=uniform;, score=0.581 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=6, weights=uniform;, score=0.643 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=6, weights=uniform;, score=0.626 total time=   0.0s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=6, weights=uniform;, score=0.686 total time=   0.0s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=6, weights=uniform;, score=0.592 total time=   0.0s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=6, weights=uniform;, score=0.595 total time=   0.0s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=6, weights=uniform;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=6, weights=uniform;, score=0.620 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=8, weights=distance;, score=0.727 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=8, weights=distance;, score=0.681 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=8, weights=distance;, score=0.694 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=8, weights=distance;, score=0.674 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=8, weights=distance;, score=0.692 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=2, weights=uniform;, score=0.522 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=2, weights=uniform;, score=0.488 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=2, weights=uniform;, score=0.488 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=2, weights=uniform;, score=0.462 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=2, weights=uniform;, score=0.486 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=9, weights=uniform;, score=0.739 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=9, weights=uniform;, score=0.694 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=9, weights=uniform;, score=0.725 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=9, weights=uniform;, score=0.732 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=9, weights=uniform;, score=0.732 total time=   0.0s\n",
      "Best parameters: {'weights': 'uniform', 'n_neighbors': 9, 'metric': 'minkowski'} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[ 89 266]\n",
      " [ 62 583]] \n",
      "\n",
      "Accuracy: 67.2 \n",
      "\n",
      "F1 Score: 78.0 \n",
      "\n",
      "Balanced accuracy: 78.0 \n",
      "\n",
      "AUC Score: 57.7 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[ 71 378]\n",
      " [119 699]] \n",
      "\n",
      "Accuracy: 60.8 \n",
      "\n",
      "F1 Score: 73.8 \n",
      "\n",
      "Balanced accuracy: 73.8 \n",
      "\n",
      "AUC Score: 50.6 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = KNNClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'weights' : ['uniform','distance'],\n",
    "    'metric' : ['minkowski','euclidean','manhattan']\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_glove, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_glove, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_knn_glove_train, accuracy_knn_glove_train, f1_knn_glove_train, balaccuracy_knn_glove_train, rocauc_knn_glove_train = classifier.evaluate(X_train_embeddings_glove, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_knn_glove_test, accuracy_knn_glove_test, f1_knn_glove_test, balaccuracy_knn_glove_test, rocauc_knn_glove_test = classifier.evaluate(X_test_embeddings_glove, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Lfsat69GIoJ"
   },
   "source": [
    "### GloVe + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "id": "rt7c3MSPG3PP",
    "ExecuteTime": {
     "end_time": "2023-05-22T00:54:03.991379249Z",
     "start_time": "2023-05-22T00:53:07.139535944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.758 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.766 total time=   0.5s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.763 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.764 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.771 total time=   0.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.667 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.641 total time=   0.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.664 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.684 total time=   0.3s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.651 total time=   0.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.615 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.627 total time=   0.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.627 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.672 total time=   0.3s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.623 total time=   0.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.604 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.667 total time=   0.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.692 total time=   0.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.654 total time=   0.3s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.662 total time=   0.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.618 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.621 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.672 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.623 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.661 total time=   0.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.742 total time=   2.1s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.756 total time=   2.1s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.763 total time=   1.9s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.759 total time=   1.9s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.759 total time=   2.0s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.683 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.703 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.709 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.726 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.688 total time=   0.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.695 total time=   0.2s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.688 total time=   0.1s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.737 total time=   0.1s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.709 total time=   0.1s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.683 total time=   0.1s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.716 total time=   1.8s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.700 total time=   2.5s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.717 total time=   1.9s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.727 total time=   0.9s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.718 total time=   0.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.766 total time=   0.1s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.731 total time=   0.1s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.737 total time=   0.1s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.775 total time=   0.1s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.747 total time=   0.1s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.625 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.646 total time=   0.3s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.699 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.686 total time=   0.3s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.621 total time=   0.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.705 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.606 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.718 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.702 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.652 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.625 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.646 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.699 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.686 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.621 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.681 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.632 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.685 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.696 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.656 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.763 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.752 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.760 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.752 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.749 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.711 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.716 total time=   0.3s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.720 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.705 total time=   0.3s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.727 total time=   0.3s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.681 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.561 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.672 total time=   0.4s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.687 total time=   0.4s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.646 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.636 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.624 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.636 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.654 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.646 total time=   0.5s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.609 total time=   0.1s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.624 total time=   0.1s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.664 total time=   0.1s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.652 total time=   0.1s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.636 total time=   0.1s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.735 total time=   0.2s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.700 total time=   0.2s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.727 total time=   0.2s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.716 total time=   0.2s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.717 total time=   0.2s\n",
      "Best parameters: {'subsample': 0.7, 'objective': 'reg:squarederror', 'n_estimators': 100, 'min_child_weight': 3, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 0.7} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[350   5]\n",
      " [  0 645]] \n",
      "\n",
      "Accuracy: 99.5 \n",
      "\n",
      "F1 Score: 99.6 \n",
      "\n",
      "Balanced accuracy: 99.6 \n",
      "\n",
      "AUC Score: 99.3 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[ 24 425]\n",
      " [ 39 779]] \n",
      "\n",
      "Accuracy: 63.4 \n",
      "\n",
      "F1 Score: 77.1 \n",
      "\n",
      "Balanced accuracy: 77.1 \n",
      "\n",
      "AUC Score: 50.3 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = XGBoostClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'learning_rate': [0.01, 0.1, 0.3, 0.5],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.5, 0.7],\n",
    "    'colsample_bytree': [0.5, 0.7],\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'objective': ['reg:squarederror']\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_glove, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_glove, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_xgb_glove_train, accuracy_xgb_glove_train, f1_xgb_glove_train, balaccuracy_xgb_glove_train, rocauc_xgb_glove_train = classifier.evaluate(X_train_embeddings_glove, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_xgb_glove_test, accuracy_xgb_glove_test, f1_xgb_glove_test, balaccuracy_xgb_glove_test, rocauc_xgb_glove_test = classifier.evaluate(X_test_embeddings_glove, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUCEQAQXGIl5"
   },
   "source": [
    "### GloVe + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "id": "GLutlbgvG3f8",
    "ExecuteTime": {
     "end_time": "2023-05-22T00:54:11.530621454Z",
     "start_time": "2023-05-22T00:54:03.994077578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.681 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.611 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.676 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.634 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.634 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.752 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.751 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.769 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.757 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.769 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.700 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.624 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.710 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.724 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.681 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.667 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.644 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.644 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.672 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.773 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.734 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.736 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.773 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.761 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.655 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.614 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.649 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.636 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.691 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.708 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.647 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.677 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.664 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.667 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.750 total time=   0.2s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.751 total time=   0.2s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.760 total time=   0.2s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.770 total time=   0.2s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.772 total time=   0.2s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.750 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.730 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.752 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.731 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.733 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.722 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.728 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.757 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.744 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.727 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.747 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.781 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.757 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.774 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.767 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.743 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.760 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.752 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.736 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.774 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.634 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.674 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.682 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.644 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.698 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.721 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.743 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.727 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.722 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.743 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.767 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.737 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.748 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.768 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.700 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.709 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.733 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.752 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.736 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.693 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.694 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.657 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.691 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.701 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.641 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.692 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.683 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.735 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.722 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.749 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.649 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.671 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.628 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.713 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.682 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.709 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.725 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.727 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.754 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.772 total time=   0.1s\n",
      "Best parameters: {'n_estimators': 25, 'max_features': 5, 'max_depth': 10, 'bootstrap': True} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[316  39]\n",
      " [  0 645]] \n",
      "\n",
      "Accuracy: 96.1 \n",
      "\n",
      "F1 Score: 97.1 \n",
      "\n",
      "Balanced accuracy: 97.1 \n",
      "\n",
      "AUC Score: 94.5 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[ 19 430]\n",
      " [ 55 763]] \n",
      "\n",
      "Accuracy: 61.7 \n",
      "\n",
      "F1 Score: 75.9 \n",
      "\n",
      "Balanced accuracy: 75.9 \n",
      "\n",
      "AUC Score: 48.8 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = RFClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'n_estimators': [10, 25], \n",
    "    'max_features': [5, 10],\n",
    "    'max_depth': [10, 50, None], \n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_glove, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_glove, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_rf_glove_train, accuracy_rf_glove_train, f1_rf_glove_train, balaccuracy_rf_glove_train, rocauc_rf_glove_train = classifier.evaluate(X_train_embeddings_glove, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_rf_glove_test, accuracy_rf_glove_test, f1_rf_glove_test, balaccuracy_rf_glove_test, rocauc_rf_glove_test = classifier.evaluate(X_test_embeddings_glove, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vldrLVlwGIg9"
   },
   "source": [
    "### GloVe + SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "id": "MqqebdmmG3qI",
    "ExecuteTime": {
     "end_time": "2023-05-22T01:00:29.529644472Z",
     "start_time": "2023-05-22T00:54:11.519244350Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:292: UserWarning: The total space of parameters 16 is smaller than n_iter=20. Running 16 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV 1/5] END ...C=1, gamma=scale, kernel=linear;, score=0.718 total time=   0.1s\n",
      "[CV 2/5] END ...C=1, gamma=scale, kernel=linear;, score=0.659 total time=   0.1s\n",
      "[CV 3/5] END ...C=1, gamma=scale, kernel=linear;, score=0.714 total time=   0.1s\n",
      "[CV 4/5] END ...C=1, gamma=scale, kernel=linear;, score=0.738 total time=   0.1s\n",
      "[CV 5/5] END ...C=1, gamma=scale, kernel=linear;, score=0.689 total time=   0.1s\n",
      "[CV 1/5] END .....C=1, gamma=scale, kernel=poly;, score=0.765 total time=   0.1s\n",
      "[CV 2/5] END .....C=1, gamma=scale, kernel=poly;, score=0.785 total time=   0.1s\n",
      "[CV 3/5] END .....C=1, gamma=scale, kernel=poly;, score=0.770 total time=   0.1s\n",
      "[CV 4/5] END .....C=1, gamma=scale, kernel=poly;, score=0.787 total time=   0.1s\n",
      "[CV 5/5] END .....C=1, gamma=scale, kernel=poly;, score=0.779 total time=   0.1s\n",
      "[CV 1/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.784 total time=   0.1s\n",
      "[CV 2/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.784 total time=   0.1s\n",
      "[CV 4/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.784 total time=   0.1s\n",
      "[CV 5/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.784 total time=   0.1s\n",
      "[CV 1/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.773 total time=   0.1s\n",
      "[CV 2/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.773 total time=   0.1s\n",
      "[CV 3/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.774 total time=   0.1s\n",
      "[CV 4/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.784 total time=   0.1s\n",
      "[CV 5/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.778 total time=   0.1s\n",
      "[CV 1/5] END ..C=10, gamma=scale, kernel=linear;, score=0.674 total time=   0.3s\n",
      "[CV 2/5] END ..C=10, gamma=scale, kernel=linear;, score=0.593 total time=   0.2s\n",
      "[CV 3/5] END ..C=10, gamma=scale, kernel=linear;, score=0.637 total time=   0.3s\n",
      "[CV 4/5] END ..C=10, gamma=scale, kernel=linear;, score=0.693 total time=   0.5s\n",
      "[CV 5/5] END ..C=10, gamma=scale, kernel=linear;, score=0.602 total time=   0.3s\n",
      "[CV 1/5] END ....C=10, gamma=scale, kernel=poly;, score=0.642 total time=   0.1s\n",
      "[CV 2/5] END ....C=10, gamma=scale, kernel=poly;, score=0.664 total time=   0.1s\n",
      "[CV 3/5] END ....C=10, gamma=scale, kernel=poly;, score=0.662 total time=   0.1s\n",
      "[CV 4/5] END ....C=10, gamma=scale, kernel=poly;, score=0.714 total time=   0.1s\n",
      "[CV 5/5] END ....C=10, gamma=scale, kernel=poly;, score=0.652 total time=   0.1s\n",
      "[CV 1/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.698 total time=   0.1s\n",
      "[CV 2/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.654 total time=   0.1s\n",
      "[CV 3/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.639 total time=   0.1s\n",
      "[CV 4/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.683 total time=   0.1s\n",
      "[CV 5/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.654 total time=   0.1s\n",
      "[CV 1/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.693 total time=   0.1s\n",
      "[CV 2/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.638 total time=   0.1s\n",
      "[CV 3/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.567 total time=   0.1s\n",
      "[CV 4/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.687 total time=   0.1s\n",
      "[CV 5/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.648 total time=   0.1s\n",
      "[CV 1/5] END .C=100, gamma=scale, kernel=linear;, score=0.669 total time=   4.2s\n",
      "[CV 2/5] END .C=100, gamma=scale, kernel=linear;, score=0.602 total time=   4.3s\n",
      "[CV 3/5] END .C=100, gamma=scale, kernel=linear;, score=0.632 total time=   8.8s\n",
      "[CV 4/5] END .C=100, gamma=scale, kernel=linear;, score=0.679 total time=   5.5s\n",
      "[CV 5/5] END .C=100, gamma=scale, kernel=linear;, score=0.618 total time=   3.0s\n",
      "[CV 1/5] END ...C=100, gamma=scale, kernel=poly;, score=0.642 total time=   0.1s\n",
      "[CV 2/5] END ...C=100, gamma=scale, kernel=poly;, score=0.623 total time=   0.1s\n",
      "[CV 3/5] END ...C=100, gamma=scale, kernel=poly;, score=0.639 total time=   0.1s\n",
      "[CV 4/5] END ...C=100, gamma=scale, kernel=poly;, score=0.681 total time=   0.1s\n",
      "[CV 5/5] END ...C=100, gamma=scale, kernel=poly;, score=0.649 total time=   0.1s\n",
      "[CV 1/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.657 total time=   0.1s\n",
      "[CV 2/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.633 total time=   0.1s\n",
      "[CV 3/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.638 total time=   0.1s\n",
      "[CV 4/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.683 total time=   0.1s\n",
      "[CV 5/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.636 total time=   0.1s\n",
      "[CV 1/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.676 total time=   0.0s\n",
      "[CV 2/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.632 total time=   0.0s\n",
      "[CV 3/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.566 total time=   0.0s\n",
      "[CV 4/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.649 total time=   0.1s\n",
      "[CV 5/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.609 total time=   0.0s\n",
      "[CV 1/5] END C=1000, gamma=scale, kernel=linear;, score=0.664 total time= 1.6min\n",
      "[CV 2/5] END C=1000, gamma=scale, kernel=linear;, score=0.601 total time=  60.0s\n",
      "[CV 3/5] END C=1000, gamma=scale, kernel=linear;, score=0.608 total time=  53.3s\n",
      "[CV 4/5] END C=1000, gamma=scale, kernel=linear;, score=0.684 total time=  59.1s\n",
      "[CV 5/5] END C=1000, gamma=scale, kernel=linear;, score=0.624 total time= 1.3min\n",
      "[CV 1/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.642 total time=   0.1s\n",
      "[CV 2/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.623 total time=   0.1s\n",
      "[CV 3/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.639 total time=   0.1s\n",
      "[CV 4/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.681 total time=   0.1s\n",
      "[CV 5/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.649 total time=   0.1s\n",
      "[CV 1/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.657 total time=   0.1s\n",
      "[CV 2/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.633 total time=   0.1s\n",
      "[CV 3/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.638 total time=   0.1s\n",
      "[CV 4/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.683 total time=   0.1s\n",
      "[CV 5/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.636 total time=   0.1s\n",
      "[CV 1/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.679 total time=   0.1s\n",
      "[CV 2/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.635 total time=   0.1s\n",
      "[CV 3/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.564 total time=   0.1s\n",
      "[CV 4/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.669 total time=   0.1s\n",
      "[CV 5/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.609 total time=   0.1s\n",
      "Best parameters: {'kernel': 'rbf', 'gamma': 'scale', 'C': 1} \n",
      "\n",
      "[LibSVM].*\n",
      "optimization finished, #iter = 1540\n",
      "obj = -635.785973, rho = 0.600692\n",
      "nSV = 909, nBSV = 515\n",
      "Total nSV = 909\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[ 13 342]\n",
      " [  0 645]] \n",
      "\n",
      "Accuracy: 65.8 \n",
      "\n",
      "F1 Score: 79.0 \n",
      "\n",
      "Balanced accuracy: 79.0 \n",
      "\n",
      "AUC Score: 51.8 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[  0 449]\n",
      " [  0 818]] \n",
      "\n",
      "Accuracy: 64.6 \n",
      "\n",
      "F1 Score: 78.5 \n",
      "\n",
      "Balanced accuracy: 78.5 \n",
      "\n",
      "AUC Score: 50.0 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = SVClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'C': [1, 10, 100, 1000],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale']\n",
    "    #'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_glove, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_glove, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_svc_glove_train, accuracy_svc_glove_train, f1_svc_glove_train, balaccuracy_svc_glove_train, rocauc_svc_glove_train = classifier.evaluate(X_train_embeddings_glove, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_svc_glove_test, accuracy_svc_glove_test, f1_svc_glove_test, balaccuracy_svc_glove_test, rocauc_svc_glove_test = classifier.evaluate(X_test_embeddings_glove, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8IPkDvHGIZw"
   },
   "source": [
    "### GloVe + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "id": "7j_OebkkG32s",
    "ExecuteTime": {
     "end_time": "2023-05-22T01:00:30.801691344Z",
     "start_time": "2023-05-22T01:00:29.531522028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END C=0.84, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.84, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.84, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.84, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.84, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.85, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.85, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.85, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.85, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.85, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.04, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.04, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.04, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.04, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.04, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.72, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.72, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.72, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.72, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.72, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ....C=0.25, penalty=l2, solver=sag;, score=0.758 total time=   0.1s\n",
      "[CV 2/5] END ....C=0.25, penalty=l2, solver=sag;, score=0.746 total time=   0.1s\n",
      "[CV 3/5] END ....C=0.25, penalty=l2, solver=sag;, score=0.771 total time=   0.1s\n",
      "[CV 4/5] END ....C=0.25, penalty=l2, solver=sag;, score=0.755 total time=   0.0s\n",
      "[CV 5/5] END ....C=0.25, penalty=l2, solver=sag;, score=0.750 total time=   0.0s\n",
      "[CV 1/5] END C=0.74, penalty=l1, solver=liblinear;, score=0.737 total time=   0.0s\n",
      "[CV 2/5] END C=0.74, penalty=l1, solver=liblinear;, score=0.734 total time=   0.0s\n",
      "[CV 3/5] END C=0.74, penalty=l1, solver=liblinear;, score=0.763 total time=   0.0s\n",
      "[CV 4/5] END C=0.74, penalty=l1, solver=liblinear;, score=0.765 total time=   0.0s\n",
      "[CV 5/5] END C=0.74, penalty=l1, solver=liblinear;, score=0.718 total time=   0.0s\n",
      "[CV 1/5] END C=0.45, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.45, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.45, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.45, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.45, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.77, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.77, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.77, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.77, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.77, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.58, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.58, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.58, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.58, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.58, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.05, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.05, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.05, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.05, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.05, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.76, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.76, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.76, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.76, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.76, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.17, penalty=none, solver=lbfgs;, score=0.672 total time=   0.0s\n",
      "[CV 2/5] END C=0.17, penalty=none, solver=lbfgs;, score=0.562 total time=   0.0s\n",
      "[CV 3/5] END C=0.17, penalty=none, solver=lbfgs;, score=0.626 total time=   0.0s\n",
      "[CV 4/5] END C=0.17, penalty=none, solver=lbfgs;, score=0.662 total time=   0.0s\n",
      "[CV 5/5] END C=0.17, penalty=none, solver=lbfgs;, score=0.640 total time=   0.0s\n",
      "[CV 1/5] END C=0.1, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 2/5] END C=0.1, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=0.1, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 4/5] END C=0.1, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 5/5] END C=0.1, penalty=l1, solver=liblinear;, score=0.780 total time=   0.0s\n",
      "[CV 1/5] END C=0.06, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.06, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.06, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.06, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.06, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ....C=0.91, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ....C=0.91, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ....C=0.91, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ....C=0.91, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ....C=0.91, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.15, penalty=l2, solver=liblinear;, score=0.768 total time=   0.0s\n",
      "[CV 2/5] END C=0.15, penalty=l2, solver=liblinear;, score=0.771 total time=   0.0s\n",
      "[CV 3/5] END C=0.15, penalty=l2, solver=liblinear;, score=0.777 total time=   0.0s\n",
      "[CV 4/5] END C=0.15, penalty=l2, solver=liblinear;, score=0.772 total time=   0.0s\n",
      "[CV 5/5] END C=0.15, penalty=l2, solver=liblinear;, score=0.765 total time=   0.0s\n",
      "[CV 1/5] END ..C=0.54, penalty=l2, solver=lbfgs;, score=0.741 total time=   0.0s\n",
      "[CV 2/5] END ..C=0.54, penalty=l2, solver=lbfgs;, score=0.709 total time=   0.0s\n",
      "[CV 3/5] END ..C=0.54, penalty=l2, solver=lbfgs;, score=0.763 total time=   0.0s\n",
      "[CV 4/5] END ..C=0.54, penalty=l2, solver=lbfgs;, score=0.752 total time=   0.0s\n",
      "[CV 5/5] END ..C=0.54, penalty=l2, solver=lbfgs;, score=0.726 total time=   0.0s\n",
      "[CV 1/5] END .....C=0.5, penalty=l2, solver=sag;, score=0.741 total time=   0.1s\n",
      "[CV 2/5] END .....C=0.5, penalty=l2, solver=sag;, score=0.709 total time=   0.1s\n",
      "[CV 3/5] END .....C=0.5, penalty=l2, solver=sag;, score=0.763 total time=   0.1s\n",
      "[CV 4/5] END .....C=0.5, penalty=l2, solver=sag;, score=0.752 total time=   0.1s\n",
      "[CV 5/5] END .....C=0.5, penalty=l2, solver=sag;, score=0.725 total time=   0.1s\n",
      "[CV 1/5] END C=0.36, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.36, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.36, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.36, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.36, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.14, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.14, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.14, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.14, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.14, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "Best parameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 0.1} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[  0 355]\n",
      " [  0 645]] \n",
      "\n",
      "Accuracy: 64.5 \n",
      "\n",
      "F1 Score: 78.4 \n",
      "\n",
      "Balanced accuracy: 78.4 \n",
      "\n",
      "AUC Score: 50.0 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[  0 449]\n",
      " [  0 818]] \n",
      "\n",
      "Accuracy: 64.6 \n",
      "\n",
      "F1 Score: 78.5 \n",
      "\n",
      "Balanced accuracy: 78.5 \n",
      "\n",
      "AUC Score: 50.0 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "65 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1471, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.75587202 0.7433849\n",
      "        nan        nan        nan        nan        nan 0.63214927\n",
      " 0.78345318        nan        nan 0.77057728 0.73812925 0.73800852\n",
      "        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = LRClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'C' : np.arange(0, 1, 0.01)\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_glove, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_glove, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_lr_glove_train, accuracy_lr_glove_train, f1_lr_glove_train, balaccuracy_lr_glove_train, rocauc_lr_glove_train = classifier.evaluate(X_train_embeddings_glove, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_lr_glove_test, accuracy_lr_glove_test, f1_lr_glove_test, balaccuracy_lr_glove_test, rocauc_lr_glove_test = classifier.evaluate(X_test_embeddings_glove, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GloVe + Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Train Loss: 0.6582 | Val Loss: 0.6523 | F1 Score: 0.7847 | Balanced Accuracy: 0.5000 | AUC: 0.5000\n",
      "Epoch 2/15 | Train Loss: 0.6394 | Val Loss: 0.6499 | F1 Score: 0.7847 | Balanced Accuracy: 0.5000 | AUC: 0.5000\n",
      "Epoch 3/15 | Train Loss: 0.6228 | Val Loss: 0.6494 | F1 Score: 0.7833 | Balanced Accuracy: 0.4993 | AUC: 0.4993\n",
      "Epoch 4/15 | Train Loss: 0.6023 | Val Loss: 0.6513 | F1 Score: 0.7751 | Balanced Accuracy: 0.5024 | AUC: 0.5024\n",
      "Epoch 5/15 | Train Loss: 0.5786 | Val Loss: 0.6556 | F1 Score: 0.7687 | Balanced Accuracy: 0.5128 | AUC: 0.5128\n",
      "Epoch 6/15 | Train Loss: 0.5496 | Val Loss: 0.6621 | F1 Score: 0.7621 | Balanced Accuracy: 0.5210 | AUC: 0.5210\n",
      "Epoch 7/15 | Train Loss: 0.5189 | Val Loss: 0.6709 | F1 Score: 0.7572 | Balanced Accuracy: 0.5364 | AUC: 0.5364\n",
      "Epoch 8/15 | Train Loss: 0.4857 | Val Loss: 0.6813 | F1 Score: 0.7440 | Balanced Accuracy: 0.5316 | AUC: 0.5316\n",
      "Early stopping triggered. No improvement in 5 epochs.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1200x400 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[205 150]\n",
      " [ 19 626]] \n",
      "\n",
      "Accuracy: 83.1 \n",
      "\n",
      "F1 Score: 88.1 \n",
      "\n",
      "Balanced accuracy: 77.4 \n",
      "\n",
      "AUC Score: 77.4 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[ 97 352]\n",
      " [125 693]] \n",
      "\n",
      "Accuracy: 62.4 \n",
      "\n",
      "F1 Score: 74.4 \n",
      "\n",
      "Balanced accuracy: 53.2 \n",
      "\n",
      "AUC Score: 53.2 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# SET PARAMETERS\n",
    "dropout_rate = 0.1\n",
    "epochs_number = 15\n",
    "learning_rate = 0.0001\n",
    "early_stopping = 5\n",
    "# --------------------\n",
    "\n",
    "input_dim = X_train_embeddings_glove.shape[1]  # Dimensionality of word embeddings\n",
    "hidden_dim = X_train_embeddings_glove.shape[1]*2  # Number of units in the hidden layer\n",
    "\n",
    "classifier = NeuralNetworkClassifier(input_dim, hidden_dim, dropout=dropout_rate)\n",
    "\n",
    "# Fit the model\n",
    "classifier.fit(X_train_embeddings_glove, y_train, X_test_embeddings_glove, y_test, num_epochs=epochs_number, lr=learning_rate, patience=early_stopping)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_nn_glove_train, accuracy_nn_glove_train, f1_nn_glove_train, balaccuracy_nn_glove_train, rocauc_nn_glove_train = classifier.evaluate(X_train_embeddings_glove, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_nn_glove_test, accuracy_nn_glove_test, f1_nn_glove_test, balaccuracy_nn_glove_test, rocauc_nn_glove_test = classifier.evaluate(X_test_embeddings_glove, y_test)\n",
    "print('_______________________________________________________________________')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T01:00:39.869953848Z",
     "start_time": "2023-05-22T01:00:30.801989572Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec + KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:00:41.587553598Z",
     "start_time": "2023-05-22T01:00:39.813202873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=6, weights=uniform;, score=0.648 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=6, weights=uniform;, score=0.638 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=6, weights=uniform;, score=0.714 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=6, weights=uniform;, score=0.612 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=6, weights=uniform;, score=0.570 total time=   0.0s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=6, weights=uniform;, score=0.632 total time=   0.0s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=6, weights=uniform;, score=0.664 total time=   0.0s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=6, weights=uniform;, score=0.690 total time=   0.0s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=6, weights=uniform;, score=0.636 total time=   0.0s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=6, weights=uniform;, score=0.614 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.677 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.674 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.634 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.654 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.606 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=2, weights=distance;, score=0.677 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=2, weights=distance;, score=0.674 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=2, weights=distance;, score=0.634 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=2, weights=distance;, score=0.654 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=2, weights=distance;, score=0.606 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.695 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.703 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.775 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.742 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.713 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=5, weights=uniform;, score=0.707 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=5, weights=uniform;, score=0.703 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=5, weights=uniform;, score=0.718 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=5, weights=uniform;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=5, weights=uniform;, score=0.636 total time=   0.0s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.684 total time=   0.0s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.691 total time=   0.0s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.713 total time=   0.0s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.676 total time=   0.0s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.631 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.688 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.690 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.742 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.686 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.639 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=8, weights=uniform;, score=0.659 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=8, weights=uniform;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=8, weights=uniform;, score=0.721 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=8, weights=uniform;, score=0.661 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=8, weights=uniform;, score=0.604 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=8, weights=uniform;, score=0.659 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=8, weights=uniform;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=8, weights=uniform;, score=0.721 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=8, weights=uniform;, score=0.661 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=8, weights=uniform;, score=0.604 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.679 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.691 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.709 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.693 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.641 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.674 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.683 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.706 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.681 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.624 total time=   0.0s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.686 total time=   0.0s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.693 total time=   0.0s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.742 total time=   0.0s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.681 total time=   0.0s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.659 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=3, weights=uniform;, score=0.674 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=3, weights=uniform;, score=0.683 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=3, weights=uniform;, score=0.706 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=3, weights=uniform;, score=0.681 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=3, weights=uniform;, score=0.624 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=3, weights=distance;, score=0.674 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=3, weights=distance;, score=0.683 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=3, weights=distance;, score=0.706 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=3, weights=distance;, score=0.681 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=3, weights=distance;, score=0.624 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.640 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.589 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.644 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.601 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.553 total time=   0.0s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=3, weights=distance;, score=0.662 total time=   0.0s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=3, weights=distance;, score=0.693 total time=   0.0s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=3, weights=distance;, score=0.722 total time=   0.0s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=3, weights=distance;, score=0.644 total time=   0.0s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=3, weights=distance;, score=0.638 total time=   0.0s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=8, weights=uniform;, score=0.661 total time=   0.0s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=8, weights=uniform;, score=0.659 total time=   0.0s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=8, weights=uniform;, score=0.729 total time=   0.0s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=8, weights=uniform;, score=0.693 total time=   0.0s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=8, weights=uniform;, score=0.621 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=5, weights=uniform;, score=0.707 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=5, weights=uniform;, score=0.703 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=5, weights=uniform;, score=0.718 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=5, weights=uniform;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=5, weights=uniform;, score=0.636 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=5, weights=distance;, score=0.707 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=5, weights=distance;, score=0.703 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=5, weights=distance;, score=0.718 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=5, weights=distance;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=5, weights=distance;, score=0.636 total time=   0.0s\n",
      "Best parameters: {'weights': 'uniform', 'n_neighbors': 9, 'metric': 'euclidean'} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[103 252]\n",
      " [ 70 575]] \n",
      "\n",
      "Accuracy: 67.8 \n",
      "\n",
      "F1 Score: 78.1 \n",
      "\n",
      "Balanced accuracy: 78.1 \n",
      "\n",
      "AUC Score: 59.1 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[ 68 381]\n",
      " [117 701]] \n",
      "\n",
      "Accuracy: 60.7 \n",
      "\n",
      "F1 Score: 73.8 \n",
      "\n",
      "Balanced accuracy: 73.8 \n",
      "\n",
      "AUC Score: 50.4 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = KNNClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'weights' : ['uniform','distance'],\n",
    "    'metric' : ['minkowski','euclidean','manhattan']\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_word2vec, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_word2vec, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_knn_w2v_train, accuracy_knn_w2v_train, f1_knn_w2v_train, balaccuracy_knn_w2v_train, rocauc_knn_w2v_train = classifier.evaluate(X_train_embeddings_word2vec, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_knn_w2v_test, accuracy_knn_w2v_test, f1_knn_w2v_test, balaccuracy_knn_w2v_test, rocauc_knn_w2v_test = classifier.evaluate(X_test_embeddings_word2vec, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:01:42.307248583Z",
     "start_time": "2023-05-22T01:00:41.593893391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.676 total time=   1.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.713 total time=   1.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.698 total time=   1.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.681 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.656 total time=   1.2s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.699 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.657 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.672 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.620 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.693 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.638 total time=   0.1s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.678 total time=   0.1s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.684 total time=   0.1s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.659 total time=   0.1s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.705 total time=   0.1s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.696 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.741 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.757 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.746 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.671 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.649 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.715 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.657 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.706 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.710 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.623 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.577 total time=   0.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.664 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.674 total time=   0.3s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.651 total time=   0.3s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.757 total time=   0.1s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.773 total time=   0.1s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.784 total time=   0.1s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.764 total time=   0.1s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.764 total time=   0.1s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.674 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.738 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.719 total time=   0.4s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.654 total time=   0.4s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.695 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.627 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.635 total time=   0.3s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.644 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.617 total time=   0.3s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.638 total time=   0.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=5, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.604 total time=   0.1s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=5, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.640 total time=   0.1s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=5, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.647 total time=   0.1s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=5, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.635 total time=   0.1s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=5, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.682 total time=   0.1s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.656 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.618 total time=   0.3s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.637 total time=   0.4s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.585 total time=   0.3s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.659 total time=   0.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.734 total time=   1.6s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.757 total time=   1.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.761 total time=   1.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.716 total time=   1.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.736 total time=   1.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.674 total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.688 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.720 total time=   1.2s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.679 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.719 total time=   1.2s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.563 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.606 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.669 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.513 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.669 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.686 total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.755 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.683 total time=   1.3s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.706 total time=   1.3s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.722 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.641 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.654 total time=   0.5s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.633 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.635 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.697 total time=   0.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.656 total time=   0.2s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.652 total time=   0.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.681 total time=   0.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.639 total time=   0.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.674 total time=   0.2s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=7, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.672 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=7, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.680 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=7, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.615 total time=   0.4s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=7, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.679 total time=   0.4s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=7, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.689 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.664 total time=   0.2s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.649 total time=   0.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.704 total time=   0.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.641 total time=   0.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.664 total time=   0.2s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.742 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.728 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.744 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.700 total time=   0.9s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.728 total time=   0.9s\n",
      "Best parameters: {'subsample': 0.5, 'objective': 'reg:squarederror', 'n_estimators': 50, 'min_child_weight': 5, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.7} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[180 175]\n",
      " [  0 645]] \n",
      "\n",
      "Accuracy: 82.5 \n",
      "\n",
      "F1 Score: 88.1 \n",
      "\n",
      "Balanced accuracy: 88.1 \n",
      "\n",
      "AUC Score: 75.4 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[ 14 435]\n",
      " [ 20 798]] \n",
      "\n",
      "Accuracy: 64.1 \n",
      "\n",
      "F1 Score: 77.8 \n",
      "\n",
      "Balanced accuracy: 77.8 \n",
      "\n",
      "AUC Score: 50.3 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = XGBoostClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'learning_rate': [0.01, 0.1, 0.3, 0.5],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.5, 0.7],\n",
    "    'colsample_bytree': [0.5, 0.7],\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'objective': ['reg:squarederror']\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_word2vec, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_word2vec, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_xgb_w2v_train, accuracy_xgb_w2v_train, f1_xgb_w2v_train, balaccuracy_xgb_w2v_train, rocauc_xgb_w2v_train = classifier.evaluate(X_train_embeddings_word2vec, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_xgb_w2v_test, accuracy_xgb_w2v_test, f1_xgb_w2v_test, balaccuracy_xgb_w2v_test, rocauc_xgb_w2v_test = classifier.evaluate(X_test_embeddings_word2vec, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:01:50.494926763Z",
     "start_time": "2023-05-22T01:01:42.307473981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.654 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.669 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.686 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.648 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.723 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.741 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.745 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.751 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.724 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.748 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.646 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.703 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.667 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.674 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.644 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.647 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.697 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.693 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.669 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.610 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.745 total time=   0.2s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.785 total time=   0.2s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.762 total time=   0.2s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.755 total time=   0.2s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.764 total time=   0.2s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.679 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.704 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.669 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.657 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.669 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.728 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.745 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.747 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.752 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.719 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.727 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.748 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.756 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.743 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.766 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.728 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.753 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.761 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.763 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.749 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.630 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.676 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.689 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.662 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.696 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.684 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.688 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.674 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.605 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.672 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.751 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.772 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.771 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.763 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.749 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.672 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.709 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.652 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.679 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.693 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.727 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.756 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.769 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.737 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.709 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.755 total time=   0.2s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.749 total time=   0.2s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.752 total time=   0.2s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.750 total time=   0.2s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.752 total time=   0.2s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.748 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.804 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.747 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.763 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.752 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.732 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.748 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.709 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.722 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.680 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.751 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.771 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.756 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.726 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.711 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.718 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.750 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.723 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.718 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.718 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.699 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.738 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.727 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.744 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.740 total time=   0.0s\n",
      "Best parameters: {'n_estimators': 25, 'max_features': 5, 'max_depth': 50, 'bootstrap': True} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[355   0]\n",
      " [  0 645]] \n",
      "\n",
      "Accuracy: 100.0 \n",
      "\n",
      "F1 Score: 100.0 \n",
      "\n",
      "Balanced accuracy: 100.0 \n",
      "\n",
      "AUC Score: 100.0 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[ 62 387]\n",
      " [101 717]] \n",
      "\n",
      "Accuracy: 61.5 \n",
      "\n",
      "F1 Score: 74.6 \n",
      "\n",
      "Balanced accuracy: 74.6 \n",
      "\n",
      "AUC Score: 50.7 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = RFClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'n_estimators': [10, 25],\n",
    "    'max_features': [5, 10],\n",
    "    'max_depth': [10, 50, None],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_word2vec, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_word2vec, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_rf_w2v_train, accuracy_rf_w2v_train, f1_rf_w2v_train, balaccuracy_rf_w2v_train, rocauc_rf_w2v_train = classifier.evaluate(X_train_embeddings_word2vec, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_rf_w2v_test, accuracy_rf_w2v_test, f1_rf_w2v_test, balaccuracy_rf_w2v_test, rocauc_rf_w2v_test = classifier.evaluate(X_test_embeddings_word2vec, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec + SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:03:31.197761231Z",
     "start_time": "2023-05-22T01:01:50.494732024Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV 1/5] END ...C=1, gamma=scale, kernel=linear;, score=0.757 total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:292: UserWarning: The total space of parameters 16 is smaller than n_iter=20. Running 16 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END ...C=1, gamma=scale, kernel=linear;, score=0.786 total time=   0.1s\n",
      "[CV 3/5] END ...C=1, gamma=scale, kernel=linear;, score=0.758 total time=   0.1s\n",
      "[CV 4/5] END ...C=1, gamma=scale, kernel=linear;, score=0.742 total time=   0.1s\n",
      "[CV 5/5] END ...C=1, gamma=scale, kernel=linear;, score=0.748 total time=   0.1s\n",
      "[CV 1/5] END .....C=1, gamma=scale, kernel=poly;, score=0.768 total time=   0.1s\n",
      "[CV 2/5] END .....C=1, gamma=scale, kernel=poly;, score=0.782 total time=   0.1s\n",
      "[CV 3/5] END .....C=1, gamma=scale, kernel=poly;, score=0.774 total time=   0.1s\n",
      "[CV 4/5] END .....C=1, gamma=scale, kernel=poly;, score=0.765 total time=   0.1s\n",
      "[CV 5/5] END .....C=1, gamma=scale, kernel=poly;, score=0.772 total time=   0.1s\n",
      "[CV 1/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.777 total time=   0.1s\n",
      "[CV 2/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.780 total time=   0.1s\n",
      "[CV 4/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.780 total time=   0.1s\n",
      "[CV 5/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.787 total time=   0.1s\n",
      "[CV 1/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.784 total time=   0.1s\n",
      "[CV 2/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.789 total time=   0.1s\n",
      "[CV 3/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.783 total time=   0.1s\n",
      "[CV 4/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.778 total time=   0.1s\n",
      "[CV 5/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.782 total time=   0.1s\n",
      "[CV 1/5] END ..C=10, gamma=scale, kernel=linear;, score=0.659 total time=   0.1s\n",
      "[CV 2/5] END ..C=10, gamma=scale, kernel=linear;, score=0.686 total time=   0.1s\n",
      "[CV 3/5] END ..C=10, gamma=scale, kernel=linear;, score=0.639 total time=   0.1s\n",
      "[CV 4/5] END ..C=10, gamma=scale, kernel=linear;, score=0.669 total time=   0.1s\n",
      "[CV 5/5] END ..C=10, gamma=scale, kernel=linear;, score=0.657 total time=   0.1s\n",
      "[CV 1/5] END ....C=10, gamma=scale, kernel=poly;, score=0.713 total time=   0.1s\n",
      "[CV 2/5] END ....C=10, gamma=scale, kernel=poly;, score=0.678 total time=   0.1s\n",
      "[CV 3/5] END ....C=10, gamma=scale, kernel=poly;, score=0.676 total time=   0.1s\n",
      "[CV 4/5] END ....C=10, gamma=scale, kernel=poly;, score=0.669 total time=   0.1s\n",
      "[CV 5/5] END ....C=10, gamma=scale, kernel=poly;, score=0.664 total time=   0.1s\n",
      "[CV 1/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.659 total time=   0.1s\n",
      "[CV 2/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.657 total time=   0.1s\n",
      "[CV 3/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.667 total time=   0.1s\n",
      "[CV 4/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.703 total time=   0.1s\n",
      "[CV 5/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.672 total time=   0.1s\n",
      "[CV 1/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.635 total time=   0.1s\n",
      "[CV 2/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.691 total time=   0.1s\n",
      "[CV 3/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.674 total time=   0.1s\n",
      "[CV 4/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.689 total time=   0.1s\n",
      "[CV 5/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.651 total time=   0.1s\n",
      "[CV 1/5] END .C=100, gamma=scale, kernel=linear;, score=0.613 total time=   1.1s\n",
      "[CV 2/5] END .C=100, gamma=scale, kernel=linear;, score=0.654 total time=   0.7s\n",
      "[CV 3/5] END .C=100, gamma=scale, kernel=linear;, score=0.622 total time=   0.5s\n",
      "[CV 4/5] END .C=100, gamma=scale, kernel=linear;, score=0.669 total time=   1.1s\n",
      "[CV 5/5] END .C=100, gamma=scale, kernel=linear;, score=0.617 total time=   1.1s\n",
      "[CV 1/5] END ...C=100, gamma=scale, kernel=poly;, score=0.711 total time=   0.1s\n",
      "[CV 2/5] END ...C=100, gamma=scale, kernel=poly;, score=0.674 total time=   0.1s\n",
      "[CV 3/5] END ...C=100, gamma=scale, kernel=poly;, score=0.681 total time=   0.1s\n",
      "[CV 4/5] END ...C=100, gamma=scale, kernel=poly;, score=0.687 total time=   0.1s\n",
      "[CV 5/5] END ...C=100, gamma=scale, kernel=poly;, score=0.672 total time=   0.1s\n",
      "[CV 1/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.664 total time=   0.1s\n",
      "[CV 2/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.647 total time=   0.1s\n",
      "[CV 3/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.664 total time=   0.1s\n",
      "[CV 4/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.701 total time=   0.1s\n",
      "[CV 5/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.667 total time=   0.1s\n",
      "[CV 1/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.632 total time=   0.1s\n",
      "[CV 2/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.690 total time=   0.1s\n",
      "[CV 3/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.638 total time=   0.1s\n",
      "[CV 4/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.650 total time=   0.1s\n",
      "[CV 5/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.637 total time=   0.1s\n",
      "[CV 1/5] END C=1000, gamma=scale, kernel=linear;, score=0.588 total time=  15.5s\n",
      "[CV 2/5] END C=1000, gamma=scale, kernel=linear;, score=0.662 total time=  14.9s\n",
      "[CV 3/5] END C=1000, gamma=scale, kernel=linear;, score=0.618 total time=  17.2s\n",
      "[CV 4/5] END C=1000, gamma=scale, kernel=linear;, score=0.667 total time=  22.4s\n",
      "[CV 5/5] END C=1000, gamma=scale, kernel=linear;, score=0.628 total time=  20.0s\n",
      "[CV 1/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.711 total time=   0.1s\n",
      "[CV 2/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.674 total time=   0.1s\n",
      "[CV 3/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.681 total time=   0.1s\n",
      "[CV 4/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.687 total time=   0.1s\n",
      "[CV 5/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.672 total time=   0.1s\n",
      "[CV 1/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.664 total time=   0.1s\n",
      "[CV 2/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.647 total time=   0.1s\n",
      "[CV 3/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.664 total time=   0.1s\n",
      "[CV 4/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.701 total time=   0.1s\n",
      "[CV 5/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.667 total time=   0.1s\n",
      "[CV 1/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.593 total time=   0.1s\n",
      "[CV 2/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.695 total time=   0.1s\n",
      "[CV 3/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.664 total time=   0.1s\n",
      "[CV 4/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.659 total time=   0.1s\n",
      "[CV 5/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.648 total time=   0.1s\n",
      "Best parameters: {'kernel': 'sigmoid', 'gamma': 'scale', 'C': 1} \n",
      "\n",
      "[LibSVM].*.*.*\n",
      "optimization finished, #iter = 2685\n",
      "obj = -742.866851, rho = 0.683928\n",
      "nSV = 737, nBSV = 675\n",
      "Total nSV = 737\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[  2 353]\n",
      " [ 10 635]] \n",
      "\n",
      "Accuracy: 63.7 \n",
      "\n",
      "F1 Score: 77.8 \n",
      "\n",
      "Balanced accuracy: 77.8 \n",
      "\n",
      "AUC Score: 49.5 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[  6 443]\n",
      " [ 13 805]] \n",
      "\n",
      "Accuracy: 64.0 \n",
      "\n",
      "F1 Score: 77.9 \n",
      "\n",
      "Balanced accuracy: 77.9 \n",
      "\n",
      "AUC Score: 49.9 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = SVClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'C': [1, 10, 100, 1000],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale']\n",
    "    #'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_word2vec, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_word2vec, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_svc_w2v_train, accuracy_svc_w2v_train, f1_svc_w2v_train, balaccuracy_svc_w2v_train, rocauc_svc_w2v_train = classifier.evaluate(X_train_embeddings_word2vec, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_svc_w2v_test, accuracy_svc_w2v_test, f1_svc_w2v_test, balaccuracy_svc_w2v_test, rocauc_svc_w2v_test = classifier.evaluate(X_test_embeddings_word2vec, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:03:36.298464070Z",
     "start_time": "2023-05-22T01:03:31.200979988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END .C=0.1, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .C=0.1, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .C=0.1, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .C=0.1, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .C=0.1, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.63, penalty=l2, solver=liblinear;, score=0.741 total time=   0.0s\n",
      "[CV 2/5] END C=0.63, penalty=l2, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 3/5] END C=0.63, penalty=l2, solver=liblinear;, score=0.776 total time=   0.0s\n",
      "[CV 4/5] END C=0.63, penalty=l2, solver=liblinear;, score=0.759 total time=   0.0s\n",
      "[CV 5/5] END C=0.63, penalty=l2, solver=liblinear;, score=0.758 total time=   0.0s\n",
      "[CV 1/5] END C=0.26, penalty=none, solver=lbfgs;, score=0.605 total time=   0.0s\n",
      "[CV 2/5] END C=0.26, penalty=none, solver=lbfgs;, score=0.664 total time=   0.0s\n",
      "[CV 3/5] END C=0.26, penalty=none, solver=lbfgs;, score=0.610 total time=   0.0s\n",
      "[CV 4/5] END C=0.26, penalty=none, solver=lbfgs;, score=0.682 total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=0.26, penalty=none, solver=lbfgs;, score=0.606 total time=   0.0s\n",
      "[CV 1/5] END C=0.35000000000000003, penalty=l2, solver=saga;, score=0.761 total time=   0.1s\n",
      "[CV 2/5] END C=0.35000000000000003, penalty=l2, solver=saga;, score=0.785 total time=   0.1s\n",
      "[CV 3/5] END C=0.35000000000000003, penalty=l2, solver=saga;, score=0.779 total time=   0.1s\n",
      "[CV 4/5] END C=0.35000000000000003, penalty=l2, solver=saga;, score=0.770 total time=   0.1s\n",
      "[CV 5/5] END C=0.35000000000000003, penalty=l2, solver=saga;, score=0.774 total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ...C=0.0, penalty=none, solver=sag;, score=0.614 total time=   0.2s\n",
      "[CV 2/5] END ...C=0.0, penalty=none, solver=sag;, score=0.686 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END ...C=0.0, penalty=none, solver=sag;, score=0.625 total time=   0.2s\n",
      "[CV 4/5] END ...C=0.0, penalty=none, solver=sag;, score=0.659 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END ...C=0.0, penalty=none, solver=sag;, score=0.606 total time=   0.2s\n",
      "[CV 1/5] END C=0.8, penalty=none, solver=newton-cg;, score=0.605 total time=   0.0s\n",
      "[CV 2/5] END C=0.8, penalty=none, solver=newton-cg;, score=0.669 total time=   0.0s\n",
      "[CV 3/5] END C=0.8, penalty=none, solver=newton-cg;, score=0.610 total time=   0.1s\n",
      "[CV 4/5] END C=0.8, penalty=none, solver=newton-cg;, score=0.688 total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=0.8, penalty=none, solver=newton-cg;, score=0.598 total time=   0.0s\n",
      "[CV 1/5] END C=0.61, penalty=none, solver=lbfgs;, score=0.605 total time=   0.0s\n",
      "[CV 2/5] END C=0.61, penalty=none, solver=lbfgs;, score=0.664 total time=   0.0s\n",
      "[CV 3/5] END C=0.61, penalty=none, solver=lbfgs;, score=0.610 total time=   0.0s\n",
      "[CV 4/5] END C=0.61, penalty=none, solver=lbfgs;, score=0.682 total time=   0.0s\n",
      "[CV 5/5] END C=0.61, penalty=none, solver=lbfgs;, score=0.606 total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ....C=0.59, penalty=l2, solver=sag;, score=0.740 total time=   0.0s\n",
      "[CV 2/5] END ....C=0.59, penalty=l2, solver=sag;, score=0.787 total time=   0.0s\n",
      "[CV 3/5] END ....C=0.59, penalty=l2, solver=sag;, score=0.774 total time=   0.0s\n",
      "[CV 4/5] END ....C=0.59, penalty=l2, solver=sag;, score=0.759 total time=   0.0s\n",
      "[CV 5/5] END ....C=0.59, penalty=l2, solver=sag;, score=0.758 total time=   0.0s\n",
      "[CV 1/5] END C=0.11, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 2/5] END C=0.11, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 3/5] END C=0.11, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 4/5] END C=0.11, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 5/5] END C=0.11, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=0.19, penalty=none, solver=lbfgs;, score=0.605 total time=   0.0s\n",
      "[CV 2/5] END C=0.19, penalty=none, solver=lbfgs;, score=0.664 total time=   0.0s\n",
      "[CV 3/5] END C=0.19, penalty=none, solver=lbfgs;, score=0.610 total time=   0.0s\n",
      "[CV 4/5] END C=0.19, penalty=none, solver=lbfgs;, score=0.682 total time=   0.0s\n",
      "[CV 5/5] END C=0.19, penalty=none, solver=lbfgs;, score=0.606 total time=   0.0s\n",
      "[CV 1/5] END C=0.43, penalty=none, solver=newton-cg;, score=0.605 total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=0.43, penalty=none, solver=newton-cg;, score=0.669 total time=   0.1s\n",
      "[CV 3/5] END C=0.43, penalty=none, solver=newton-cg;, score=0.610 total time=   0.1s\n",
      "[CV 4/5] END C=0.43, penalty=none, solver=newton-cg;, score=0.688 total time=   0.0s\n",
      "[CV 5/5] END C=0.43, penalty=none, solver=newton-cg;, score=0.598 total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ....C=0.6, penalty=l1, solver=saga;, score=0.780 total time=   0.2s\n",
      "[CV 2/5] END ....C=0.6, penalty=l1, solver=saga;, score=0.784 total time=   0.2s\n",
      "[CV 3/5] END ....C=0.6, penalty=l1, solver=saga;, score=0.780 total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END ....C=0.6, penalty=l1, solver=saga;, score=0.784 total time=   0.3s\n",
      "[CV 5/5] END ....C=0.6, penalty=l1, solver=saga;, score=0.780 total time=   0.2s\n",
      "[CV 1/5] END C=0.23, penalty=l2, solver=liblinear;, score=0.785 total time=   0.0s\n",
      "[CV 2/5] END C=0.23, penalty=l2, solver=liblinear;, score=0.785 total time=   0.0s\n",
      "[CV 3/5] END C=0.23, penalty=l2, solver=liblinear;, score=0.783 total time=   0.0s\n",
      "[CV 4/5] END C=0.23, penalty=l2, solver=liblinear;, score=0.779 total time=   0.0s\n",
      "[CV 5/5] END C=0.23, penalty=l2, solver=liblinear;, score=0.777 total time=   0.0s\n",
      "[CV 1/5] END C=0.64, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.64, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.64, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.64, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.64, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ...C=0.48, penalty=l1, solver=saga;, score=0.784 total time=   0.2s\n",
      "[CV 2/5] END ...C=0.48, penalty=l1, solver=saga;, score=0.784 total time=   0.2s\n",
      "[CV 3/5] END ...C=0.48, penalty=l1, solver=saga;, score=0.784 total time=   0.2s\n",
      "[CV 4/5] END ...C=0.48, penalty=l1, solver=saga;, score=0.784 total time=   0.2s\n",
      "[CV 5/5] END ...C=0.48, penalty=l1, solver=saga;, score=0.784 total time=   0.1s\n",
      "[CV 1/5] END C=0.91, penalty=l1, solver=liblinear;, score=0.765 total time=   0.0s\n",
      "[CV 2/5] END C=0.91, penalty=l1, solver=liblinear;, score=0.779 total time=   0.0s\n",
      "[CV 3/5] END C=0.91, penalty=l1, solver=liblinear;, score=0.789 total time=   0.0s\n",
      "[CV 4/5] END C=0.91, penalty=l1, solver=liblinear;, score=0.773 total time=   0.0s\n",
      "[CV 5/5] END C=0.91, penalty=l1, solver=liblinear;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END C=0.59, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.59, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.59, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.59, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.59, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.34, penalty=none, solver=newton-cg;, score=0.605 total time=   0.0s\n",
      "[CV 2/5] END C=0.34, penalty=none, solver=newton-cg;, score=0.669 total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=0.34, penalty=none, solver=newton-cg;, score=0.610 total time=   0.1s\n",
      "[CV 4/5] END C=0.34, penalty=none, solver=newton-cg;, score=0.688 total time=   0.0s\n",
      "[CV 5/5] END C=0.34, penalty=none, solver=newton-cg;, score=0.598 total time=   0.0s\n",
      "[CV 1/5] END ...C=0.25, penalty=l1, solver=saga;, score=0.784 total time=   0.1s\n",
      "[CV 2/5] END ...C=0.25, penalty=l1, solver=saga;, score=0.784 total time=   0.0s\n",
      "[CV 3/5] END ...C=0.25, penalty=l1, solver=saga;, score=0.784 total time=   0.0s\n",
      "[CV 4/5] END ...C=0.25, penalty=l1, solver=saga;, score=0.784 total time=   0.0s\n",
      "[CV 5/5] END ...C=0.25, penalty=l1, solver=saga;, score=0.784 total time=   0.0s\n",
      "[CV 1/5] END C=0.35000000000000003, penalty=l2, solver=lbfgs;, score=0.761 total time=   0.0s\n",
      "[CV 2/5] END C=0.35000000000000003, penalty=l2, solver=lbfgs;, score=0.785 total time=   0.0s\n",
      "[CV 3/5] END C=0.35000000000000003, penalty=l2, solver=lbfgs;, score=0.779 total time=   0.0s\n",
      "[CV 4/5] END C=0.35000000000000003, penalty=l2, solver=lbfgs;, score=0.770 total time=   0.0s\n",
      "[CV 5/5] END C=0.35000000000000003, penalty=l2, solver=lbfgs;, score=0.774 total time=   0.0s\n",
      "Best parameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 0.11} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[  0 355]\n",
      " [  0 645]] \n",
      "\n",
      "Accuracy: 64.5 \n",
      "\n",
      "F1 Score: 78.4 \n",
      "\n",
      "Balanced accuracy: 78.4 \n",
      "\n",
      "AUC Score: 50.0 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[  0 449]\n",
      " [  0 818]] \n",
      "\n",
      "Accuracy: 64.6 \n",
      "\n",
      "F1 Score: 78.5 \n",
      "\n",
      "Balanced accuracy: 78.5 \n",
      "\n",
      "AUC Score: 50.0 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "15 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.76350243 0.63347241 0.77392072 0.6380668  0.63407572\n",
      " 0.63347241 0.76362977 0.78419453 0.63347241 0.63407572 0.78197049\n",
      " 0.78186526        nan 0.78419453 0.77364069        nan 0.63407572\n",
      " 0.78419453 0.77392072]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = LRClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'C' : np.arange(0, 1, 0.01)\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_word2vec, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_word2vec, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_lr_w2v_train, accuracy_lr_w2v_train, f1_lr_w2v_train, balaccuracy_lr_w2v_train, rocauc_lr_w2v_train = classifier.evaluate(X_train_embeddings_word2vec, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_lr_w2v_test, accuracy_lr_w2v_test, f1_lr_w2v_test, balaccuracy_lr_w2v_test, rocauc_lr_w2v_test = classifier.evaluate(X_test_embeddings_word2vec, y_test)\n",
    "print('_______________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Word2Vec + Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Train Loss: 0.6557 | Val Loss: 0.6495 | F1 Score: 0.7847 | Balanced Accuracy: 0.5000 | AUC: 0.5000\n",
      "Epoch 2/15 | Train Loss: 0.6402 | Val Loss: 0.6499 | F1 Score: 0.7847 | Balanced Accuracy: 0.5000 | AUC: 0.5000\n",
      "Epoch 3/15 | Train Loss: 0.6277 | Val Loss: 0.6518 | F1 Score: 0.7839 | Balanced Accuracy: 0.4999 | AUC: 0.4999\n",
      "Epoch 4/15 | Train Loss: 0.6129 | Val Loss: 0.6555 | F1 Score: 0.7819 | Balanced Accuracy: 0.5007 | AUC: 0.5007\n",
      "Epoch 5/15 | Train Loss: 0.5940 | Val Loss: 0.6617 | F1 Score: 0.7792 | Balanced Accuracy: 0.5088 | AUC: 0.5088\n",
      "Epoch 6/15 | Train Loss: 0.5737 | Val Loss: 0.6694 | F1 Score: 0.7732 | Balanced Accuracy: 0.5107 | AUC: 0.5107\n",
      "Early stopping triggered. No improvement in 5 epochs.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1200x400 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[ 78 277]\n",
      " [  6 639]] \n",
      "\n",
      "Accuracy: 71.7 \n",
      "\n",
      "F1 Score: 81.9 \n",
      "\n",
      "Balanced accuracy: 60.5 \n",
      "\n",
      "AUC Score: 60.5 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[ 31 418]\n",
      " [ 39 779]] \n",
      "\n",
      "Accuracy: 63.9 \n",
      "\n",
      "F1 Score: 77.3 \n",
      "\n",
      "Balanced accuracy: 51.1 \n",
      "\n",
      "AUC Score: 51.1 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# SET PARAMETERS\n",
    "dropout_rate = 0.1\n",
    "epochs_number = 15\n",
    "learning_rate = 0.0001\n",
    "early_stopping = 5\n",
    "# --------------------\n",
    "\n",
    "input_dim = X_train_embeddings_word2vec.shape[1]  # Dimensionality of word embeddings\n",
    "hidden_dim = X_train_embeddings_word2vec.shape[1]*2  # Number of units in the hidden layer\n",
    "\n",
    "classifier = NeuralNetworkClassifier(input_dim, hidden_dim, dropout=dropout_rate)\n",
    "\n",
    "# Fit the model\n",
    "classifier.fit(X_train_embeddings_word2vec, y_train, X_test_embeddings_word2vec, y_test, num_epochs=epochs_number, lr=learning_rate, patience=early_stopping)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_nn_w2v_train, accuracy_nn_w2v_train, f1_nn_w2v_train, balaccuracy_nn_w2v_train, rocauc_nn_w2v_train = classifier.evaluate(X_train_embeddings_word2vec, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_nn_w2v_test, accuracy_nn_w2v_test, f1_nn_w2v_test, balaccuracy_nn_w2v_test, rocauc_nn_w2v_test = classifier.evaluate(X_test_embeddings_word2vec, y_test)\n",
    "print('_______________________________________________________________________')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T01:03:42.049533783Z",
     "start_time": "2023-05-22T01:03:36.300816863Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GPT2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GPT2 + KNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.693 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.688 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.699 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.705 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=9, weights=uniform;, score=0.683 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=3, weights=distance;, score=0.552 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=3, weights=distance;, score=0.550 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=3, weights=distance;, score=0.653 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=3, weights=distance;, score=0.672 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=3, weights=distance;, score=0.582 total time=   0.0s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=7, weights=uniform;, score=0.649 total time=   0.1s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=7, weights=uniform;, score=0.601 total time=   0.1s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=7, weights=uniform;, score=0.674 total time=   0.1s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=7, weights=uniform;, score=0.722 total time=   0.1s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=7, weights=uniform;, score=0.662 total time=   0.1s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.492 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.479 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.505 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.562 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=2, weights=distance;, score=0.493 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=4, weights=uniform;, score=0.433 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=4, weights=uniform;, score=0.398 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=4, weights=uniform;, score=0.514 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=4, weights=uniform;, score=0.595 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=4, weights=uniform;, score=0.498 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=7, weights=uniform;, score=0.659 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=7, weights=uniform;, score=0.652 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=7, weights=uniform;, score=0.676 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=7, weights=uniform;, score=0.752 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=7, weights=uniform;, score=0.679 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=5, weights=uniform;, score=0.661 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=5, weights=uniform;, score=0.614 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=5, weights=uniform;, score=0.667 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=5, weights=uniform;, score=0.749 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=5, weights=uniform;, score=0.664 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=7, weights=distance;, score=0.659 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=7, weights=distance;, score=0.652 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=7, weights=distance;, score=0.676 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=7, weights=distance;, score=0.752 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=7, weights=distance;, score=0.679 total time=   0.0s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=5, weights=distance;, score=0.669 total time=   0.1s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=5, weights=distance;, score=0.584 total time=   0.1s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=5, weights=distance;, score=0.659 total time=   0.1s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=5, weights=distance;, score=0.693 total time=   0.1s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=5, weights=distance;, score=0.614 total time=   0.1s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=3, weights=distance;, score=0.569 total time=   0.1s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=3, weights=distance;, score=0.570 total time=   0.1s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=3, weights=distance;, score=0.609 total time=   0.1s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=3, weights=distance;, score=0.679 total time=   0.1s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=3, weights=distance;, score=0.565 total time=   0.1s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.576 total time=   0.1s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.534 total time=   0.1s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.596 total time=   0.1s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.679 total time=   0.1s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.573 total time=   0.1s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=3, weights=distance;, score=0.552 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=3, weights=distance;, score=0.550 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=3, weights=distance;, score=0.653 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=3, weights=distance;, score=0.672 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=3, weights=distance;, score=0.582 total time=   0.0s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.669 total time=   0.1s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.584 total time=   0.1s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.659 total time=   0.1s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.693 total time=   0.1s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=5, weights=uniform;, score=0.614 total time=   0.1s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.630 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.634 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.657 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.725 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=8, weights=distance;, score=0.669 total time=   0.0s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=4, weights=uniform;, score=0.449 total time=   0.1s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=4, weights=uniform;, score=0.456 total time=   0.1s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=4, weights=uniform;, score=0.433 total time=   0.1s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=4, weights=uniform;, score=0.621 total time=   0.1s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=4, weights=uniform;, score=0.463 total time=   0.1s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=5, weights=distance;, score=0.661 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=5, weights=distance;, score=0.614 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=5, weights=distance;, score=0.667 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=5, weights=distance;, score=0.749 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=5, weights=distance;, score=0.664 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.550 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.522 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.642 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.652 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.591 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.552 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.550 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.653 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.672 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.582 total time=   0.0s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=2, weights=uniform;, score=0.317 total time=   0.1s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=2, weights=uniform;, score=0.302 total time=   0.1s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=2, weights=uniform;, score=0.288 total time=   0.1s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=2, weights=uniform;, score=0.444 total time=   0.1s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=2, weights=uniform;, score=0.352 total time=   0.1s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=7, weights=distance;, score=0.649 total time=   0.1s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=7, weights=distance;, score=0.601 total time=   0.1s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=7, weights=distance;, score=0.674 total time=   0.1s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=7, weights=distance;, score=0.722 total time=   0.1s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=7, weights=distance;, score=0.662 total time=   0.1s\n",
      "Best parameters: {'weights': 'uniform', 'n_neighbors': 9, 'metric': 'euclidean'} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[147 208]\n",
      " [105 540]] \n",
      "\n",
      "Accuracy: 68.7 \n",
      "\n",
      "F1 Score: 77.5 \n",
      "\n",
      "Balanced accuracy: 77.5 \n",
      "\n",
      "AUC Score: 62.6 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[107 342]\n",
      " [181 637]] \n",
      "\n",
      "Accuracy: 58.7 \n",
      "\n",
      "F1 Score: 70.9 \n",
      "\n",
      "Balanced accuracy: 70.9 \n",
      "\n",
      "AUC Score: 50.9 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = KNNClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'weights' : ['uniform','distance'],\n",
    "    'metric' : ['minkowski','euclidean','manhattan']\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_gpt2, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_gpt2, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_knn_gpt2_train, accuracy_knn_gpt2_train, f1_knn_gpt2_train, balaccuracy_knn_gpt2_train, rocauc_knn_gpt2_train = classifier.evaluate(X_train_embeddings_gpt2, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_knn_gpt2_test, accuracy_knn_gpt2_test, f1_knn_gpt2_test, balaccuracy_knn_gpt2_test, rocauc_knn_gpt2_test = classifier.evaluate(X_test_embeddings_gpt2, y_test)\n",
    "print('_______________________________________________________________________')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T01:03:45.270796608Z",
     "start_time": "2023-05-22T01:03:42.048900808Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GPT2 + XGBoost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.712 total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.682 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.695 total time=   1.1s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.669 total time=   1.1s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.696 total time=   1.2s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.659 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.625 total time=   0.5s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.649 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.662 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.667 total time=   0.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.724 total time=   1.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.608 total time=   1.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.669 total time=   1.4s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.667 total time=   1.3s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.636 total time=   1.4s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.761 total time=   4.0s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.744 total time=   4.0s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.753 total time=   4.0s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.766 total time=   4.0s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.741 total time=   4.0s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.769 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.751 total time=   0.3s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.789 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.775 total time=   0.3s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.775 total time=   0.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.646 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.637 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.629 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.620 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.588 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.669 total time=   1.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.688 total time=   1.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.734 total time=   1.4s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.703 total time=   1.4s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.712 total time=   1.4s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.662 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.628 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.639 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.681 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=7, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.652 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.716 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.618 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.718 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.659 total time=   0.9s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.701 total time=   0.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.643 total time=   0.1s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.646 total time=   0.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.659 total time=   0.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.653 total time=   0.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.632 total time=   0.1s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.764 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.765 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.791 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.760 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.766 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.710 total time=   2.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.683 total time=   2.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.742 total time=   2.4s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.693 total time=   2.4s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.714 total time=   2.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.692 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.703 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.730 total time=   0.4s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.678 total time=   0.4s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.698 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.646 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.696 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.640 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.669 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.689 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.710 total time=   1.8s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.681 total time=   1.8s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.679 total time=   1.8s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.662 total time=   1.8s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.619 total time=   1.8s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.609 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.645 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.646 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.682 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.614 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.770 total time=   2.1s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.739 total time=   2.3s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.778 total time=   2.3s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.770 total time=   2.3s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.752 total time=   2.2s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.669 total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.659 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.708 total time=   1.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.667 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.631 total time=   1.1s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.646 total time=   1.1s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.637 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.629 total time=   1.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.625 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=500, objective=reg:squarederror, subsample=0.5;, score=0.599 total time=   1.2s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.669 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.584 total time=   0.5s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.664 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.677 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.622 total time=   0.5s\n",
      "Best parameters: {'subsample': 0.5, 'objective': 'reg:squarederror', 'n_estimators': 50, 'min_child_weight': 1, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.7} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[228 127]\n",
      " [  0 645]] \n",
      "\n",
      "Accuracy: 87.3 \n",
      "\n",
      "F1 Score: 91.0 \n",
      "\n",
      "Balanced accuracy: 91.0 \n",
      "\n",
      "AUC Score: 82.1 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[ 11 438]\n",
      " [ 14 804]] \n",
      "\n",
      "Accuracy: 64.3 \n",
      "\n",
      "F1 Score: 78.1 \n",
      "\n",
      "Balanced accuracy: 78.1 \n",
      "\n",
      "AUC Score: 50.4 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = XGBoostClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'learning_rate': [0.01, 0.1, 0.3, 0.5],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.5, 0.7],\n",
    "    'colsample_bytree': [0.5, 0.7],\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'objective': ['reg:squarederror']\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_gpt2, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_gpt2, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_xgb_gpt2_train, accuracy_xgb_gpt2_train, f1_xgb_gpt2_train, balaccuracy_xgb_gpt2_train, rocauc_xgb_gpt2_train = classifier.evaluate(X_train_embeddings_gpt2, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_xgb_gpt2_test, accuracy_xgb_gpt2_test, f1_xgb_gpt2_test, balaccuracy_xgb_gpt2_test, rocauc_xgb_gpt2_test = classifier.evaluate(X_test_embeddings_gpt2, y_test)\n",
    "print('_______________________________________________________________________')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T01:05:40.285539512Z",
     "start_time": "2023-05-22T01:03:45.270056163Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GPT2 + Random Forest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.674 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.620 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.677 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.705 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.708 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.734 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.701 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.724 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.716 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.750 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.740 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.728 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.752 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.736 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=25;, score=0.755 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.737 total time=   0.2s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.748 total time=   0.2s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.748 total time=   0.2s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.770 total time=   0.2s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.773 total time=   0.2s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.737 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.745 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.752 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.753 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=25;, score=0.763 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.667 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.725 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.595 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.681 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.671 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.718 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.745 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.748 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.740 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.739 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.662 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.654 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.633 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.652 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=10;, score=0.693 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.730 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.725 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.719 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.754 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.734 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.759 total time=   0.2s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.743 total time=   0.2s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.782 total time=   0.2s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.758 total time=   0.2s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.760 total time=   0.2s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.681 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.687 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.636 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.634 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.690 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.757 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.744 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.753 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.745 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=25;, score=0.719 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.651 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.683 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.672 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.677 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.643 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.743 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.725 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.766 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.774 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.775 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.749 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.756 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.773 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.769 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.783 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.735 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.744 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.759 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.735 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.741 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.744 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.730 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.759 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.739 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.734 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.625 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.667 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.667 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.654 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.679 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.774 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.749 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.735 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.787 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.772 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.728 total time=   0.2s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.753 total time=   0.2s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.743 total time=   0.2s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.755 total time=   0.2s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.751 total time=   0.2s\n",
      "Best parameters: {'n_estimators': 25, 'max_features': 5, 'max_depth': 10, 'bootstrap': False} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[325  30]\n",
      " [  0 645]] \n",
      "\n",
      "Accuracy: 97.0 \n",
      "\n",
      "F1 Score: 97.7 \n",
      "\n",
      "Balanced accuracy: 97.7 \n",
      "\n",
      "AUC Score: 95.8 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[ 10 439]\n",
      " [ 31 787]] \n",
      "\n",
      "Accuracy: 62.9 \n",
      "\n",
      "F1 Score: 77.0 \n",
      "\n",
      "Balanced accuracy: 77.0 \n",
      "\n",
      "AUC Score: 49.2 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = RFClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'n_estimators': [10, 25],\n",
    "    'max_features': [5, 10],\n",
    "    'max_depth': [10, 50, None],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_gpt2, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_gpt2, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_rf_gpt2_train, accuracy_rf_gpt2_train, f1_rf_gpt2_train, balaccuracy_rf_gpt2_train, rocauc_rf_gpt2_train = classifier.evaluate(X_train_embeddings_gpt2, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_rf_gpt2_test, accuracy_rf_gpt2_test, f1_rf_gpt2_test, balaccuracy_rf_gpt2_test, rocauc_rf_gpt2_test = classifier.evaluate(X_test_embeddings_gpt2, y_test)\n",
    "print('_______________________________________________________________________')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T01:05:49.318675958Z",
     "start_time": "2023-05-22T01:05:40.285007496Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GPT2 + SVC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:292: UserWarning: The total space of parameters 16 is smaller than n_iter=20. Running 16 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ...C=1, gamma=scale, kernel=linear;, score=0.769 total time=   0.1s\n",
      "[CV 2/5] END ...C=1, gamma=scale, kernel=linear;, score=0.783 total time=   0.1s\n",
      "[CV 3/5] END ...C=1, gamma=scale, kernel=linear;, score=0.784 total time=   0.1s\n",
      "[CV 4/5] END ...C=1, gamma=scale, kernel=linear;, score=0.784 total time=   0.1s\n",
      "[CV 5/5] END ...C=1, gamma=scale, kernel=linear;, score=0.777 total time=   0.1s\n",
      "[CV 1/5] END .....C=1, gamma=scale, kernel=poly;, score=0.777 total time=   0.1s\n",
      "[CV 2/5] END .....C=1, gamma=scale, kernel=poly;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END .....C=1, gamma=scale, kernel=poly;, score=0.784 total time=   0.1s\n",
      "[CV 4/5] END .....C=1, gamma=scale, kernel=poly;, score=0.784 total time=   0.1s\n",
      "[CV 5/5] END .....C=1, gamma=scale, kernel=poly;, score=0.784 total time=   0.1s\n",
      "[CV 1/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.784 total time=   0.2s\n",
      "[CV 2/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.784 total time=   0.2s\n",
      "[CV 3/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.784 total time=   0.2s\n",
      "[CV 4/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.784 total time=   0.2s\n",
      "[CV 5/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.784 total time=   0.2s\n",
      "[CV 1/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.784 total time=   0.2s\n",
      "[CV 2/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.784 total time=   0.2s\n",
      "[CV 3/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.784 total time=   0.2s\n",
      "[CV 4/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.784 total time=   0.2s\n",
      "[CV 5/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.784 total time=   0.1s\n",
      "[CV 1/5] END ..C=10, gamma=scale, kernel=linear;, score=0.677 total time=   0.1s\n",
      "[CV 2/5] END ..C=10, gamma=scale, kernel=linear;, score=0.641 total time=   0.1s\n",
      "[CV 3/5] END ..C=10, gamma=scale, kernel=linear;, score=0.681 total time=   0.1s\n",
      "[CV 4/5] END ..C=10, gamma=scale, kernel=linear;, score=0.644 total time=   0.1s\n",
      "[CV 5/5] END ..C=10, gamma=scale, kernel=linear;, score=0.652 total time=   0.1s\n",
      "[CV 1/5] END ....C=10, gamma=scale, kernel=poly;, score=0.688 total time=   0.1s\n",
      "[CV 2/5] END ....C=10, gamma=scale, kernel=poly;, score=0.662 total time=   0.1s\n",
      "[CV 3/5] END ....C=10, gamma=scale, kernel=poly;, score=0.695 total time=   0.1s\n",
      "[CV 4/5] END ....C=10, gamma=scale, kernel=poly;, score=0.657 total time=   0.1s\n",
      "[CV 5/5] END ....C=10, gamma=scale, kernel=poly;, score=0.688 total time=   0.1s\n",
      "[CV 1/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.703 total time=   0.2s\n",
      "[CV 2/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.677 total time=   0.2s\n",
      "[CV 3/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.700 total time=   0.2s\n",
      "[CV 4/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.688 total time=   0.2s\n",
      "[CV 5/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.709 total time=   0.2s\n",
      "[CV 1/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.742 total time=   0.2s\n",
      "[CV 2/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.760 total time=   0.1s\n",
      "[CV 3/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.763 total time=   0.2s\n",
      "[CV 4/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.761 total time=   0.1s\n",
      "[CV 5/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.778 total time=   0.1s\n",
      "[CV 1/5] END .C=100, gamma=scale, kernel=linear;, score=0.656 total time=   0.2s\n",
      "[CV 2/5] END .C=100, gamma=scale, kernel=linear;, score=0.664 total time=   0.2s\n",
      "[CV 3/5] END .C=100, gamma=scale, kernel=linear;, score=0.614 total time=   0.2s\n",
      "[CV 4/5] END .C=100, gamma=scale, kernel=linear;, score=0.598 total time=   0.2s\n",
      "[CV 5/5] END .C=100, gamma=scale, kernel=linear;, score=0.632 total time=   0.2s\n",
      "[CV 1/5] END ...C=100, gamma=scale, kernel=poly;, score=0.669 total time=   0.1s\n",
      "[CV 2/5] END ...C=100, gamma=scale, kernel=poly;, score=0.637 total time=   0.1s\n",
      "[CV 3/5] END ...C=100, gamma=scale, kernel=poly;, score=0.672 total time=   0.1s\n",
      "[CV 4/5] END ...C=100, gamma=scale, kernel=poly;, score=0.636 total time=   0.2s\n",
      "[CV 5/5] END ...C=100, gamma=scale, kernel=poly;, score=0.672 total time=   0.1s\n",
      "[CV 1/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.679 total time=   0.2s\n",
      "[CV 2/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.643 total time=   0.2s\n",
      "[CV 3/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.686 total time=   0.2s\n",
      "[CV 4/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.641 total time=   0.2s\n",
      "[CV 5/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.667 total time=   0.2s\n",
      "[CV 1/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.682 total time=   0.2s\n",
      "[CV 2/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.657 total time=   0.2s\n",
      "[CV 3/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.647 total time=   0.4s\n",
      "[CV 4/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.667 total time=   0.2s\n",
      "[CV 5/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.639 total time=   0.2s\n",
      "[CV 1/5] END C=1000, gamma=scale, kernel=linear;, score=0.651 total time=   0.2s\n",
      "[CV 2/5] END C=1000, gamma=scale, kernel=linear;, score=0.637 total time=   0.2s\n",
      "[CV 3/5] END C=1000, gamma=scale, kernel=linear;, score=0.598 total time=   0.2s\n",
      "[CV 4/5] END C=1000, gamma=scale, kernel=linear;, score=0.602 total time=   0.2s\n",
      "[CV 5/5] END C=1000, gamma=scale, kernel=linear;, score=0.630 total time=   0.2s\n",
      "[CV 1/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.669 total time=   0.2s\n",
      "[CV 2/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.637 total time=   0.1s\n",
      "[CV 3/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.672 total time=   0.1s\n",
      "[CV 4/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.636 total time=   0.1s\n",
      "[CV 5/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.672 total time=   0.2s\n",
      "[CV 1/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.679 total time=   0.2s\n",
      "[CV 2/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.643 total time=   0.2s\n",
      "[CV 3/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.686 total time=   0.2s\n",
      "[CV 4/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.641 total time=   0.2s\n",
      "[CV 5/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.667 total time=   0.2s\n",
      "[CV 1/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.682 total time=   0.2s\n",
      "[CV 2/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.636 total time=   0.2s\n",
      "[CV 3/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.656 total time=   0.2s\n",
      "[CV 4/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.654 total time=   0.4s\n",
      "[CV 5/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.651 total time=   0.2s\n",
      "Best parameters: {'kernel': 'rbf', 'gamma': 'scale', 'C': 1} \n",
      "\n",
      "[LibSVM].*\n",
      "optimization finished, #iter = 1473\n",
      "obj = -670.006932, rho = 0.652750\n",
      "nSV = 889, nBSV = 536\n",
      "Total nSV = 889\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[  0 355]\n",
      " [  0 645]] \n",
      "\n",
      "Accuracy: 64.5 \n",
      "\n",
      "F1 Score: 78.4 \n",
      "\n",
      "Balanced accuracy: 78.4 \n",
      "\n",
      "AUC Score: 50.0 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[  0 449]\n",
      " [  0 818]] \n",
      "\n",
      "Accuracy: 64.6 \n",
      "\n",
      "F1 Score: 78.5 \n",
      "\n",
      "Balanced accuracy: 78.5 \n",
      "\n",
      "AUC Score: 50.0 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = SVClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'C': [1, 10, 100, 1000],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale']\n",
    "    #'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_gpt2, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_gpt2, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_svc_gpt2_train, accuracy_svc_gpt2_train, f1_svc_gpt2_train, balaccuracy_svc_gpt2_train, rocauc_svc_gpt2_train = classifier.evaluate(X_train_embeddings_gpt2, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_svc_gpt2_test, accuracy_svc_gpt2_test, f1_svc_gpt2_test, balaccuracy_svc_gpt2_test, rocauc_svc_gpt2_test = classifier.evaluate(X_test_embeddings_gpt2, y_test)\n",
    "print('_______________________________________________________________________')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T01:06:04.032916444Z",
     "start_time": "2023-05-22T01:05:49.324220928Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GPT2 + Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END ....C=0.46, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ....C=0.46, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ....C=0.46, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ....C=0.46, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ....C=0.46, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.9400000000000001, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.9400000000000001, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.9400000000000001, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.9400000000000001, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.9400000000000001, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ....C=0.73, penalty=l2, solver=sag;, score=0.773 total time=   0.1s\n",
      "[CV 2/5] END ....C=0.73, penalty=l2, solver=sag;, score=0.785 total time=   0.1s\n",
      "[CV 3/5] END ....C=0.73, penalty=l2, solver=sag;, score=0.780 total time=   0.1s\n",
      "[CV 4/5] END ....C=0.73, penalty=l2, solver=sag;, score=0.784 total time=   0.1s\n",
      "[CV 5/5] END ....C=0.73, penalty=l2, solver=sag;, score=0.784 total time=   0.1s\n",
      "[CV 1/5] END C=0.78, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 2/5] END C=0.78, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 3/5] END C=0.78, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 4/5] END C=0.78, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 5/5] END C=0.78, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 1/5] END C=0.25, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 2/5] END C=0.25, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 3/5] END C=0.25, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 4/5] END C=0.25, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 5/5] END C=0.25, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 1/5] END ....C=0.09, penalty=l2, solver=sag;, score=0.784 total time=   0.1s\n",
      "[CV 2/5] END ....C=0.09, penalty=l2, solver=sag;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END ....C=0.09, penalty=l2, solver=sag;, score=0.784 total time=   0.1s\n",
      "[CV 4/5] END ....C=0.09, penalty=l2, solver=sag;, score=0.784 total time=   0.1s\n",
      "[CV 5/5] END ....C=0.09, penalty=l2, solver=sag;, score=0.784 total time=   0.1s\n",
      "[CV 1/5] END C=0.6, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.6, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.6, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.6, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.6, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ....C=0.66, penalty=l2, solver=sag;, score=0.777 total time=   0.1s\n",
      "[CV 2/5] END ....C=0.66, penalty=l2, solver=sag;, score=0.787 total time=   0.1s\n",
      "[CV 3/5] END ....C=0.66, penalty=l2, solver=sag;, score=0.784 total time=   0.1s\n",
      "[CV 4/5] END ....C=0.66, penalty=l2, solver=sag;, score=0.784 total time=   0.1s\n",
      "[CV 5/5] END ....C=0.66, penalty=l2, solver=sag;, score=0.784 total time=   0.1s\n",
      "[CV 1/5] END C=0.8, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.8, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.8, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.8, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.8, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.48, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.48, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.48, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.48, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.48, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ....C=0.61, penalty=l2, solver=sag;, score=0.777 total time=   0.1s\n",
      "[CV 2/5] END ....C=0.61, penalty=l2, solver=sag;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END ....C=0.61, penalty=l2, solver=sag;, score=0.784 total time=   0.1s\n",
      "[CV 4/5] END ....C=0.61, penalty=l2, solver=sag;, score=0.784 total time=   0.1s\n",
      "[CV 5/5] END ....C=0.61, penalty=l2, solver=sag;, score=0.784 total time=   0.1s\n",
      "[CV 1/5] END C=0.68, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.68, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.68, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.68, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.68, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.5700000000000001, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.5700000000000001, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.5700000000000001, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.5700000000000001, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.5700000000000001, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ......C=0.86, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ......C=0.86, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ......C=0.86, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ......C=0.86, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ......C=0.86, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ...C=0.54, penalty=l1, solver=saga;, score=0.784 total time=   0.1s\n",
      "[CV 2/5] END ...C=0.54, penalty=l1, solver=saga;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END ...C=0.54, penalty=l1, solver=saga;, score=0.784 total time=   0.1s\n",
      "[CV 4/5] END ...C=0.54, penalty=l1, solver=saga;, score=0.784 total time=   0.1s\n",
      "[CV 5/5] END ...C=0.54, penalty=l1, solver=saga;, score=0.784 total time=   0.1s\n",
      "[CV 1/5] END C=0.41000000000000003, penalty=l2, solver=lbfgs;, score=0.780 total time=   0.0s\n",
      "[CV 2/5] END C=0.41000000000000003, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.0s\n",
      "[CV 3/5] END C=0.41000000000000003, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.0s\n",
      "[CV 4/5] END C=0.41000000000000003, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.0s\n",
      "[CV 5/5] END C=0.41000000000000003, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.0s\n",
      "[CV 1/5] END C=0.31, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.31, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.31, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.31, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.31, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ..C=0.59, penalty=l2, solver=lbfgs;, score=0.777 total time=   0.0s\n",
      "[CV 2/5] END ..C=0.59, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.0s\n",
      "[CV 3/5] END ..C=0.59, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.0s\n",
      "[CV 4/5] END ..C=0.59, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.0s\n",
      "[CV 5/5] END ..C=0.59, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.0s\n",
      "[CV 1/5] END C=0.02, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.02, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.02, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.02, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.02, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.9, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.9, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.9, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.9, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.9, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "Best parameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 0.78} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[  0 355]\n",
      " [  0 645]] \n",
      "\n",
      "Accuracy: 64.5 \n",
      "\n",
      "F1 Score: 78.4 \n",
      "\n",
      "Balanced accuracy: 78.4 \n",
      "\n",
      "AUC Score: 50.0 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[  0 449]\n",
      " [  0 818]] \n",
      "\n",
      "Accuracy: 64.6 \n",
      "\n",
      "F1 Score: 78.5 \n",
      "\n",
      "Balanced accuracy: 78.5 \n",
      "\n",
      "AUC Score: 50.0 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "55 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1471, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 464, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.78143181 0.78419453 0.78419453 0.78419453\n",
      "        nan 0.78318547        nan        nan 0.78270731        nan\n",
      "        nan        nan 0.78419453 0.78345318        nan 0.78270731\n",
      "        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = LRClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'C' : np.arange(0, 1, 0.01)\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_gpt2, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_gpt2, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_lr_gpt2_train, accuracy_lr_gpt2_train, f1_lr_gpt2_train, balaccuracy_lr_gpt2_train, rocauc_lr_gpt2_train = classifier.evaluate(X_train_embeddings_gpt2, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_lr_gpt2_test, accuracy_lr_gpt2_test, f1_lr_gpt2_test, balaccuracy_lr_gpt2_test, rocauc_lr_gpt2_test = classifier.evaluate(X_test_embeddings_gpt2, y_test)\n",
    "print('_______________________________________________________________________')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T01:06:07.081711772Z",
     "start_time": "2023-05-22T01:06:04.033203542Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GPT2 + Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Train Loss: 0.6549 | Val Loss: 0.6492 | F1 Score: 0.7847 | Balanced Accuracy: 0.5000 | AUC: 0.5000\n",
      "Epoch 2/15 | Train Loss: 0.6445 | Val Loss: 0.6491 | F1 Score: 0.7847 | Balanced Accuracy: 0.5000 | AUC: 0.5000\n",
      "Epoch 3/15 | Train Loss: 0.6327 | Val Loss: 0.6500 | F1 Score: 0.7847 | Balanced Accuracy: 0.5000 | AUC: 0.5000\n",
      "Epoch 4/15 | Train Loss: 0.6145 | Val Loss: 0.6528 | F1 Score: 0.7753 | Balanced Accuracy: 0.4961 | AUC: 0.4961\n",
      "Epoch 5/15 | Train Loss: 0.5894 | Val Loss: 0.6592 | F1 Score: 0.7587 | Balanced Accuracy: 0.4992 | AUC: 0.4992\n",
      "Epoch 6/15 | Train Loss: 0.5579 | Val Loss: 0.6705 | F1 Score: 0.7499 | Balanced Accuracy: 0.5168 | AUC: 0.5168\n",
      "Epoch 7/15 | Train Loss: 0.5275 | Val Loss: 0.6864 | F1 Score: 0.7406 | Balanced Accuracy: 0.5157 | AUC: 0.5157\n",
      "Early stopping triggered. No improvement in 5 epochs.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1200x400 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[179 176]\n",
      " [ 32 613]] \n",
      "\n",
      "Accuracy: 79.2 \n",
      "\n",
      "F1 Score: 85.5 \n",
      "\n",
      "Balanced accuracy: 72.7 \n",
      "\n",
      "AUC Score: 72.7 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[ 80 369]\n",
      " [120 698]] \n",
      "\n",
      "Accuracy: 61.4 \n",
      "\n",
      "F1 Score: 74.1 \n",
      "\n",
      "Balanced accuracy: 51.6 \n",
      "\n",
      "AUC Score: 51.6 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# SET PARAMETERS\n",
    "dropout_rate = 0.1\n",
    "epochs_number = 15\n",
    "learning_rate = 0.0001\n",
    "early_stopping = 5\n",
    "# --------------------\n",
    "\n",
    "input_dim = X_train_embeddings_gpt2.shape[1]  # Dimensionality of word embeddings\n",
    "hidden_dim = X_train_embeddings_gpt2.shape[1]*2  # Number of units in the hidden layer\n",
    "\n",
    "classifier = NeuralNetworkClassifier(input_dim, hidden_dim, dropout=dropout_rate)\n",
    "\n",
    "# Fit the model\n",
    "classifier.fit(X_train_embeddings_gpt2, y_train, X_test_embeddings_gpt2, y_test, num_epochs=epochs_number, lr=learning_rate, patience=early_stopping)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_nn_gpt2_train, accuracy_nn_gpt2_train, f1_nn_gpt2_train, balaccuracy_nn_gpt2_train, rocauc_nn_gpt2_train = classifier.evaluate(X_train_embeddings_gpt2, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_nn_gpt2_test, accuracy_nn_gpt2_test, f1_nn_gpt2_test, balaccuracy_nn_gpt2_test, rocauc_nn_gpt2_test = classifier.evaluate(X_test_embeddings_gpt2, y_test)\n",
    "print('_______________________________________________________________________')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T01:06:42.468684512Z",
     "start_time": "2023-05-22T01:06:07.015363770Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RoBERTa"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RoBERTa + KNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=2, weights=distance;, score=0.621 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=2, weights=distance;, score=0.630 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=2, weights=distance;, score=0.613 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=2, weights=distance;, score=0.567 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=2, weights=distance;, score=0.601 total time=   0.0s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.659 total time=   0.1s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.688 total time=   0.1s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.698 total time=   0.1s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.599 total time=   0.1s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=4, weights=distance;, score=0.623 total time=   0.1s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=5, weights=distance;, score=0.705 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=5, weights=distance;, score=0.714 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=5, weights=distance;, score=0.711 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=5, weights=distance;, score=0.708 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=5, weights=distance;, score=0.683 total time=   0.0s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=2, weights=uniform;, score=0.491 total time=   0.1s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=2, weights=uniform;, score=0.479 total time=   0.1s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=2, weights=uniform;, score=0.524 total time=   0.1s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=2, weights=uniform;, score=0.423 total time=   0.1s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=2, weights=uniform;, score=0.425 total time=   0.1s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.703 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.736 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.679 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.685 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=6, weights=distance;, score=0.676 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=9, weights=uniform;, score=0.734 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=9, weights=uniform;, score=0.756 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=9, weights=uniform;, score=0.748 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=9, weights=uniform;, score=0.714 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=9, weights=uniform;, score=0.714 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=7, weights=distance;, score=0.738 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=7, weights=distance;, score=0.715 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=7, weights=distance;, score=0.714 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=7, weights=distance;, score=0.696 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=7, weights=distance;, score=0.717 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=5, weights=distance;, score=0.705 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=5, weights=distance;, score=0.714 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=5, weights=distance;, score=0.711 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=5, weights=distance;, score=0.708 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=5, weights=distance;, score=0.683 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=7, weights=distance;, score=0.738 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=7, weights=distance;, score=0.715 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=7, weights=distance;, score=0.714 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=7, weights=distance;, score=0.696 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=7, weights=distance;, score=0.717 total time=   0.0s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=6, weights=distance;, score=0.707 total time=   0.1s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=6, weights=distance;, score=0.731 total time=   0.1s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=6, weights=distance;, score=0.724 total time=   0.1s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=6, weights=distance;, score=0.662 total time=   0.1s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=6, weights=distance;, score=0.678 total time=   0.1s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=7, weights=uniform;, score=0.739 total time=   0.1s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=7, weights=uniform;, score=0.724 total time=   0.1s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=7, weights=uniform;, score=0.750 total time=   0.1s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=7, weights=uniform;, score=0.714 total time=   0.1s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=7, weights=uniform;, score=0.695 total time=   0.1s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=8, weights=uniform;, score=0.705 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=8, weights=uniform;, score=0.698 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=8, weights=uniform;, score=0.681 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=8, weights=uniform;, score=0.664 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=8, weights=uniform;, score=0.644 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.594 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.597 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.597 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.602 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=4, weights=uniform;, score=0.571 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=4, weights=distance;, score=0.672 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=4, weights=distance;, score=0.693 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=4, weights=distance;, score=0.664 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=4, weights=distance;, score=0.642 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=4, weights=distance;, score=0.642 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.672 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.693 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.664 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.642 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=4, weights=distance;, score=0.642 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=7, weights=uniform;, score=0.742 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=7, weights=uniform;, score=0.715 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=7, weights=uniform;, score=0.714 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=7, weights=uniform;, score=0.696 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=7, weights=uniform;, score=0.717 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=5, weights=uniform;, score=0.705 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=5, weights=uniform;, score=0.714 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=5, weights=uniform;, score=0.711 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=5, weights=uniform;, score=0.708 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=5, weights=uniform;, score=0.683 total time=   0.0s\n",
      "[CV 1/5] END metric=minkowski, n_neighbors=3, weights=distance;, score=0.679 total time=   0.0s\n",
      "[CV 2/5] END metric=minkowski, n_neighbors=3, weights=distance;, score=0.725 total time=   0.0s\n",
      "[CV 3/5] END metric=minkowski, n_neighbors=3, weights=distance;, score=0.689 total time=   0.0s\n",
      "[CV 4/5] END metric=minkowski, n_neighbors=3, weights=distance;, score=0.649 total time=   0.0s\n",
      "[CV 5/5] END metric=minkowski, n_neighbors=3, weights=distance;, score=0.640 total time=   0.0s\n",
      "[CV 1/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.679 total time=   0.0s\n",
      "[CV 2/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.725 total time=   0.0s\n",
      "[CV 3/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.689 total time=   0.0s\n",
      "[CV 4/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.649 total time=   0.0s\n",
      "[CV 5/5] END metric=euclidean, n_neighbors=3, weights=uniform;, score=0.640 total time=   0.0s\n",
      "[CV 1/5] END metric=manhattan, n_neighbors=3, weights=distance;, score=0.682 total time=   0.1s\n",
      "[CV 2/5] END metric=manhattan, n_neighbors=3, weights=distance;, score=0.693 total time=   0.1s\n",
      "[CV 3/5] END metric=manhattan, n_neighbors=3, weights=distance;, score=0.693 total time=   0.1s\n",
      "[CV 4/5] END metric=manhattan, n_neighbors=3, weights=distance;, score=0.649 total time=   0.1s\n",
      "[CV 5/5] END metric=manhattan, n_neighbors=3, weights=distance;, score=0.638 total time=   0.1s\n",
      "Best parameters: {'weights': 'uniform', 'n_neighbors': 9, 'metric': 'minkowski'} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[ 94 261]\n",
      " [ 64 581]] \n",
      "\n",
      "Accuracy: 67.5 \n",
      "\n",
      "F1 Score: 78.1 \n",
      "\n",
      "Balanced accuracy: 78.1 \n",
      "\n",
      "AUC Score: 58.3 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[ 67 382]\n",
      " [140 678]] \n",
      "\n",
      "Accuracy: 58.8 \n",
      "\n",
      "F1 Score: 72.2 \n",
      "\n",
      "Balanced accuracy: 72.2 \n",
      "\n",
      "AUC Score: 48.9 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = KNNClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'weights' : ['uniform','distance'],\n",
    "    'metric' : ['minkowski','euclidean','manhattan']\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_roberta, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_roberta, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_knn_roberta_train, accuracy_knn_roberta_train, f1_knn_roberta_train, balaccuracy_knn_roberta_train, rocauc_knn_roberta_train = classifier.evaluate(X_train_embeddings_roberta, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_knn_roberta_test, accuracy_knn_roberta_test, f1_knn_roberta_test, balaccuracy_knn_roberta_test, rocauc_knn_roberta_test = classifier.evaluate(X_test_embeddings_roberta, y_test)\n",
    "print('_______________________________________________________________________')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T01:06:44.953606536Z",
     "start_time": "2023-05-22T01:06:42.414864620Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RoBERTa + XGBoost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.644 total time=   1.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.638 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.614 total time=   1.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.619 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.715 total time=   1.2s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.715 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.711 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.716 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.681 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=9, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.714 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.704 total time=   2.2s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.718 total time=   2.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.688 total time=   2.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.698 total time=   2.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.708 total time=   2.2s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.726 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.697 total time=   0.5s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.749 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.710 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.727 total time=   0.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=9, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.672 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=9, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.672 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=9, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.677 total time=   0.4s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=9, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.681 total time=   0.4s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.3, max_depth=9, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.664 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.672 total time=   1.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.683 total time=   1.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.672 total time=   1.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.594 total time=   1.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.688 total time=   1.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.667 total time=   1.5s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.652 total time=   1.5s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.606 total time=   1.5s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.651 total time=   1.5s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.649 total time=   1.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.785 total time=   0.1s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.777 total time=   0.1s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.784 total time=   0.1s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.780 total time=   0.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.787 total time=   0.1s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.644 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.638 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.614 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.619 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.715 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.759 total time=   1.6s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.746 total time=   1.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.775 total time=   1.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.767 total time=   1.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.770 total time=   1.8s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.765 total time=   1.5s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.757 total time=   1.5s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.770 total time=   1.5s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.775 total time=   1.5s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=200, objective=reg:squarederror, subsample=0.5;, score=0.774 total time=   1.5s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.691 total time=   1.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.635 total time=   1.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.671 total time=   1.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.662 total time=   1.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.3, max_depth=9, min_child_weight=1, n_estimators=500, objective=reg:squarederror, subsample=0.7;, score=0.633 total time=   1.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.667 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.612 total time=   0.5s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.677 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.654 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=3, min_child_weight=3, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.619 total time=   0.5s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.727 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.724 total time=   0.5s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.711 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.721 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=50, objective=reg:squarederror, subsample=0.7;, score=0.727 total time=   0.5s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.702 total time=   1.1s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.664 total time=   1.0s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.632 total time=   1.1s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.628 total time=   1.1s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.669 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.654 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.657 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.615 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.628 total time=   0.9s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.5, max_depth=5, min_child_weight=1, n_estimators=200, objective=reg:squarederror, subsample=0.7;, score=0.627 total time=   0.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.777 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.764 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.784 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.775 total time=   0.4s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.783 total time=   0.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.659 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.640 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.608 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.672 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.5, max_depth=9, min_child_weight=5, n_estimators=100, objective=reg:squarederror, subsample=0.7;, score=0.662 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.769 total time=   0.2s\n",
      "[CV 2/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.763 total time=   0.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.783 total time=   0.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.772 total time=   0.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=50, objective=reg:squarederror, subsample=0.5;, score=0.785 total time=   0.2s\n",
      "[CV 1/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.772 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.772 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.788 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.772 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, objective=reg:squarederror, subsample=0.5;, score=0.772 total time=   0.7s\n",
      "Best parameters: {'subsample': 0.5, 'objective': 'reg:squarederror', 'n_estimators': 50, 'min_child_weight': 3, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.5} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[ 29 326]\n",
      " [  0 645]] \n",
      "\n",
      "Accuracy: 67.4 \n",
      "\n",
      "F1 Score: 79.8 \n",
      "\n",
      "Balanced accuracy: 79.8 \n",
      "\n",
      "AUC Score: 54.1 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[  2 447]\n",
      " [  0 818]] \n",
      "\n",
      "Accuracy: 64.7 \n",
      "\n",
      "F1 Score: 78.5 \n",
      "\n",
      "Balanced accuracy: 78.5 \n",
      "\n",
      "AUC Score: 50.2 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = XGBoostClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'learning_rate': [0.01, 0.1, 0.3, 0.5],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.5, 0.7],\n",
    "    'colsample_bytree': [0.5, 0.7],\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'objective': ['reg:squarederror']\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_roberta, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_roberta, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_xgb_roberta_train, accuracy_xgb_roberta_train, f1_xgb_roberta_train, balaccuracy_xgb_roberta_train, rocauc_xgb_roberta_train = classifier.evaluate(X_train_embeddings_roberta, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_xgb_roberta_test, accuracy_xgb_roberta_test, f1_xgb_roberta_test, balaccuracy_xgb_roberta_test, rocauc_xgb_roberta_test = classifier.evaluate(X_test_embeddings_roberta, y_test)\n",
    "print('_______________________________________________________________________')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T01:08:18.508224668Z",
     "start_time": "2023-05-22T01:06:44.935785144Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RoBERTa + Random Forest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.772 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.757 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.779 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.766 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=25;, score=0.777 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.735 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.712 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.719 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.719 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=5, n_estimators=10;, score=0.732 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.781 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.739 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.721 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.752 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=10;, score=0.721 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.634 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.669 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.701 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.701 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=10;, score=0.723 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.756 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.762 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.755 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.765 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=10, n_estimators=25;, score=0.756 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.723 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.742 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.760 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.753 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=25;, score=0.737 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.682 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.713 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.634 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.669 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=5, n_estimators=10;, score=0.686 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.732 total time=   0.2s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.713 total time=   0.2s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.757 total time=   0.2s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.756 total time=   0.2s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=25;, score=0.757 total time=   0.2s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.760 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.759 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.759 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.772 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=25;, score=0.753 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.629 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.679 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.634 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.705 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=True, max_depth=50, max_features=10, n_estimators=10;, score=0.650 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.711 total time=   0.2s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.767 total time=   0.2s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.751 total time=   0.2s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.741 total time=   0.2s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=10, n_estimators=25;, score=0.749 total time=   0.2s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.671 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.727 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.679 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.618 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=10;, score=0.669 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.763 total time=   0.2s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.744 total time=   0.2s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.771 total time=   0.2s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.762 total time=   0.2s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=25;, score=0.756 total time=   0.2s\n",
      "[CV 1/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.747 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.753 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.756 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.753 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=50, max_features=5, n_estimators=25;, score=0.733 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.700 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.659 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.604 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.667 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=10, n_estimators=10;, score=0.698 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.698 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.629 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.674 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.652 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=10, n_estimators=10;, score=0.612 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.748 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.752 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.734 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.721 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=None, max_features=5, n_estimators=25;, score=0.731 total time=   0.1s\n",
      "[CV 1/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.664 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.644 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.639 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.618 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=True, max_depth=None, max_features=5, n_estimators=10;, score=0.683 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.703 total time=   0.0s\n",
      "[CV 2/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.747 total time=   0.0s\n",
      "[CV 3/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.732 total time=   0.0s\n",
      "[CV 4/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.743 total time=   0.0s\n",
      "[CV 5/5] END bootstrap=True, max_depth=10, max_features=5, n_estimators=10;, score=0.741 total time=   0.0s\n",
      "[CV 1/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.741 total time=   0.1s\n",
      "[CV 2/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.748 total time=   0.1s\n",
      "[CV 3/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.743 total time=   0.1s\n",
      "[CV 4/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.718 total time=   0.1s\n",
      "[CV 5/5] END bootstrap=False, max_depth=10, max_features=10, n_estimators=10;, score=0.732 total time=   0.1s\n",
      "Best parameters: {'n_estimators': 25, 'max_features': 5, 'max_depth': 10, 'bootstrap': False} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[347   8]\n",
      " [  0 645]] \n",
      "\n",
      "Accuracy: 99.2 \n",
      "\n",
      "F1 Score: 99.4 \n",
      "\n",
      "Balanced accuracy: 99.4 \n",
      "\n",
      "AUC Score: 98.9 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[ 13 436]\n",
      " [ 24 794]] \n",
      "\n",
      "Accuracy: 63.7 \n",
      "\n",
      "F1 Score: 77.5 \n",
      "\n",
      "Balanced accuracy: 77.5 \n",
      "\n",
      "AUC Score: 50.0 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = RFClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'n_estimators': [10, 25],\n",
    "    'max_features': [5, 10],\n",
    "    'max_depth': [10, 50, None],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_roberta, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_roberta, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_rf_roberta_train, accuracy_rf_roberta_train, f1_rf_roberta_train, balaccuracy_rf_roberta_train, rocauc_rf_roberta_train = classifier.evaluate(X_train_embeddings_roberta, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_rf_roberta_test, accuracy_rf_roberta_test, f1_rf_roberta_test, balaccuracy_rf_roberta_test, rocauc_rf_roberta_test = classifier.evaluate(X_test_embeddings_roberta, y_test)\n",
    "print('_______________________________________________________________________')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T01:08:26.902812089Z",
     "start_time": "2023-05-22T01:08:18.482891640Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RoBERTa + SVC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:292: UserWarning: The total space of parameters 16 is smaller than n_iter=20. Running 16 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ...C=1, gamma=scale, kernel=linear;, score=0.784 total time=   0.1s\n",
      "[CV 2/5] END ...C=1, gamma=scale, kernel=linear;, score=0.780 total time=   0.1s\n",
      "[CV 3/5] END ...C=1, gamma=scale, kernel=linear;, score=0.784 total time=   0.1s\n",
      "[CV 4/5] END ...C=1, gamma=scale, kernel=linear;, score=0.784 total time=   0.1s\n",
      "[CV 5/5] END ...C=1, gamma=scale, kernel=linear;, score=0.784 total time=   0.1s\n",
      "[CV 1/5] END .....C=1, gamma=scale, kernel=poly;, score=0.784 total time=   0.1s\n",
      "[CV 2/5] END .....C=1, gamma=scale, kernel=poly;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END .....C=1, gamma=scale, kernel=poly;, score=0.784 total time=   0.1s\n",
      "[CV 4/5] END .....C=1, gamma=scale, kernel=poly;, score=0.784 total time=   0.1s\n",
      "[CV 5/5] END .....C=1, gamma=scale, kernel=poly;, score=0.784 total time=   0.1s\n",
      "[CV 1/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.784 total time=   0.2s\n",
      "[CV 2/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.784 total time=   0.2s\n",
      "[CV 3/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.784 total time=   0.1s\n",
      "[CV 4/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.784 total time=   0.1s\n",
      "[CV 5/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.784 total time=   0.1s\n",
      "[CV 1/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.784 total time=   0.1s\n",
      "[CV 2/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.784 total time=   0.1s\n",
      "[CV 4/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.784 total time=   0.1s\n",
      "[CV 5/5] END ..C=1, gamma=scale, kernel=sigmoid;, score=0.784 total time=   0.1s\n",
      "[CV 1/5] END ..C=10, gamma=scale, kernel=linear;, score=0.691 total time=   0.1s\n",
      "[CV 2/5] END ..C=10, gamma=scale, kernel=linear;, score=0.662 total time=   0.1s\n",
      "[CV 3/5] END ..C=10, gamma=scale, kernel=linear;, score=0.683 total time=   0.1s\n",
      "[CV 4/5] END ..C=10, gamma=scale, kernel=linear;, score=0.686 total time=   0.1s\n",
      "[CV 5/5] END ..C=10, gamma=scale, kernel=linear;, score=0.699 total time=   0.1s\n",
      "[CV 1/5] END ....C=10, gamma=scale, kernel=poly;, score=0.784 total time=   0.1s\n",
      "[CV 2/5] END ....C=10, gamma=scale, kernel=poly;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END ....C=10, gamma=scale, kernel=poly;, score=0.784 total time=   0.1s\n",
      "[CV 4/5] END ....C=10, gamma=scale, kernel=poly;, score=0.784 total time=   0.1s\n",
      "[CV 5/5] END ....C=10, gamma=scale, kernel=poly;, score=0.784 total time=   0.1s\n",
      "[CV 1/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.784 total time=   0.1s\n",
      "[CV 2/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.784 total time=   0.1s\n",
      "[CV 4/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.784 total time=   0.2s\n",
      "[CV 5/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.784 total time=   0.1s\n",
      "[CV 1/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.784 total time=   0.1s\n",
      "[CV 2/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.784 total time=   0.2s\n",
      "[CV 4/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.784 total time=   0.1s\n",
      "[CV 5/5] END .C=10, gamma=scale, kernel=sigmoid;, score=0.784 total time=   0.1s\n",
      "[CV 1/5] END .C=100, gamma=scale, kernel=linear;, score=0.633 total time=   0.2s\n",
      "[CV 2/5] END .C=100, gamma=scale, kernel=linear;, score=0.605 total time=   0.2s\n",
      "[CV 3/5] END .C=100, gamma=scale, kernel=linear;, score=0.616 total time=   0.3s\n",
      "[CV 4/5] END .C=100, gamma=scale, kernel=linear;, score=0.599 total time=   0.2s\n",
      "[CV 5/5] END .C=100, gamma=scale, kernel=linear;, score=0.684 total time=   0.2s\n",
      "[CV 1/5] END ...C=100, gamma=scale, kernel=poly;, score=0.773 total time=   0.1s\n",
      "[CV 2/5] END ...C=100, gamma=scale, kernel=poly;, score=0.745 total time=   0.1s\n",
      "[CV 3/5] END ...C=100, gamma=scale, kernel=poly;, score=0.744 total time=   0.1s\n",
      "[CV 4/5] END ...C=100, gamma=scale, kernel=poly;, score=0.772 total time=   0.1s\n",
      "[CV 5/5] END ...C=100, gamma=scale, kernel=poly;, score=0.785 total time=   0.1s\n",
      "[CV 1/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.784 total time=   0.1s\n",
      "[CV 2/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.773 total time=   0.1s\n",
      "[CV 3/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.777 total time=   0.1s\n",
      "[CV 4/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.784 total time=   0.1s\n",
      "[CV 5/5] END ....C=100, gamma=scale, kernel=rbf;, score=0.784 total time=   0.1s\n",
      "[CV 1/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.768 total time=   0.1s\n",
      "[CV 2/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.748 total time=   0.1s\n",
      "[CV 3/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.765 total time=   0.1s\n",
      "[CV 4/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.754 total time=   0.1s\n",
      "[CV 5/5] END C=100, gamma=scale, kernel=sigmoid;, score=0.772 total time=   0.1s\n",
      "[CV 1/5] END C=1000, gamma=scale, kernel=linear;, score=0.600 total time=   0.4s\n",
      "[CV 2/5] END C=1000, gamma=scale, kernel=linear;, score=0.573 total time=   0.5s\n",
      "[CV 3/5] END C=1000, gamma=scale, kernel=linear;, score=0.653 total time=   0.4s\n",
      "[CV 4/5] END C=1000, gamma=scale, kernel=linear;, score=0.568 total time=   0.5s\n",
      "[CV 5/5] END C=1000, gamma=scale, kernel=linear;, score=0.667 total time=   0.6s\n",
      "[CV 1/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.659 total time=   0.1s\n",
      "[CV 2/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.636 total time=   0.1s\n",
      "[CV 3/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.657 total time=   0.1s\n",
      "[CV 4/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.639 total time=   0.1s\n",
      "[CV 5/5] END ..C=1000, gamma=scale, kernel=poly;, score=0.688 total time=   0.1s\n",
      "[CV 1/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.674 total time=   0.2s\n",
      "[CV 2/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.656 total time=   0.2s\n",
      "[CV 3/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.669 total time=   0.2s\n",
      "[CV 4/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.647 total time=   0.2s\n",
      "[CV 5/5] END ...C=1000, gamma=scale, kernel=rbf;, score=0.702 total time=   0.2s\n",
      "[CV 1/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.648 total time=   0.1s\n",
      "[CV 2/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.593 total time=   0.1s\n",
      "[CV 3/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.685 total time=   0.1s\n",
      "[CV 4/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.632 total time=   0.1s\n",
      "[CV 5/5] END C=1000, gamma=scale, kernel=sigmoid;, score=0.582 total time=   0.1s\n",
      "Best parameters: {'kernel': 'poly', 'gamma': 'scale', 'C': 1} \n",
      "\n",
      "[LibSVM]*\n",
      "optimization finished, #iter = 971\n",
      "obj = -709.584749, rho = 0.998691\n",
      "nSV = 801, nBSV = 616\n",
      "Total nSV = 801\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[  0 355]\n",
      " [  0 645]] \n",
      "\n",
      "Accuracy: 64.5 \n",
      "\n",
      "F1 Score: 78.4 \n",
      "\n",
      "Balanced accuracy: 78.4 \n",
      "\n",
      "AUC Score: 50.0 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[  0 449]\n",
      " [  0 818]] \n",
      "\n",
      "Accuracy: 64.6 \n",
      "\n",
      "F1 Score: 78.5 \n",
      "\n",
      "Balanced accuracy: 78.5 \n",
      "\n",
      "AUC Score: 50.0 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = SVClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'C': [1, 10, 100, 1000],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale']\n",
    "    #'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_roberta, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_roberta, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_svc_roberta_train, accuracy_svc_roberta_train, f1_svc_roberta_train, balaccuracy_svc_roberta_train, rocauc_svc_roberta_train = classifier.evaluate(X_train_embeddings_roberta, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_svc_roberta_test, accuracy_svc_roberta_test, f1_svc_roberta_test, balaccuracy_svc_roberta_test, rocauc_svc_roberta_test = classifier.evaluate(X_test_embeddings_roberta, y_test)\n",
    "print('_______________________________________________________________________')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T01:08:39.933979119Z",
     "start_time": "2023-05-22T01:08:26.901157981Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RoBERTa + Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ....C=0.84, penalty=l2, solver=sag;, score=0.779 total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END ....C=0.84, penalty=l2, solver=sag;, score=0.773 total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END ....C=0.84, penalty=l2, solver=sag;, score=0.779 total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END ....C=0.84, penalty=l2, solver=sag;, score=0.784 total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END ....C=0.84, penalty=l2, solver=sag;, score=0.787 total time=   0.4s\n",
      "[CV 1/5] END C=0.7000000000000001, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.7000000000000001, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.7000000000000001, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.7000000000000001, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.7000000000000001, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.15, penalty=none, solver=lbfgs;, score=0.590 total time=   0.1s\n",
      "[CV 2/5] END C=0.15, penalty=none, solver=lbfgs;, score=0.638 total time=   0.1s\n",
      "[CV 3/5] END C=0.15, penalty=none, solver=lbfgs;, score=0.687 total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=0.15, penalty=none, solver=lbfgs;, score=0.623 total time=   0.1s\n",
      "[CV 5/5] END C=0.15, penalty=none, solver=lbfgs;, score=0.715 total time=   0.1s\n",
      "[CV 1/5] END C=0.49, penalty=l2, solver=newton-cg;, score=0.784 total time=   0.0s\n",
      "[CV 2/5] END C=0.49, penalty=l2, solver=newton-cg;, score=0.773 total time=   0.0s\n",
      "[CV 3/5] END C=0.49, penalty=l2, solver=newton-cg;, score=0.784 total time=   0.1s\n",
      "[CV 4/5] END C=0.49, penalty=l2, solver=newton-cg;, score=0.784 total time=   0.1s\n",
      "[CV 5/5] END C=0.49, penalty=l2, solver=newton-cg;, score=0.784 total time=   0.0s\n",
      "[CV 1/5] END C=0.04, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.04, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.04, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.04, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.04, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ....C=0.29, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ....C=0.29, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ....C=0.29, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ....C=0.29, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ....C=0.29, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END .C=0.66, penalty=none, solver=saga;, score=0.765 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END .C=0.66, penalty=none, solver=saga;, score=0.748 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END .C=0.66, penalty=none, solver=saga;, score=0.763 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END .C=0.66, penalty=none, solver=saga;, score=0.758 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END .C=0.66, penalty=none, solver=saga;, score=0.772 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ...C=0.46, penalty=l2, solver=saga;, score=0.784 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END ...C=0.46, penalty=l2, solver=saga;, score=0.773 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END ...C=0.46, penalty=l2, solver=saga;, score=0.784 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END ...C=0.46, penalty=l2, solver=saga;, score=0.784 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END ...C=0.46, penalty=l2, solver=saga;, score=0.784 total time=   0.5s\n",
      "[CV 1/5] END ..C=0.18, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.0s\n",
      "[CV 2/5] END ..C=0.18, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END ..C=0.18, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.0s\n",
      "[CV 4/5] END ..C=0.18, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.0s\n",
      "[CV 5/5] END ..C=0.18, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.0s\n",
      "[CV 1/5] END ..C=0.23, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.0s\n",
      "[CV 2/5] END ..C=0.23, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.0s\n",
      "[CV 3/5] END ..C=0.23, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.0s\n",
      "[CV 4/5] END ..C=0.23, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.0s\n",
      "[CV 5/5] END ..C=0.23, penalty=l2, solver=lbfgs;, score=0.784 total time=   0.0s\n",
      "[CV 1/5] END C=0.35000000000000003, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.35000000000000003, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.35000000000000003, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.35000000000000003, penalty=l1, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.35000000000000003, penalty=l1, solver=sag;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=0.6900000000000001, penalty=l2, solver=saga;, score=0.784 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=0.6900000000000001, penalty=l2, solver=saga;, score=0.773 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=0.6900000000000001, penalty=l2, solver=saga;, score=0.787 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=0.6900000000000001, penalty=l2, solver=saga;, score=0.784 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=0.6900000000000001, penalty=l2, solver=saga;, score=0.784 total time=   0.5s\n",
      "[CV 1/5] END C=0.2, penalty=l2, solver=newton-cg;, score=0.784 total time=   0.1s\n",
      "[CV 2/5] END C=0.2, penalty=l2, solver=newton-cg;, score=0.784 total time=   0.1s\n",
      "[CV 3/5] END C=0.2, penalty=l2, solver=newton-cg;, score=0.784 total time=   0.1s\n",
      "[CV 4/5] END C=0.2, penalty=l2, solver=newton-cg;, score=0.784 total time=   0.1s\n",
      "[CV 5/5] END C=0.2, penalty=l2, solver=newton-cg;, score=0.784 total time=   0.0s\n",
      "[CV 1/5] END C=0.1, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.1, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.1, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.1, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.1, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.43, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.43, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.43, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.43, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.43, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.8200000000000001, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.8200000000000001, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.8200000000000001, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.8200000000000001, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.8200000000000001, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.2, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 2/5] END C=0.2, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 3/5] END C=0.2, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 4/5] END C=0.2, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 5/5] END C=0.2, penalty=l1, solver=liblinear;, score=0.784 total time=   0.0s\n",
      "[CV 1/5] END C=0.22, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.22, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.22, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.22, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.22, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.61, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.61, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.61, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.61, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.61, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.8, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.8, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.8, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.8, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.8, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "Best parameters: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 0.18} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[  0 355]\n",
      " [  0 645]] \n",
      "\n",
      "Accuracy: 64.5 \n",
      "\n",
      "F1 Score: 78.4 \n",
      "\n",
      "Balanced accuracy: 78.4 \n",
      "\n",
      "AUC Score: 50.0 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[  0 449]\n",
      " [  0 818]] \n",
      "\n",
      "Accuracy: 64.6 \n",
      "\n",
      "F1 Score: 78.5 \n",
      "\n",
      "Balanced accuracy: 78.5 \n",
      "\n",
      "AUC Score: 50.0 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "50 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 464, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.78041365        nan 0.65044385 0.78195685        nan        nan\n",
      " 0.76121065 0.78195685 0.78419453 0.78419453        nan 0.78243502\n",
      " 0.78419453        nan        nan        nan 0.78419453        nan\n",
      "        nan        nan]\n",
      "  warnings.warn(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/szymon/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "classifier = LRClassifier()\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "param_distributions = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'C' : np.arange(0, 1, 0.01)\n",
    "}\n",
    "classifier.randomized_search(X_train_embeddings_roberta, y_train, param_distributions)\n",
    "\n",
    "# Train classifier on training data\n",
    "classifier.fit(X_train_embeddings_roberta, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_lr_roberta_train, accuracy_lr_roberta_train, f1_lr_roberta_train, balaccuracy_lr_roberta_train, rocauc_lr_roberta_train = classifier.evaluate(X_train_embeddings_roberta, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_lr_roberta_test, accuracy_lr_roberta_test, f1_lr_roberta_test, balaccuracy_lr_roberta_test, rocauc_lr_roberta_test = classifier.evaluate(X_test_embeddings_roberta, y_test)\n",
    "print('_______________________________________________________________________')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T01:08:50.413713572Z",
     "start_time": "2023-05-22T01:08:39.932072493Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RoBERTa + Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Train Loss: 0.6565 | Val Loss: 0.6509 | F1 Score: 0.7847 | Balanced Accuracy: 0.5000 | AUC: 0.5000\n",
      "Epoch 2/15 | Train Loss: 0.6514 | Val Loss: 0.6511 | F1 Score: 0.7847 | Balanced Accuracy: 0.5000 | AUC: 0.5000\n",
      "Epoch 3/15 | Train Loss: 0.6496 | Val Loss: 0.6509 | F1 Score: 0.7847 | Balanced Accuracy: 0.5000 | AUC: 0.5000\n",
      "Epoch 4/15 | Train Loss: 0.6458 | Val Loss: 0.6508 | F1 Score: 0.7847 | Balanced Accuracy: 0.5000 | AUC: 0.5000\n",
      "Epoch 5/15 | Train Loss: 0.6416 | Val Loss: 0.6509 | F1 Score: 0.7847 | Balanced Accuracy: 0.5000 | AUC: 0.5000\n",
      "Epoch 6/15 | Train Loss: 0.6377 | Val Loss: 0.6517 | F1 Score: 0.7841 | Balanced Accuracy: 0.4994 | AUC: 0.4994\n",
      "Epoch 7/15 | Train Loss: 0.6340 | Val Loss: 0.6529 | F1 Score: 0.7795 | Balanced Accuracy: 0.4961 | AUC: 0.4961\n",
      "Epoch 8/15 | Train Loss: 0.6295 | Val Loss: 0.6543 | F1 Score: 0.7773 | Balanced Accuracy: 0.4974 | AUC: 0.4974\n",
      "Epoch 9/15 | Train Loss: 0.6244 | Val Loss: 0.6569 | F1 Score: 0.7719 | Balanced Accuracy: 0.4999 | AUC: 0.4999\n",
      "Early stopping triggered. No improvement in 5 epochs.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1200x400 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "_______________________________________________________________________\n",
      "EVALUATION\n",
      "\n",
      "----- TRAIN -----\n",
      "\n",
      "Confusion matrix\n",
      " [[ 41 314]\n",
      " [ 16 629]] \n",
      "\n",
      "Accuracy: 67.0 \n",
      "\n",
      "F1 Score: 79.2 \n",
      "\n",
      "Balanced accuracy: 54.5 \n",
      "\n",
      "AUC Score: 54.5 \n",
      "\n",
      "\n",
      "----- TEST -----\n",
      "\n",
      "Confusion matrix\n",
      " [[ 18 431]\n",
      " [ 33 785]] \n",
      "\n",
      "Accuracy: 63.4 \n",
      "\n",
      "F1 Score: 77.2 \n",
      "\n",
      "Balanced accuracy: 50.0 \n",
      "\n",
      "AUC Score: 50.0 \n",
      "\n",
      "_______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# SET PARAMETERS\n",
    "dropout_rate = 0.1\n",
    "epochs_number = 15\n",
    "learning_rate = 0.0001\n",
    "early_stopping = 5\n",
    "# --------------------\n",
    "\n",
    "input_dim = X_train_embeddings_roberta.shape[1]  # Dimensionality of word embeddings\n",
    "hidden_dim = X_train_embeddings_roberta.shape[1]*2  # Number of units in the hidden layer\n",
    "\n",
    "classifier = NeuralNetworkClassifier(input_dim, hidden_dim, dropout=dropout_rate)\n",
    "\n",
    "# Fit the model\n",
    "classifier.fit(X_train_embeddings_roberta, y_train, X_test_embeddings_roberta, y_test, num_epochs=epochs_number, lr=learning_rate, patience=early_stopping)\n",
    "\n",
    "# Evaluate classifier\n",
    "print('\\n')\n",
    "print('\\n_______________________________________________________________________')\n",
    "print('EVALUATION')\n",
    "\n",
    "print('\\n----- TRAIN -----')\n",
    "cm_nn_roberta_train, accuracy_nn_roberta_train, f1_nn_roberta_train, balaccuracy_nn_roberta_train, rocauc_nn_roberta_train = classifier.evaluate(X_train_embeddings_roberta, y_train)\n",
    "\n",
    "print('\\n----- TEST -----')\n",
    "cm_nn_roberta_test, accuracy_nn_roberta_test, f1_nn_roberta_test, balaccuracy_nn_roberta_test, rocauc_nn_roberta_test = classifier.evaluate(X_test_embeddings_roberta, y_test)\n",
    "print('_______________________________________________________________________')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T01:09:38.464715395Z",
     "start_time": "2023-05-22T01:08:50.416675061Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T01:09:38.465052113Z",
     "start_time": "2023-05-22T01:09:38.428136128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model                      Accuracy    F1-Score    Balanced Accuracy    ROC AUC \n",
      "\n",
      " LR+GloVe                       64.6        78.5                 50         50   \n",
      "\n",
      " KNN+GloVe                      60.8        73.8                 50.6       50.6 \n",
      "\n",
      " SVC+GloVe                      64.6        78.5                 50         50   \n",
      "\n",
      " RandomForest+GloVe             61.7        75.9                 48.8       48.8 \n",
      "\n",
      " XGBoost+GloVe                  63.4        77.1                 50.3       50.3 \n",
      "\n",
      " NeuralNetwork+GloVe            62.4        74.4                 53.2       53.2 \n",
      "\n",
      " LR+Word2Vec                    64.6        78.5                 50         50   \n",
      "\n",
      " KNN+Word2Vec                   60.7        73.8                 50.4       50.4 \n",
      "\n",
      " SVC+Word2Vec                   64          77.9                 49.9       49.9 \n",
      "\n",
      " RandomForest + Word2Vec        61.5        74.6                 50.7       50.7 \n",
      "\n",
      " XGBoost+Word2Vec               64.1        77.8                 50.3       50.3 \n",
      "\n",
      " NeuralNetwork+Word2Vec         63.9        77.3                 51.1       51.1 \n",
      "\n",
      " LR+BERT                        58.6        70.5                 51.2       51.2 \n",
      "\n",
      " KNN+BERT                       58.4        71.7                 48.9       48.9 \n",
      "\n",
      " SVC+BERT                       64.6        78.5                 50         50   \n",
      "\n",
      " RandomForest+BERT              64.7        78                   51.5       51.5 \n",
      "\n",
      " XGBoost+BERT                   64.2        78.2                 49.8       49.8 \n",
      "\n",
      " NeuralNetwork+BERT             59          71                   51.2       51.2 \n",
      "\n",
      " LR+RoBERTa                     64.6        78.5                 50         50   \n",
      "\n",
      " KNN+RoBERTa                    58.8        72.2                 48.9       48.9 \n",
      "\n",
      " SVC+RoBERTa                    64.6        78.5                 50         50   \n",
      "\n",
      " RandomForest+RoBERTa           63.7        77.5                 50         50   \n",
      "\n",
      " XGBoost+RoBERTa                64.7        78.5                 50.2       50.2 \n",
      "\n",
      " NeuralNetwork+RoBERTa          63.4        77.2                 50         50   \n",
      "\n",
      " LR+GPT2                        64.6        78.5                 50         50   \n",
      "\n",
      " KNN+GPT2                       58.7        70.9                 50.9       50.9 \n",
      "\n",
      " SVC+GPT2                       64.6        78.5                 50         50   \n",
      "\n",
      " RandomForest+GPT2              62.9        77                   49.2       49.2 \n",
      "\n",
      " XGBoost+GPT2                   64.3        78.1                 50.4       50.4 \n",
      "\n",
      " NeuralNetwork+GPT2             61.4        74.1                 51.6       51.6 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create data\n",
    "data = [[\"LR+GloVe\", accuracy_lr_glove_test, f1_lr_glove_test, balaccuracy_lr_glove_test, rocauc_lr_glove_test],\n",
    "        [\"KNN+GloVe\", accuracy_knn_glove_test, f1_knn_glove_test, balaccuracy_knn_glove_test, rocauc_knn_glove_test],\n",
    "        [\"SVC+GloVe\", accuracy_svc_glove_test, f1_svc_glove_test, balaccuracy_svc_glove_test, rocauc_svc_glove_test],\n",
    "        [\"RandomForest+GloVe\", accuracy_rf_glove_test, f1_rf_glove_test, balaccuracy_rf_glove_test, rocauc_rf_glove_test],\n",
    "        [\"XGBoost+GloVe\", accuracy_xgb_glove_test, f1_xgb_glove_test, balaccuracy_xgb_glove_test, rocauc_xgb_glove_test],\n",
    "        [\"NeuralNetwork+GloVe\", accuracy_nn_glove_test, f1_nn_glove_test, balaccuracy_nn_glove_test, rocauc_nn_glove_test],\n",
    "        [\"LR+Word2Vec\", accuracy_lr_w2v_test, f1_lr_w2v_test, balaccuracy_lr_w2v_test, rocauc_lr_w2v_test],\n",
    "        [\"KNN+Word2Vec\", accuracy_knn_w2v_test, f1_knn_w2v_test, balaccuracy_knn_w2v_test, rocauc_knn_w2v_test],\n",
    "        [\"SVC+Word2Vec\", accuracy_svc_w2v_test, f1_svc_w2v_test, balaccuracy_svc_w2v_test, rocauc_svc_w2v_test],\n",
    "        [\"RandomForest + Word2Vec\", accuracy_rf_w2v_test, f1_rf_w2v_test, balaccuracy_rf_w2v_test, rocauc_rf_w2v_test],\n",
    "        [\"XGBoost+Word2Vec\", accuracy_xgb_w2v_test, f1_xgb_w2v_test, balaccuracy_xgb_w2v_test, rocauc_xgb_w2v_test],\n",
    "        [\"NeuralNetwork+Word2Vec\", accuracy_nn_w2v_test, f1_nn_w2v_test, balaccuracy_nn_w2v_test, rocauc_nn_w2v_test],\n",
    "        [\"LR+BERT\", accuracy_lr_bert_test, f1_lr_bert_test, balaccuracy_lr_bert_test, rocauc_lr_bert_test],\n",
    "        [\"KNN+BERT\", accuracy_knn_bert_test, f1_knn_bert_test, balaccuracy_knn_bert_test, rocauc_knn_bert_test],\n",
    "        [\"SVC+BERT\", accuracy_svc_bert_test, f1_svc_bert_test, balaccuracy_svc_bert_test, rocauc_svc_bert_test],\n",
    "        [\"RandomForest+BERT\", accuracy_rf_bert_test, f1_rf_bert_test, balaccuracy_rf_bert_test, rocauc_rf_bert_test],\n",
    "        [\"XGBoost+BERT\", accuracy_xgb_bert_test, f1_xgb_bert_test, balaccuracy_xgb_bert_test, rocauc_xgb_bert_test],\n",
    "        [\"NeuralNetwork+BERT\", accuracy_nn_bert_test, f1_nn_bert_test, balaccuracy_nn_bert_test, rocauc_nn_bert_test],\n",
    "        [\"LR+RoBERTa\", accuracy_lr_roberta_test, f1_lr_roberta_test, balaccuracy_lr_roberta_test, rocauc_lr_roberta_test],\n",
    "        [\"KNN+RoBERTa\", accuracy_knn_roberta_test, f1_knn_roberta_test, balaccuracy_knn_roberta_test, rocauc_knn_roberta_test],\n",
    "        [\"SVC+RoBERTa\", accuracy_svc_roberta_test, f1_svc_roberta_test, balaccuracy_svc_roberta_test, rocauc_svc_roberta_test],\n",
    "        [\"RandomForest+RoBERTa\", accuracy_rf_roberta_test, f1_rf_roberta_test, balaccuracy_rf_roberta_test, rocauc_rf_roberta_test],\n",
    "        [\"XGBoost+RoBERTa\", accuracy_xgb_roberta_test, f1_xgb_roberta_test, balaccuracy_xgb_roberta_test, rocauc_xgb_roberta_test],\n",
    "        [\"NeuralNetwork+RoBERTa\", accuracy_nn_roberta_test, f1_nn_roberta_test, balaccuracy_nn_roberta_test, rocauc_nn_roberta_test],\n",
    "        [\"LR+GPT2\", accuracy_lr_gpt2_test, f1_lr_gpt2_test, balaccuracy_lr_gpt2_test, rocauc_lr_gpt2_test],\n",
    "        [\"KNN+GPT2\", accuracy_knn_gpt2_test, f1_knn_gpt2_test, balaccuracy_knn_gpt2_test, rocauc_knn_gpt2_test],\n",
    "        [\"SVC+GPT2\", accuracy_svc_gpt2_test, f1_svc_gpt2_test, balaccuracy_svc_gpt2_test, rocauc_svc_gpt2_test],\n",
    "        [\"RandomForest+GPT2\", accuracy_rf_gpt2_test, f1_rf_gpt2_test, balaccuracy_rf_gpt2_test, rocauc_rf_gpt2_test],\n",
    "        [\"XGBoost+GPT2\", accuracy_xgb_gpt2_test, f1_xgb_gpt2_test, balaccuracy_xgb_gpt2_test, rocauc_xgb_gpt2_test],\n",
    "        [\"NeuralNetwork+GPT2\", accuracy_nn_gpt2_test, f1_nn_gpt2_test, balaccuracy_nn_gpt2_test, rocauc_nn_gpt2_test]]\n",
    "  \n",
    "#define header names\n",
    "col_names = [\"Model\", \"Accuracy\", \"F1-Score\", \"Balanced Accuracy\", \"ROC AUC\"]\n",
    "\n",
    "#save results to csv\n",
    "if fast:\n",
    "    with open(\"results_fast_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(col_names)\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "else:\n",
    "    with open(\"results_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\", mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(col_names)\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "#display table\n",
    "print(tabulate(data, headers=col_names, tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\hline\n",
      " Model                   &   Accuracy &   F1-Score &   Balanced Accuracy &   ROC AUC \\\\\n",
      "\\hline\n",
      " LR+GloVe                &       64.6 &       78.5 &                50   &      50   \\\\\n",
      " KNN+GloVe               &       60.8 &       73.8 &                50.6 &      50.6 \\\\\n",
      " SVC+GloVe               &       64.6 &       78.5 &                50   &      50   \\\\\n",
      " RandomForest+GloVe      &       61.7 &       75.9 &                48.8 &      48.8 \\\\\n",
      " XGBoost+GloVe           &       63.4 &       77.1 &                50.3 &      50.3 \\\\\n",
      " NeuralNetwork+GloVe     &       62.4 &       74.4 &                53.2 &      53.2 \\\\\n",
      " LR+Word2Vec             &       64.6 &       78.5 &                50   &      50   \\\\\n",
      " KNN+Word2Vec            &       60.7 &       73.8 &                50.4 &      50.4 \\\\\n",
      " SVC+Word2Vec            &       64   &       77.9 &                49.9 &      49.9 \\\\\n",
      " RandomForest + Word2Vec &       61.5 &       74.6 &                50.7 &      50.7 \\\\\n",
      " XGBoost+Word2Vec        &       64.1 &       77.8 &                50.3 &      50.3 \\\\\n",
      " NeuralNetwork+Word2Vec  &       63.9 &       77.3 &                51.1 &      51.1 \\\\\n",
      " LR+BERT                 &       58.6 &       70.5 &                51.2 &      51.2 \\\\\n",
      " KNN+BERT                &       58.4 &       71.7 &                48.9 &      48.9 \\\\\n",
      " SVC+BERT                &       64.6 &       78.5 &                50   &      50   \\\\\n",
      " RandomForest+BERT       &       64.7 &       78   &                51.5 &      51.5 \\\\\n",
      " XGBoost+BERT            &       64.2 &       78.2 &                49.8 &      49.8 \\\\\n",
      " NeuralNetwork+BERT      &       59   &       71   &                51.2 &      51.2 \\\\\n",
      " LR+RoBERTa              &       64.6 &       78.5 &                50   &      50   \\\\\n",
      " KNN+RoBERTa             &       58.8 &       72.2 &                48.9 &      48.9 \\\\\n",
      " SVC+RoBERTa             &       64.6 &       78.5 &                50   &      50   \\\\\n",
      " RandomForest+RoBERTa    &       63.7 &       77.5 &                50   &      50   \\\\\n",
      " XGBoost+RoBERTa         &       64.7 &       78.5 &                50.2 &      50.2 \\\\\n",
      " NeuralNetwork+RoBERTa   &       63.4 &       77.2 &                50   &      50   \\\\\n",
      " LR+GPT2                 &       64.6 &       78.5 &                50   &      50   \\\\\n",
      " KNN+GPT2                &       58.7 &       70.9 &                50.9 &      50.9 \\\\\n",
      " SVC+GPT2                &       64.6 &       78.5 &                50   &      50   \\\\\n",
      " RandomForest+GPT2       &       62.9 &       77   &                49.2 &      49.2 \\\\\n",
      " XGBoost+GPT2            &       64.3 &       78.1 &                50.4 &      50.4 \\\\\n",
      " NeuralNetwork+GPT2      &       61.4 &       74.1 &                51.6 &      51.6 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "#print(tabulate(data, headers=col_names, tablefmt=\"latex\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T01:10:06.779483117Z",
     "start_time": "2023-05-22T01:10:06.709782207Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "\n",
      " Model       Accuracy    F1-Score \n",
      "\n",
      " GloVe           53.4        33.2 \n",
      "\n",
      " Word2Vec        59          40.7 \n",
      "\n",
      " BERT            52.8        34.6 \n",
      "\n",
      " RoBERTa         51.9        38   \n",
      "\n",
      " GPT2            55.2        37.6 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #create data\n",
    "# lr_results = [\n",
    "#     [\"GloVe\", accuracy_lr_glove_test, f1_lr_glove_test],\n",
    "#     [\"Word2Vec\", accuracy_lr_w2v_test, f1_lr_w2v_test],\n",
    "#     [\"BERT\", accuracy_lr_bert_test, f1_lr_bert_test],\n",
    "#     [\"RoBERTa\", accuracy_lr_roberta_test, f1_lr_roberta_test],\n",
    "#     [\"GPT2\", accuracy_lr_gpt2_test, f1_lr_gpt2_test]\n",
    "# ]\n",
    "#\n",
    "# #define header names\n",
    "# col_names = [\"Model\", \"Accuracy\", \"F1-Score\"]\n",
    "#\n",
    "# #display table\n",
    "# print(\"Logistic Regression\")\n",
    "# # print(tabulate(lr_results, headers=col_names, tablefmt=\"fancy_grid\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T00:11:44.648347738Z",
     "start_time": "2023-05-08T00:11:44.628206561Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "\n",
      " Model       Accuracy    F1-Score \n",
      "\n",
      " GloVe           53.7        34.7 \n",
      "\n",
      " Word2Vec        55.5        40   \n",
      "\n",
      " BERT            53.1        35.7 \n",
      "\n",
      " RoBERTa         52.8        38.2 \n",
      "\n",
      " GPT2            48.5        46.6 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #create data\n",
    "# knn_results = [\n",
    "#     [\"GloVe\", accuracy_knn_glove_test, f1_knn_glove_test],\n",
    "#     [\"Word2Vec\", accuracy_knn_w2v_test, f1_knn_w2v_test],\n",
    "#     [\"BERT\", accuracy_knn_bert_test, f1_knn_bert_test],\n",
    "#     [\"RoBERTa\", accuracy_knn_roberta_test, f1_knn_roberta_test],\n",
    "#     [\"GPT2\", accuracy_knn_gpt2_test, f1_knn_gpt2_test]\n",
    "# ]\n",
    "#\n",
    "# #define header names\n",
    "# col_names = [\"Model\", \"Accuracy\", \"F1-Score\"]\n",
    "#\n",
    "# #display table\n",
    "# print(\"KNN\")\n",
    "# print(tabulate(knn_results, headers=col_names, tablefmt=\"fancy_grid\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T00:11:45.623006697Z",
     "start_time": "2023-05-08T00:11:45.607642365Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classifier\n",
      "\n",
      " Model       Accuracy    F1-Score \n",
      "\n",
      " GloVe           56.1        34.4 \n",
      "\n",
      " Word2Vec        59.3        39.2 \n",
      "\n",
      " BERT            50.4        33   \n",
      "\n",
      " RoBERTa         52.9        36   \n",
      "\n",
      " GPT2            55.6        35.7 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #create data\n",
    "# svc_results = [\n",
    "#     [\"GloVe\", accuracy_svc_glove_test, f1_svc_glove_test],\n",
    "#     [\"Word2Vec\", accuracy_svc_w2v_test, f1_svc_w2v_test],\n",
    "#     [\"BERT\", accuracy_svc_bert_test, f1_svc_bert_test],\n",
    "#     [\"RoBERTa\", accuracy_svc_roberta_test, f1_svc_roberta_test],\n",
    "#     [\"GPT2\", accuracy_svc_gpt2_test, f1_svc_gpt2_test]\n",
    "# ]\n",
    "#\n",
    "# #define header names\n",
    "# col_names = [\"Model\", \"Accuracy\", \"F1-Score\"]\n",
    "#\n",
    "# #display table\n",
    "# print(\"SVM Classifier\")\n",
    "# print(tabulate(svc_results, headers=col_names, tablefmt=\"fancy_grid\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T00:11:46.045799687Z",
     "start_time": "2023-05-08T00:11:46.031440509Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "\n",
      " Model       Accuracy    F1-Score \n",
      "\n",
      " GloVe           61.2        22.9 \n",
      "\n",
      " Word2Vec        62.9        29.9 \n",
      "\n",
      " BERT            57.9        22.4 \n",
      "\n",
      " RoBERTa         61.4        19.7 \n",
      "\n",
      " GPT2            58.2        17.2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #create data\n",
    "# rf_results = [\n",
    "#     [\"GloVe\", accuracy_rf_glove_test, f1_rf_glove_test],\n",
    "#     [\"Word2Vec\", accuracy_rf_w2v_test, f1_rf_w2v_test],\n",
    "#     [\"BERT\", accuracy_rf_bert_test, f1_rf_bert_test],\n",
    "#     [\"RoBERTa\", accuracy_rf_roberta_test, f1_rf_roberta_test],\n",
    "#     [\"GPT2\", accuracy_rf_gpt2_test, f1_rf_gpt2_test]\n",
    "# ]\n",
    "#\n",
    "# #define header names\n",
    "# col_names = [\"Model\", \"Accuracy\", \"F1-Score\"]\n",
    "#\n",
    "# #display table\n",
    "# print(\"Random Forest\")\n",
    "# print(tabulate(rf_results, headers=col_names, tablefmt=\"fancy_grid\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T00:11:46.310402046Z",
     "start_time": "2023-05-08T00:11:46.299016397Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "\n",
      " Model       Accuracy    F1-Score \n",
      "\n",
      " GloVe           53          36.4 \n",
      "\n",
      " Word2Vec        56.3        38.2 \n",
      "\n",
      " BERT            52.3        34.8 \n",
      "\n",
      " RoBERTa         53.9        36.2 \n",
      "\n",
      " GPT2            52.6        35.9 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #create data\n",
    "# xgb_results = [\n",
    "#     [\"GloVe\", accuracy_xgb_glove_test, f1_xgb_glove_test],\n",
    "#     [\"Word2Vec\", accuracy_xgb_w2v_test, f1_xgb_w2v_test],\n",
    "#     [\"BERT\", accuracy_xgb_bert_test, f1_xgb_bert_test],\n",
    "#     [\"RoBERTa\", accuracy_xgb_roberta_test, f1_xgb_roberta_test],\n",
    "#     [\"GPT2\", accuracy_xgb_gpt2_test, f1_xgb_gpt2_test]\n",
    "# ]\n",
    "#\n",
    "# #define header names\n",
    "# col_names = [\"Model\", \"Accuracy\", \"F1-Score\"]\n",
    "#\n",
    "# #display table\n",
    "# print(\"XGBoost\")\n",
    "# print(tabulate(xgb_results, headers=col_names, tablefmt=\"fancy_grid\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T00:11:46.663167632Z",
     "start_time": "2023-05-08T00:11:46.636014024Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork\n",
      "\n",
      " Model       Accuracy    F1-Score \n",
      "\n",
      " GloVe           52.5        37.8 \n",
      "\n",
      " Word2Vec        60.1        40   \n",
      "\n",
      " BERT            59.6        17.9 \n",
      "\n",
      " RoBERTa         62.5         9.5 \n",
      "\n",
      " GPT2            56.4        29   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create data\n",
    "nn_results = [\n",
    "    [\"GloVe\", accuracy_nn_glove_test, f1_nn_glove_test],\n",
    "    [\"Word2Vec\", accuracy_nn_w2v_test, f1_nn_w2v_test],\n",
    "    [\"BERT\", accuracy_nn_bert_test, f1_nn_bert_test],\n",
    "    [\"RoBERTa\", accuracy_nn_roberta_test, f1_nn_roberta_test],\n",
    "    [\"GPT2\", accuracy_nn_gpt2_test, f1_nn_gpt2_test]\n",
    "]\n",
    "\n",
    "#define header names\n",
    "col_names = [\"Model\", \"Accuracy\", \"F1-Score\"]\n",
    "\n",
    "#display table\n",
    "print(\"NeuralNetwork\")\n",
    "print(tabulate(nn_results, headers=col_names, tablefmt=\"fancy_grid\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T00:11:47.147217610Z",
     "start_time": "2023-05-08T00:11:47.088596164Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24t1eSrlFLK6"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
