{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOddu0jEzYmgNIVZFmFnlc5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT word embedding + KNN\n",
        "Embedding sepparated (outside of training loops)"
      ],
      "metadata": {
        "id": "vpFId1mjCySe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrZu8iBOnAZ4",
        "outputId": "5374ce57-c2a2-4978-c899-5188416a5790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.3\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as functional\n",
        "import matplotlib.pyplot as plt\n",
        "import transformers\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "import gc\n",
        "from transformers import BertModel\n",
        "from sklearn.metrics import roc_auc_score,f1_score\n",
        "import time\n",
        "import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "import re,string,unicodedata\n",
        "import nltk\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ],
      "metadata": {
        "id": "TCU7NPLUuQrU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "fake = pd.read_csv('/content/drive/MyDrive/thesis-data/Fake.csv')\n",
        "true = pd.read_csv('/content/drive/MyDrive/thesis-data/True.csv')\n",
        "\n",
        "fake[\"label\"] = 0\n",
        "true[\"label\"] = 1\n",
        "\n",
        "df = pd.concat([fake, true], ignore_index = True)\n",
        "\n",
        "df['text'] = df['title'] + \" \" + df['text']\n",
        "df.drop(columns=['title', 'date', 'subject'], inplace = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaFYAFhluSCU",
        "outputId": "09d99e7f-8aa6-4bda-bcd8-53e4eef9baa6"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "punctuation = list(string.punctuation)\n",
        "stop.update(punctuation)\n",
        "\n",
        "\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "# Removing URL's\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub(r'http\\S+', '', text)\n",
        "#Removing the stopwords from text\n",
        "def remove_stopwords(text):\n",
        "    final_text = []\n",
        "    for i in text.split():\n",
        "        if i.strip().lower() not in stop:\n",
        "            final_text.append(i.strip())\n",
        "    return \" \".join(final_text)\n",
        "    \n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    text = remove_stopwords(text)\n",
        "    return text\n",
        "\n",
        "#Apply function on review column\n",
        "df['text']=df['text'].apply(denoise_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieOrxB_AuTwA",
        "outputId": "d751dc7e-8745-47b8-a271-d6e5e63beafd"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "<ipython-input-59-67ef971f5969>:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_original = df.copy()\n",
        "df = df.sample(frac=1).reset_index(drop=True)[:1000]"
      ],
      "metadata": {
        "id": "3OAKnGLyuVS0"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "\n",
        "class XGBClassifier:\n",
        "    def __init__(self, n_neighbors=2):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        # self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        # self.bert = AutoModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
        "\n",
        "    # def _get_bert_embedding(self, text):\n",
        "    #     input_ids = self.tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=64)\n",
        "    #     input_ids = np.array(input_ids)\n",
        "    #     input_ids = np.expand_dims(input_ids, axis=0)\n",
        "    #     input_ids = torch.tensor(input_ids)\n",
        "\n",
        "    #     with torch.no_grad():\n",
        "    #         outputs = self.bert(input_ids)\n",
        "    #         last_hidden_state = outputs.last_hidden_state\n",
        "    #         last_hidden_state = last_hidden_state[:, 0, :].numpy()\n",
        "\n",
        "    #     return last_hidden_state\n",
        "\n",
        "    def fit(self, X_train_embeddings, y_train):\n",
        "        # X_train_embeddings = []\n",
        "        # for text in X_train:\n",
        "        #     embedding = self._get_bert_embedding(text)\n",
        "        #     X_train_embeddings.append(embedding)\n",
        "        # X_train_embeddings = np.array(X_train_embeddings)\n",
        "        # X_train_embeddings = np.squeeze(X_train_embeddings, axis=1)\n",
        "\n",
        "        self.model = KNeighborsClassifier(n_neighbors=self.n_neighbors)\n",
        "        self.model.fit(X_train_embeddings, y_train)\n",
        "\n",
        "    def predict(self, X_test_embeddings):\n",
        "        # X_test_embeddings = []\n",
        "        # for text in X_test:\n",
        "        #     embedding = self._get_bert_embedding(text)\n",
        "        #     X_test_embeddings.append(embedding)\n",
        "        # X_test_embeddings = np.array(X_test_embeddings)\n",
        "        # X_test_embeddings = np.squeeze(X_test_embeddings, axis=1)\n",
        "\n",
        "        y_pred = self.model.predict(X_test_embeddings)\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def randomized_search(self, X_train_embeddings, y_train, param_distributions, cv=5, n_iter=20):\n",
        "        # X_train_embeddings = []\n",
        "        # for text in X_train:\n",
        "        #     embedding = self._get_bert_embedding(text)\n",
        "        #     X_train_embeddings.append(embedding)\n",
        "        # X_train_embeddings = np.array(X_train_embeddings)\n",
        "        # X_train_embeddings = np.squeeze(X_train_embeddings, axis=1)\n",
        "\n",
        "        self.model = KNeighborsClassifier()\n",
        "        random_search = RandomizedSearchCV(self.model, param_distributions=param_distributions, cv=cv, n_iter=n_iter, verbose=3)\n",
        "        random_search.fit(X_train_embeddings, y_train)\n",
        "\n",
        "        self.n_neighbors = random_search.best_params_['n_neighbors']\n",
        "\n",
        "        self.model = KNeighborsClassifier(n_neighbors=self.n_neighbors)\n",
        "\n",
        "    def evaluate(self, X_test_embeddings, y_test):\n",
        "        y_pred = self.predict(X_test_embeddings)\n",
        "        # acc = \n",
        "        # f1 = \n",
        "        return confusion_matrix(y_test, y_pred), accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "c9uWcB_EuXsK"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "#df = pd.read_csv('data.csv')\n",
        "X = df['text'].tolist()\n",
        "y = df['label'].tolist()\n",
        "\n",
        "# Split data into training and test sets\n",
        "train_size = int(0.8 * len(X))\n",
        "X_train = X[:train_size]\n",
        "y_train = y[:train_size]\n",
        "X_test = X[train_size:]\n",
        "y_test = y[train_size:]\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert = AutoModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
        "\n",
        "def _get_bert_embedding(text):\n",
        "    input_ids = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=512)\n",
        "    input_ids = np.array(input_ids)\n",
        "    input_ids = np.expand_dims(input_ids, axis=0)\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = bert(input_ids)\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "        last_hidden_state = last_hidden_state[:, 0, :].numpy()\n",
        "\n",
        "    return last_hidden_state\n",
        "\n",
        "X_train_embeddings = []\n",
        "for text in X_train:\n",
        "    embedding = _get_bert_embedding(text)\n",
        "    X_train_embeddings.append(embedding)\n",
        "X_train_embeddings = np.array(X_train_embeddings)\n",
        "X_train_embeddings = np.squeeze(X_train_embeddings, axis=1)\n",
        "\n",
        "X_test_embeddings = []\n",
        "for text in X_test:\n",
        "    embedding = _get_bert_embedding(text)\n",
        "    X_test_embeddings.append(embedding)\n",
        "X_test_embeddings = np.array(X_test_embeddings)\n",
        "X_test_embeddings = np.squeeze(X_test_embeddings, axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbSRaebguY2a",
        "outputId": "6eccd73b-7b47-419d-9cef-9efafd5f7005"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 5 is smaller than n_iter=20. Running 5 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "[CV 1/5] END .....................n_neighbors=2;, score=0.969 total time=   0.0s\n",
            "[CV 2/5] END .....................n_neighbors=2;, score=0.938 total time=   0.0s\n",
            "[CV 3/5] END .....................n_neighbors=2;, score=0.944 total time=   0.0s\n",
            "[CV 4/5] END .....................n_neighbors=2;, score=0.950 total time=   0.0s\n",
            "[CV 5/5] END .....................n_neighbors=2;, score=0.969 total time=   0.0s\n",
            "[CV 1/5] END .....................n_neighbors=3;, score=0.975 total time=   0.0s\n",
            "[CV 2/5] END .....................n_neighbors=3;, score=0.956 total time=   0.0s\n",
            "[CV 3/5] END .....................n_neighbors=3;, score=0.938 total time=   0.0s\n",
            "[CV 4/5] END .....................n_neighbors=3;, score=0.950 total time=   0.0s\n",
            "[CV 5/5] END .....................n_neighbors=3;, score=0.950 total time=   0.0s\n",
            "[CV 1/5] END .....................n_neighbors=5;, score=0.975 total time=   0.0s\n",
            "[CV 2/5] END .....................n_neighbors=5;, score=0.956 total time=   0.0s\n",
            "[CV 3/5] END .....................n_neighbors=5;, score=0.925 total time=   0.0s\n",
            "[CV 4/5] END .....................n_neighbors=5;, score=0.969 total time=   0.0s\n",
            "[CV 5/5] END .....................n_neighbors=5;, score=0.963 total time=   0.0s\n",
            "[CV 1/5] END .....................n_neighbors=7;, score=0.975 total time=   0.0s\n",
            "[CV 2/5] END .....................n_neighbors=7;, score=0.969 total time=   0.0s\n",
            "[CV 3/5] END .....................n_neighbors=7;, score=0.906 total time=   0.0s\n",
            "[CV 4/5] END .....................n_neighbors=7;, score=0.950 total time=   0.0s\n",
            "[CV 5/5] END .....................n_neighbors=7;, score=0.981 total time=   0.0s\n",
            "[CV 1/5] END .....................n_neighbors=9;, score=0.969 total time=   0.0s\n",
            "[CV 2/5] END .....................n_neighbors=9;, score=0.963 total time=   0.0s\n",
            "[CV 3/5] END .....................n_neighbors=9;, score=0.925 total time=   0.0s\n",
            "[CV 4/5] END .....................n_neighbors=9;, score=0.950 total time=   0.0s\n",
            "[CV 5/5] END .....................n_neighbors=9;, score=0.988 total time=   0.0s\n",
            "Confusion matrix\n",
            " [[106   2]\n",
            " [  3  89]] \n",
            "Accuracy: 0.975 \n",
            "F1 Score: 0.9726775956284153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import accuracy_score\n",
        "# from sklearn.metrics import f1_score\n",
        "\n",
        "# Instantiate classifier\n",
        "classifier = XGBClassifier()\n",
        "\n",
        "# Perform randomized search over hyperparameters\n",
        "param_distributions = {\n",
        "    'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9]\n",
        "}\n",
        "classifier.randomized_search(X_train_embeddings, y_train, param_distributions)\n",
        "\n",
        "# Train classifier on training data\n",
        "classifier.fit(X_train_embeddings, y_train)\n",
        "\n",
        "# Evaluate classifier on test data\n",
        "conf_matrix, accuracy, f1_sc = classifier.evaluate(X_test_embeddings, y_test)\n",
        "print('Confusion matrix\\n', conf_matrix, '\\nAccuracy:', accuracy, '\\nF1 Score:', f1_sc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HzI3FV7zy6t",
        "outputId": "c4caa9ac-3f07-4915-bc9e-51464a57617c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 8 is smaller than n_iter=20. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "[CV 1/5] END .....................n_neighbors=2;, score=0.969 total time=   0.0s\n",
            "[CV 2/5] END .....................n_neighbors=2;, score=0.938 total time=   0.0s\n",
            "[CV 3/5] END .....................n_neighbors=2;, score=0.944 total time=   0.0s\n",
            "[CV 4/5] END .....................n_neighbors=2;, score=0.950 total time=   0.0s\n",
            "[CV 5/5] END .....................n_neighbors=2;, score=0.969 total time=   0.0s\n",
            "[CV 1/5] END .....................n_neighbors=3;, score=0.975 total time=   0.0s\n",
            "[CV 2/5] END .....................n_neighbors=3;, score=0.956 total time=   0.0s\n",
            "[CV 3/5] END .....................n_neighbors=3;, score=0.938 total time=   0.0s\n",
            "[CV 4/5] END .....................n_neighbors=3;, score=0.950 total time=   0.0s\n",
            "[CV 5/5] END .....................n_neighbors=3;, score=0.950 total time=   0.0s\n",
            "[CV 1/5] END .....................n_neighbors=4;, score=0.975 total time=   0.0s\n",
            "[CV 2/5] END .....................n_neighbors=4;, score=0.963 total time=   0.0s\n",
            "[CV 3/5] END .....................n_neighbors=4;, score=0.931 total time=   0.0s\n",
            "[CV 4/5] END .....................n_neighbors=4;, score=0.950 total time=   0.0s\n",
            "[CV 5/5] END .....................n_neighbors=4;, score=0.963 total time=   0.0s\n",
            "[CV 1/5] END .....................n_neighbors=5;, score=0.975 total time=   0.0s\n",
            "[CV 2/5] END .....................n_neighbors=5;, score=0.956 total time=   0.0s\n",
            "[CV 3/5] END .....................n_neighbors=5;, score=0.925 total time=   0.0s\n",
            "[CV 4/5] END .....................n_neighbors=5;, score=0.969 total time=   0.0s\n",
            "[CV 5/5] END .....................n_neighbors=5;, score=0.963 total time=   0.0s\n",
            "[CV 1/5] END .....................n_neighbors=6;, score=0.975 total time=   0.0s\n",
            "[CV 2/5] END .....................n_neighbors=6;, score=0.963 total time=   0.0s\n",
            "[CV 3/5] END .....................n_neighbors=6;, score=0.931 total time=   0.0s\n",
            "[CV 4/5] END .....................n_neighbors=6;, score=0.963 total time=   0.0s\n",
            "[CV 5/5] END .....................n_neighbors=6;, score=0.981 total time=   0.0s\n",
            "[CV 1/5] END .....................n_neighbors=7;, score=0.975 total time=   0.0s\n",
            "[CV 2/5] END .....................n_neighbors=7;, score=0.969 total time=   0.0s\n",
            "[CV 3/5] END .....................n_neighbors=7;, score=0.906 total time=   0.0s\n",
            "[CV 4/5] END .....................n_neighbors=7;, score=0.950 total time=   0.0s\n",
            "[CV 5/5] END .....................n_neighbors=7;, score=0.981 total time=   0.0s\n",
            "[CV 1/5] END .....................n_neighbors=8;, score=0.981 total time=   0.0s\n",
            "[CV 2/5] END .....................n_neighbors=8;, score=0.969 total time=   0.0s\n",
            "[CV 3/5] END .....................n_neighbors=8;, score=0.919 total time=   0.0s\n",
            "[CV 4/5] END .....................n_neighbors=8;, score=0.963 total time=   0.0s\n",
            "[CV 5/5] END .....................n_neighbors=8;, score=0.988 total time=   0.0s\n",
            "[CV 1/5] END .....................n_neighbors=9;, score=0.969 total time=   0.0s\n",
            "[CV 2/5] END .....................n_neighbors=9;, score=0.963 total time=   0.0s\n",
            "[CV 3/5] END .....................n_neighbors=9;, score=0.925 total time=   0.0s\n",
            "[CV 4/5] END .....................n_neighbors=9;, score=0.950 total time=   0.0s\n",
            "[CV 5/5] END .....................n_neighbors=9;, score=0.988 total time=   0.0s\n",
            "Confusion matrix\n",
            " [[107   1]\n",
            " [  3  89]] \n",
            "Accuracy: 0.98 \n",
            "F1 Score: 0.978021978021978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t2WcKakF0Mss"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}