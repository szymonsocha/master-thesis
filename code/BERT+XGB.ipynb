{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtzzJPat1dQrUg52QvNCGk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT word embedding + XGBoost\n",
        "## First attempt"
      ],
      "metadata": {
        "id": "v-OCAW4OZAIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvOacK4N39wR",
        "outputId": "b72bee83-6ffd-437c-b931-c2eab4343b58"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as functional\n",
        "import matplotlib.pyplot as plt\n",
        "import transformers\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "import gc\n",
        "from transformers import BertModel\n",
        "from sklearn.metrics import roc_auc_score,f1_score\n",
        "import time\n",
        "import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "import re,string,unicodedata\n",
        "import nltk\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ],
      "metadata": {
        "id": "-jDLa8gQ382t"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5x0tUoTq37Ta",
        "outputId": "9bfb5660-47ac-421a-ae35-c36593dfedbe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fake = pd.read_csv('/content/drive/MyDrive/thesis-data/Fake.csv')\n",
        "true = pd.read_csv('/content/drive/MyDrive/thesis-data/True.csv')\n",
        "\n",
        "fake[\"label\"] = 0\n",
        "true[\"label\"] = 1\n",
        "\n",
        "df = pd.concat([fake, true], ignore_index = True)\n",
        "\n",
        "df['text'] = df['title'] + \" \" + df['text']\n",
        "df.drop(columns=['title', 'date', 'subject'], inplace = True)"
      ],
      "metadata": {
        "id": "mbJgnjVA356O"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "punctuation = list(string.punctuation)\n",
        "stop.update(punctuation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRnPpRwR3tGI",
        "outputId": "e03c5203-b6fc-4933-8b33-eff208f70a84"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "# Removing URL's\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub(r'http\\S+', '', text)\n",
        "#Removing the stopwords from text\n",
        "def remove_stopwords(text):\n",
        "    final_text = []\n",
        "    for i in text.split():\n",
        "        if i.strip().lower() not in stop:\n",
        "            final_text.append(i.strip())\n",
        "    return \" \".join(final_text)\n",
        "    \n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    text = remove_stopwords(text)\n",
        "    return text\n",
        "\n",
        "#Apply function on review column\n",
        "df['text']=df['text'].apply(denoise_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QQT46aM3r76",
        "outputId": "f263adbd-3a06-40bc-a83c-a9d831bedfbe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-6e48383d87b9>:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugpfoisd3p9M",
        "outputId": "17b57200-9a4b-41f1-8a3f-5eec6da09b06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_original = df.copy()\n",
        "df = df.sample(frac=1).reset_index(drop=True)[:1000]"
      ],
      "metadata": {
        "id": "vv9WTz5u3odm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# texts = df.text.values\n",
        "# labels = df.label.values\n",
        "\n",
        "text = df['text']\n",
        "target = df['label']"
      ],
      "metadata": {
        "id": "HXcFc8GE3miP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_encode(input_text, max_len):\n",
        "    input_ids = []\n",
        "    attension_masks = []\n",
        "    for text in input_text:\n",
        "        output_dict = tokenizer.encode_plus(\n",
        "            text, \n",
        "            add_special_tokens = True,\n",
        "            truncation=True,\n",
        "            max_length = max_len,\n",
        "            pad_to_max_length = True,\n",
        "            return_attention_mask = True\n",
        "        )\n",
        "        input_ids.append(output_dict['input_ids'])\n",
        "        attension_masks.append(output_dict['attention_mask'])\n",
        "    return np.array(input_ids), np.array(attension_masks)"
      ],
      "metadata": {
        "id": "r6ILgHIz_X1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_ids, train_attention_masks = bert_encode(text, 256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgUJ-yk9_ZEV",
        "outputId": "d3ab19af-3a93-408c-b6c1-0af7908ae423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train an XGBoost model using the BERT embeddings and attention mask features\n",
        "dtrain = xgb.DMatrix(np.concatenate([train_input_ids[:900], train_attention_masks[:900]], axis=1), label=labels[:900])\n",
        "dtest = xgb.DMatrix(np.concatenate([train_input_ids[901:], train_attention_masks[901:]], axis=1), label=labels[901:])\n",
        "params = {\n",
        "    'objective': 'multi:softmax',\n",
        "    'num_class': len(np.unique(labels[:900])),\n",
        "    'eval_metric': 'merror',\n",
        "    'max_depth': 6\n",
        "}\n",
        "model = xgb.train(params, dtrain)\n",
        "\n",
        "# Make predictions on the testing set and evaluate the model\n",
        "y_pred = model.predict(dtest)\n",
        "accuracy = accuracy_score(labels[901:], y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PA2-C9_4PPv",
        "outputId": "cfade8b4-0ab3-4835-e4bf-0bf217038c8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8585858585858586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters = {'objective':['reg:squarederror'],\n",
        "#               'booster':['gbtree','gblinear'],\n",
        "#               'learning_rate': [0.1], \n",
        "#               'max_depth': [7,10,15,20],\n",
        "#               'min_child_weight': [10,15,20,25],\n",
        "#               'colsample_bytree': [0.8, 0.9, 1],\n",
        "#               'n_estimators': [300,400,500,600],\n",
        "#               \"reg_alpha\"   : [0.5,0.2,1],\n",
        "#               \"reg_lambda\"  : [2,3,5],\n",
        "#               \"gamma\"       : [1,2,3]}\n",
        "\n",
        "# xgb_model = XGBRegressor(random_state=30)\n",
        "\n",
        "# grid_obj_xgb = RandomizedSearchCV(xgb_model,parameters, cv=5,n_iter=15,scoring='neg_mean_absolute_error',verbose=5,n_jobs=12)\n",
        "# grid_obj_xgb.fit(df_train, y_train,verbose = 1)\n",
        "\n",
        "# y_pred_train = grid_obj_xgb.predict(df_train)\n",
        "# y_pred_test = grid_obj_xgb.predict(df_test)\n",
        "\n",
        "# err_xgb_train=mean_absolute_error(y_train, y_pred_train, multioutput='raw_values')"
      ],
      "metadata": {
        "id": "vaQjq16DdJBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# balanced_accuracy_list = []\n",
        "# for train, test in sKFold.split(X_train, y_train):\n",
        "#     pipeline = make_pipeline(SMOTETomek(tomek=TomekLinks(sampling_strategy='majority')), rcv_tree)\n",
        "#     model = pipeline.fit(X_train[train], y_train[train])\n",
        "#     best_tree = rcv_tree.best_estimator_\n",
        "#     prediction = best_tree.predict(X_train[test])\n",
        "#     balanced_accuracy_list.append(balanced_accuracy_score(orgytrain[test], prediction))\n",
        "# print(\"DecisionTreeClassifier balanced accuracy: {}\".format(np.mean(balanced_accuracy_list)))"
      ],
      "metadata": {
        "id": "Vh86kY3YX0PS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import random\n",
        "import transformers\n",
        "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "\n",
        "class XGBClassifierWithBERTWrapper:\n",
        "    \n",
        "    def __init__(self, params=None, n_iter=50, cv=5, random_state=None):\n",
        "        self.params = params\n",
        "        self.n_iter = n_iter\n",
        "        self.cv = cv\n",
        "        self.random_state = random_state\n",
        "        self.tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.kfolds = StratifiedKFold(n_splits=self.cv, shuffle=True, random_state=self.random_state)\n",
        "        self.estimator = xgb.XGBClassifier()\n",
        "        self.search = RandomizedSearchCV(estimator=self.estimator, param_distributions=self.params, \n",
        "                                         n_iter=self.n_iter, cv=self.kfolds, scoring='roc_auc', \n",
        "                                         n_jobs=-1, verbose=3, random_state=self.random_state)\n",
        "    \n",
        "    def tokenize(self, X):\n",
        "        tokenized_X = []\n",
        "        max_length = 0\n",
        "        for text in X:\n",
        "            input_ids = torch.tensor(self.tokenizer.encode(text, add_special_tokens=True))\n",
        "            tokenized_X.append(input_ids)\n",
        "            max_length = max(max_length, len(input_ids))\n",
        "        padded_X = []\n",
        "        for input_ids in tokenized_X:\n",
        "            padded_X.append(torch.cat([input_ids, torch.zeros(max_length-len(input_ids), dtype=torch.long)]))\n",
        "        return padded_X\n",
        "    \n",
        "    def train(self, X, y):\n",
        "        tokenized_X = self.tokenize(X)\n",
        "        folds_scores = []\n",
        "        for fold, (train_idx, test_idx) in enumerate(self.kfolds.split(tokenized_X, y)):\n",
        "            train_X = [tokenized_X[idx] for idx in train_idx]\n",
        "            train_y = y[train_idx]\n",
        "            test_X = [tokenized_X[idx] for idx in test_idx]\n",
        "            test_y = y[test_idx]\n",
        "            train_X = [torch.cat(train_X, dim=0).squeeze()]\n",
        "            test_X = [torch.cat(test_X, dim=0).squeeze()]\n",
        "            self.estimator.fit(train_X, train_y)\n",
        "            preds = self.estimator.predict(test_X)\n",
        "            score = roc_auc_score(test_y, preds)\n",
        "            folds_scores.append(score)\n",
        "            print(f\"Fold {fold+1} ROC AUC score: {score}\")\n",
        "        self.cv_score = np.mean(folds_scores)\n",
        "        self.search.fit(tokenized_X, y)\n",
        "        print(f\"Best parameters: {self.search.best_params_}\")\n",
        "    \n",
        "    def predict(self, X):\n",
        "        tokenized_X = self.tokenize(X)\n",
        "        tokenized_X = [torch.cat(tokenized_X, dim=0).squeeze()]\n",
        "        preds = self.estimator.predict(tokenized_X)\n",
        "        return preds"
      ],
      "metadata": {
        "id": "Uvx-XuOfjOJT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an instance of the wrapper with hyperparameters to search over\n",
        "params = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.1, 0.01],\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'subsample': [0.5, 0.75, 1],\n",
        "    'colsample_bytree': [0.5, 0.75, 1],\n",
        "    'gamma': [0, 0.1, 0.2],\n",
        "    'reg_alpha': [0, 0.1, 0.5],\n",
        "    'reg_lambda': [0, 0.1, 0.5]\n",
        "}\n",
        "clf = XGBClassifierWithBERTWrapper(params=params, n_iter=50, cv=5, random_state=42)\n",
        "\n",
        "# load data\n",
        "#data = pd.read_csv('data.csv')\n",
        "X = df['text'].values\n",
        "y = df['label'].values\n",
        "\n",
        "\n",
        "\n",
        "# train the model\n",
        "clf.train(X, y)"
      ],
      "metadata": {
        "id": "H4I7GaUyjSA_",
        "outputId": "c65d53d7-7870-4186-9036-f39b9e44cc5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (656 > 512). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.9/dist-packages/xgboost/data.py:897: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  array = np.array(data)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-eb38dd330f10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-3b171cc6eed0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mtrain_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mtest_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1469\u001b[0m                 \u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1470\u001b[0m             )\n\u001b[0;32m-> 1471\u001b[0;31m             train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[0m\u001b[1;32m   1472\u001b[0m                 \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36m_wrap_evaluation_matrices\u001b[0;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\n\u001b[1;32m    447\u001b[0m     way.\"\"\"\n\u001b[0;32m--> 448\u001b[0;31m     train_dmatrix = create_dmatrix(\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36m_create_dmatrix\u001b[0;34m(self, ref, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_evaluation_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingCallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvalsLog\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical)\u001b[0m\n\u001b[1;32m    741\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m         handle, feature_names, feature_types = dispatch_data_backend(\n\u001b[0m\u001b[1;32m    744\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m             \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36mdispatch_data_backend\u001b[0;34m(data, missing, threads, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_from_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_from_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_from_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_from_list\u001b[0;34m(data, missing, n_threads, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    895\u001b[0m     \u001b[0mfeature_types\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFeatureTypes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m ) -> DispatchedDataBackendReturnType:\n\u001b[0;32m--> 897\u001b[0;31m     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m     \u001b[0m_check_data_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_from_numpy_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_threads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf"
      ],
      "metadata": {
        "id": "hibM9x9wjxJR",
        "outputId": "b6c624da-b133-4d0a-c727-114127099a79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.XGBClassifierWithBERTWrapper at 0x7f07ecbdd2e0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on new data\n",
        "new_data = pd.read_csv('new_data.csv')\n",
        "new_X = new_data['text'].values\n",
        "preds = xgb_wrapper.predict(new_X)"
      ],
      "metadata": {
        "id": "oivozpV3j1sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xu7FwUQTni9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Working wrapper below"
      ],
      "metadata": {
        "id": "pL7vH_n343w3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define a function to perform BERT text embedding\n",
        "def bert_embedding(texts):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for text in texts:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                            text,\n",
        "                            add_special_tokens = True,\n",
        "                            max_length = 64,\n",
        "                            pad_to_max_length = True,\n",
        "                            return_attention_mask = True,\n",
        "                            return_tensors = 'tf'\n",
        "                       )\n",
        "        \n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = tf.concat(input_ids, axis=0)\n",
        "    attention_masks = tf.concat(attention_masks, axis=0)\n",
        "\n",
        "    bert_outputs = bert_model([input_ids, attention_masks])\n",
        "    pooled_output = bert_outputs[1]\n",
        "    \n",
        "    return pooled_output.numpy()\n",
        "\n",
        "# Define the XGBoost text classifier wrapper\n",
        "class XGBTextClassifier:\n",
        "    def __init__(self, params=None):\n",
        "        self.params = params or {}\n",
        "        self.clf = None\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        X_embed = bert_embedding(X)\n",
        "        self.clf = xgb.XGBClassifier(**self.params)\n",
        "        self.clf.fit(X_embed, y)\n",
        "        \n",
        "    def predict(self, X):\n",
        "        X_embed = bert_embedding(X)\n",
        "        y_pred = self.clf.predict(X_embed)\n",
        "        return y_pred\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        X_embed = bert_embedding(X)\n",
        "        y_pred_proba = self.clf.predict_proba(X_embed)\n",
        "        return y_pred_proba\n",
        "    \n",
        "    def get_params(self, deep=True):\n",
        "        return self.params\n",
        "    \n",
        "    def set_params(self, **params):\n",
        "        if 'max_depth' in params:\n",
        "            # Replace 'max_depth' with the appropriate parameter for your version of XGBClassifier\n",
        "            if 'max_depths' in xgb.XGBClassifier().get_params():\n",
        "                params['max_depths'] = params.pop('max_depth')\n",
        "            elif 'max_depth' in xgb.XGBClassifier().get_params():\n",
        "                params['max_depth'] = params.pop('max_depth')\n",
        "        self.params.update(params)\n",
        "        return self\n",
        "\n",
        "# Define the hyperparameter space for randomized search\n",
        "params_space = {\n",
        "    #'learning_rate': np.logspace(-5, 0, 100),\n",
        "    #'max_depth': np.arange(1, 11),\n",
        "    #'n_estimators': np.arange(10, 201, 10),\n",
        "    #'gamma': np.arange(0, 1.1, 0.1)\n",
        "}\n",
        "\n",
        "# Load the dataset\n",
        "#df = pd.read_csv('dataset.csv')\n",
        "X = df['text'].values\n",
        "y = df['label'].values\n",
        "\n",
        "# Define the cross-validation folds\n",
        "skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform randomized search for hyperparameter tuning\n",
        "xgb_clf = XGBTextClassifier()\n",
        "random_search = RandomizedSearchCV(xgb_clf, params_space, cv=skf, scoring='accuracy', n_iter=1)\n",
        "random_search.fit(X, y)\n",
        "\n",
        "# Print the best hyperparameters and the corresponding accuracy score\n",
        "print('Best params:', random_search.best_params_)\n",
        "print('Accuracy score:', random_search.best_score_)"
      ],
      "metadata": {
        "id": "DkXhWIPgnidz",
        "outputId": "4c2303a5-410b-417f-b8cd-b377514ae1ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params: {}\n",
            "Accuracy score: 0.883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nT0ebN7A2VzH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}