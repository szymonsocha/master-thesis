In the present era, marked by the rapid digitization and migration of media to the Internet, information accessibility has become increasingly pervasive, offering manifold benefits. With unobstructed information flow, individuals can stay abreast of new events in near real-time, as the distribution of information no longer entails verification hurdles and participation is open to all. Consequently, news agencies have ceded their information-sharing monopoly to social networks and independent news portals, resulting in a decentralized information landscape that is less susceptible to lobbying biases. The elimination of verification processes further enables the instant dissemination of information, providing further advantages.

The widespread availability of information in the digital age brings about certain advantages, such as the unhindered flow of information and immediate access to news. However, this phenomenon also entails some drawbacks. The lack of verification measures in conjunction with the unrestricted access to information heightens the risk of disseminating misinformation. Consequently, the sources of the new information or those sharing it need not rely on dependable sources or possess the necessary expertise on the subject matter. Furthermore, since every user is capable of contributing to the creation and transmission of new information, traditional methods of quality control become ineffectual when the volume of content is immense.

It is worth defining what false information is. In this paper, the term \textit{fake news} is defined as news that contains false or manipulative information, formulated in such a way as to resemble true information as much as possible. They can be spread through various platforms such as television, print media, online news portals, and social media platforms. Fake news usually aims to exert pressure and implant a specific view on a particular phenomenon among a certain group of people. A well-directed campaign of fake news can have a significant impact on social or political events.

At the receiving end of the information transmission network, lies the responsibility of verifying the authenticity and accuracy of the information received. The sheer volume of information available poses a challenge to the recipient, who must filter out false information to remain well-informed. Besides the desire for knowledge, there are other motivations, such as public health and safety concerns, for the detection and filtering of false information. The COVID-19 pandemic is a pertinent example where the detection of fake news was crucial, given the potential impact on public health. Misinformation, including fake cures and conspiracy theories, rapidly proliferated during the pandemic, leading to confusion and real-life consequences. The spread of such news impeded efforts to control the pandemic and its impact. Therefore, methods to identify and label misinformation can aid in the fight against fake news, thereby enhancing public safety. The search for the optimal approach for predicting fake news is a trending research area, with potential significant implications for shaping the future of humanity.

This paper focuses on examining how different approaches to embedding methods and classification methods impact the performance of models detecting fake news. The following models will be investigated: GloVe, Word2Vec, and language models such as Bidirectional Encoder Representation from Transformers (BERT) developed by Google, Robustly Optimized BERT Approach (RoBERTa), and Generative Pre-trained Transformer 2 (GPT-2).

Among the classification methods, Logistic Regression, K-Nearest Neighbors, Support Vector Machine Classifier, Random Forest, eXtreme Gradient Boosting, and Neural Network were selected for analysis.

This paper, in an unprecedented manner in the literature, analyzes the performance of different combinations of embedding methods with classification methods, examining the combinations between different types of embedding methods and representatives of different types of classification methods.

The first part of the paper will provide a literature review, including the latest research and various approaches to fake news detection. Based on the literature review, research hypotheses will be formulated.
In the second part of the paper, the model architecture will be proposed. The dataset will be described, including its source and data characteristics. The architecture of the embedding models will be explained. The classification algorithms, their principles of operation, and methods of tuning will be described. The research results will be presented, and the findings will be discussed in terms of verifying the research hypotheses.

In the third and final part, the study will be summarized, conclusions will be drawn from the analysis, and suggestions will be made regarding which aspects of this research could be improved in future studies.