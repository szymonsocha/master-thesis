The detection of fake news using machine learning techniques is a widely researched problem in the literature \autocite{Pathak2020}. Researchers have proposed various approaches to address this problem, which can be categorized into three distinct types: traditional methods, deep learning methods, and extensions with language models. The primary objective of these approaches is to enhance the performance of the models to detect fake news as accurately as possible. The classical approach to detecting fake news involves analyzing text length, word frequency, and the use of n-grams \autocite{Shu2017}. Additionally, researchers have proposed using sentiment analysis to improve the performance of the model. It has been found that the sentiment of a text is associated with the type of information conveyed, indicating whether the information is misinformation or not \autocite{rubin2016fake}.

The classical approach to detecting fake news using machine learning techniques is gradually being replaced by deep learning methods. Recent research papers demonstrate that deep learning techniques outperform classical approaches in identifying fake news. For example, Wang et al. achieved superior performance by employing a model based on convolutional neural networks (CNN) as opposed to classical text classification methods \autocite{wang-2017-liar}. Similarly, Rashkin et al. reported comparable results by utilizing the Long Short-Term Memory (LSTM) model \autocite{rashkin-etal-2017-truth}. Furthermore, various machine learning algorithms were evaluated on the same dataset to compare their performance in detecting fake news. The findings indicate that deep learning approaches, such as Deep CNN, LSTM, or RNN, exhibit superior performance over classical machine learning methods. These results are consistent across different datasets, demonstrating the overall superiority of deep learning methods in identifying fake news \autocite{IEEE2021}. 

Moreover, there is a growing body of research indicating that deep learning models can be further enhanced through the incorporation of language models \autocite{Conroy2015}. The authors of this paper address the topic of fake news detection, achieving the best result with Multimodal CNN, with an F1 Score of 0.87. The second best model turned out to be BERT, with an F1 Score of 0.74. BiLSTM, CNN, and SVM followed in subsequent positions, with scores of 0.70, 0.69, and 0.67, respectively. Therefore, BERT proved to be the best unimodal model, while SVM was outperformed by all deep learning models. Notably, one of the most widely used language models is the Bidirectional Encoder Representations from Transformers (BERT) \autocite{Devlin2018}. BERT is a pre-trained model designed to learn the contextual relations between words in unlabeled text. There are several variants of BERT available, with RoBERTa (Robustly optimized BERT approach) being one of the most effective in terms of performance. However, RoBERTa is computationally expensive and time-consuming \autocite{Liu2019}. Another BERT variant is DistilBERT, a lighter version that requires fewer resources and is faster, but has weaker performance \autocite{Sanh2019}. This relationship between language models and deep learning also holds true for the detection of fake news. The incorporation of language models has been shown to improve the performance of existing state-of-the-art models. In comparison to traditional machine learning methods such as SVM, Logistic Regression, Decision Tree, and AdaBoost, language model-based models (DistilBERT, BERT, RoBERTa) have achieved better results on the LIAR dataset. Among the language models, BERT and RoBERTa performed the best with an F1 Score of 0.62, followed by deep learning Conv-HAN model with an F1 Score of 0.59. In this study, I utilize the same dataset. In order to compare the obtained results, I also categorize the levels of fake news in the same manner as Khan et al. This will allow for a comparative analysis of the findings from both studies \autocite{Khan2021}. In their study, Joy et al. compared the performance of BERT and RoBERTa. They concluded that RoBERTa, with an F1 Score of 0.98, outperforms BERT, which achieved an F1 Score of 0.89. Furthermore, they compared their results to those of other papers conducted on the same dataset, where DistilBERT achieved a score of 0.94 and SVM scored 0.93 \autocite{Joy2022}.

In this paper, I will focus on two types of BERT models, Vanilla and RoBERTa, and examine how their utilization impacts the improvement of fake news prediction. Given the relatively small dataset at hand, I have decided to employ RoBERTa as it should not pose computational challenges. The investigation omits the examination of the DistilBERT model, which is lighter and faster but exhibits weaker performance.

As previously discussed, language models have been shown to be effective for text classification, including the specific case of detecting fake news. BERT, a widely used language model, has also been confirmed to be useful in this domain by previous research, as highlighted in the paper authored by Gundapu and Mamidi \autocite{Gundapu2021}. In the study, the authors demonstrated that BERT, with an F1 Score of 0.9813, outperforms traditional methods such as SVM with an F1 Score of 0.9640. Furthermore, BERT proved to be superior to BiLSTM with Attention Mechanism, which achieved an F1 Score of 0.9785.
Furthermore, a multitude of research works \autocite{Alonso-Bartolome2021, Wang2021, Singhal2019, Aljawarneh2022, Jwa2019, Yang2019} have corroborated the applicability of BERT in various approaches to detecting fake news.

In order to enhance its performance, the Bidirectional Encoder Representations from Transformers (BERT) model can be fine-tuned to a particular task or dataset. This approach allows users to tailor the pre-trained model, which incurs significant costs in terms of time and resources, to suit their specific requirements, such as a specific linguistic structure or subject matter of texts. Fine-tuning the BERT model is a cost-effective alternative to training the base model from scratch, although it still demands a substantial amount of computational resources. Technically, the fine-tuning process entails adding an additional layer to the final BERT layer and training the entire model for several epochs \autocite{Devlin2018}. As a result, the state-of-the-art performance achieved via fine-tuning has made it widely adopted for various Natural Language Processing (NLP) tasks.

However, Devlin et al. have contended that state-of-the-art results can also be obtained without the need for fine-tuning. They suggest utilizing pre-trained BERT embeddings as inputs to a Bidirectional Long Short-Term Memory (BiLSTM) network prior to the final classification layer. This technique produces an F1 Score that is only 0.3 percentage points lower than the score achieved via fine-tuning. Peters et al. (2018) have arrived at similar findings. They have conducted a comparative analysis of fine-tuned and feature-based models on various NLP tasks, such as semantic text similarity, named entity recognition, sentiment analysis, and natural language inference. Their results indicate that, except for semantic text similarity, both types of models, fine-tuned and feature-based approaches, exhibit comparable performance on the aforementioned NLP tasks \autocite{Peters2019}.

According to the existing literature, it can be inferred that the incorporation of BERT leads to improved prediction results, irrespective of the model applied to it \autocite{Kaliyar2021}. Kaliyar et al. (2021) proposed a framework in which they employed BERT for word embedding, and subsequently applied diverse machine learning models such as Multinomial Naïve Bayes, Decision Tree, Random Forest, and KNN. The authors noted that the utilization of BERT led to enhanced prediction results. Alternatively, one can aim to enhance the performance of Vanilla BERT by combining it with LSTM (BERT + LSTM). In their study, Rai et al. investigated the performance of BERT with a modification involving the addition of LSTM on two imbalanced datasets, namely FakeNewsNet (PolitiFact and GossipCop). The incorporation of LSTM in one dataset demonstrated an improvement in the F1 Score from 0.88 to 0.90. In the second dataset, the F1 Score remained unchanged at 0.89 \autocite{Rai2022}.
Considering the susceptibility of neural networks to overfitting, Naïve Bayes has been observed to outperform deep learning and language models on small datasets. However, as the size of the dataset increases, deep learning models begin to surpass Naïve Bayes in fake news prediction. The utilization of language models is expected to yield improved and state-of-the-art performance \autocite{Khan2021}.


In addition to the extensively described models for predicting fake news based on the BERT architecture found in the literature, there is another family of models that has gained popularity recently. The language models Generative Pre-trained Transformer (GPT), created by OpenAI, have gained significant popularity in recent months due to their unprecedented ability to generate human-like text. These models can be used to extract embeddings that are useful for predicting fake news. The latest GPT model released in open-source mode is GPT-2, which will be utilized in this study.

There are very few references in the literature that discuss the idea of using GPT-2 for fake news prediction. However, the results of one study indicate that RoBERTa outperforms GPT-2 in terms of accuracy, achieving 0.979 compared to GPT-2's accuracy of 0.974 (in the same study, BERT achieved 0.971) \autocite{Shifath2021}. Evidence of RoBERTa's superiority over GPT-2 can also be found in other studies \autocite{Cruz2019}. Nevertheless, the literature also provides confirmation that using GPT-2 for fake news prediction may perform better than BERT. Although the authors do not extract the pre-trained embedding layer, this is the closest approach found in the literature regarding the use of GPT-2 for fake news prediction \autocite{Goel2021}.

When comparing the results of the models from the presented studies, it can be observed that they significantly differ in terms of performance. This is due to the fact that the quality of predictions is largely dependent on the dataset used. Data sets, or rather news texts, are entirely dependent on the data collection methodology. Some datasets (e.g., Fake News) are characterized by easily detectable patterns. Typically, in such datasets, both fake and true news contain specific words that allow for easy classification. For studies that used these types of datasets, the performance will be nominally very high.

On the other hand, there are also datasets such as LIAR, used in this paper, for which the performance is nominally weaker than the others. Its structure does not exhibit easily detectable dependencies between fake news and true news.

Therefore, it is important to note that due to the differences in the datasets, it is not possible to compare the performance of models between different studies if they used different datasets. It is possible to compare models within a specific paper and then verify whether a given model performs similarly compared to other models in other studies.

In each of the aforementioned sources, the authors state that they achieved state-of-the-art performance using language models. I will verify this by formulating the first research hypothesis:
\vspace{0.2cm}

\noindent\textbf{H1: Language model-based models (BERT, RoBERTa, GPT-2) outperform other embedding models (Word2Vec, GloVe).}
\vspace{0.2cm}

During the literature review, I did not find any study that comprehensively examined how different types of embeddings, in combination with various classification methods, affect the model's performance. Additionally, due to differences in datasets among the papers, it is not possible to resolve this issue by analyzing the results of different studies. Therefore, I have decided to investigate whether embedding methods and classification methods affect the model's performance. I will examine this phenomenon separately by formulating a research hypothesis:
\vspace{0.2cm}

\noindent\textbf{H2: The type of classification method used significantly influences the model's results.}
\vspace{0.2cm}

To verify these hypotheses, I will employ statistical tests, applying them to the obtained results of all model combinations.