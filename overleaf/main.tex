\documentclass[
  journal=largetwo,
  manuscript=article-type,
  year=2023,
  volume=1,
]{cup-journal}

\usepackage{amsmath}
\usepackage[nopatch]{microtype}
\usepackage{booktabs}
\usepackage{subfiles}
\usepackage{soul} % write strikethrough text
\usepackage{multirow}
\usepackage{hyperref} 


\title{Exploring the Efficacy of BERT for Improving Fake News Detection Across Different Types of Machine Learning Models}

\author{S. Socha}
\affiliation{Faculty of Economic Sciences, University of Warsaw, Warsaw, Poland}
\email[S. Socha]{s.socha2@student.uw.edu.pl}

\addbibresource{bibliography.bib}

\keywords{fake news detection, NLP, BERT} %% First letter not capped

\begin{document}

\begin{abstract}
Recent events such as the COVID-19 pandemic and the conflict in Ukraine have demonstrated that disinformation has reached unprecedented levels. The proliferation of disinformation methods and their increasingly sophisticated concealment techniques have highlighted the need for more powerful tools for detecting it. In this study, I investigate the efficacy of Bidirectional Encoder Representations from Transformers (BERT) in enhancing fake news detection performance across various machine learning algorithms. The results of this research demonstrate that integrating BERT is likely to enhance prediction accuracy. I show that... \textit{WIP (to be finished after paper is finished)...}
\end{abstract}


\section{Introduction \textit{(WIP)}}
\subfile{sections/section1_introduction}

\section{Related Work \textit{(WIP)}}
\subfile{sections/section2_relatedwork}

\section{Methodology \textit{(WIP)}}
\subfile{sections/section3_methodology}

\section{Conclusion \textit{(WIP)}}
\subfile{sections/section4_conclusion}



\printendnotes

\printbibliography

\end{document}